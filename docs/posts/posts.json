[
  {
    "path": "posts/2023-05-04-fraud-classification/",
    "title": "Clasificaci贸n de fraudes con tarjetas de cr茅dito",
    "description": "En este posteo aplicaremos Machine Learning para clasificar transacciones fraudulentas con tarjetas de cr茅dito. Como suele ser com煤n en esta 谩rea, nos encontraremos con un dataset con clases desbalanceadas, por lo que tambi茅n analizaremos como tratar con esta situaci贸n.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2023-05-04",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroducci贸n \r\nAn谩lisis Exploratorio \r\n\r\n\r\nEntrenamos los modelos\r\n Dividimos el dataset en\r\ntrain y test\r\n锔 Instanciamos los modelos\r\na entrenar\r\n\r\nEvaluamos los modelos\r\n\r\n\r\n锔Re-entrenamos los modelos\r\n\r\nAnalizamos el impacto del reentrenamiento en el negocio\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIntroducci贸n \r\nEn esta publicaci贸n estaremos aplicando algunos algoritmos de Machine\r\nLearning para clasificar transacciones fraudulentas con tarjetas de\r\ncr茅dito. Para ello, utilizaremos un dataset publicado en Kaggle\r\nque contiene 284.807 transacciones con tarjetas de cr茅dito, de las\r\ncu谩les 492 son fradulentas, lo que implica un conjunto de datos muy\r\ndesbalanceado.\r\nPor cuestiones relacionadas a la confidencialidad, todas las\r\nvariables num茅ricas han sido transformadas mediante PCA, excepto por la\r\nvariable Time y Amount.\r\nSin m谩s nos adentramos en el ejercicio.\r\nAn谩lisis Exploratorio \r\nPrimeramente vamos a cargar el dataset.\r\n\r\n\r\ndata <- readr::read_csv(\"data/creditcard.csv\")\r\n\r\n\r\n\r\nComo primer paso, con la librer铆a DataExplorer observaremos si el\r\ndataset tiene valores p茅rdidos.\r\n\r\n\r\n\r\nComo puede observarse no existen valores p茅rdidos en el dataset, por\r\nlo que ahora avanzaremos con la distribuci贸n de la variable target\r\n(Class).\r\n\r\n\r\n\r\nAqu铆 puede observarse que s贸lo el 0,24% de las transacciones han sido\r\nfraudulentas, en tanto que m谩s del 99% no lo fueron.\r\nDistribuci贸n\r\nde las transacciones (fraudulentas y no) seg煤n la variable time\r\n\r\n\r\n\r\nEn el gr谩fico de densidad, puede observarse que las transacciones\r\nfraudulentas suelen darse en un periodo de tiempo menor. Asimismo, luego\r\nde los 100 mil segundos las transacciones no fradulentas caen, en cambio\r\nlas fraudulentas se mantienen.\r\nDescripci贸n de la variable\r\nAmount\r\n\r\n\r\nClass\r\n\r\n\r\nMean\r\n\r\n\r\nMedian\r\n\r\n\r\nQ1\r\n\r\n\r\nQ3\r\n\r\n\r\nMax\r\n\r\n\r\n0\r\n\r\n\r\n88.29102\r\n\r\n\r\n22.00\r\n\r\n\r\n5.65\r\n\r\n\r\n77.05\r\n\r\n\r\n25691.16\r\n\r\n\r\n1\r\n\r\n\r\n122.21132\r\n\r\n\r\n9.25\r\n\r\n\r\n1.00\r\n\r\n\r\n105.89\r\n\r\n\r\n2125.87\r\n\r\n\r\nSi miramos la distribuci贸n del amount entre las transacciones por\r\nClass, se puede apreciar que la distribuci贸n de los montos de las\r\ntransacciones fraudulentas es m谩s asim茅trica en comparaci贸n con las no\r\nfraudulentas (ver la diferencia entre el promedio y la mediana). M谩s\r\na煤n, si comparamos la mediana en este caso puede verse que las\r\ntransacciones no fraudulentas muestran un valor m谩s alto (22 vs\r\n9.25).\r\nEsto tambi茅n puede verse en el siguiente gr谩fico, en d贸nde adem谩s\r\nremovemos los valores at铆picos (valores por encima del Q3 + (1.5 * IQR)\r\no por debajo del Q1 - (1.5 * IQR)).\r\n\r\n\r\n\r\n Entrenamos los modelos\r\n Dividimos el dataset en\r\ntrain y test\r\n\r\n\r\ndata$Class <- relevel(as.factor(data$Class),ref = \"1\")\r\n\r\nset.seed(123)\r\n\r\nsplits <- initial_split(data,strata = \"Class\",prop = 0.7)\r\n\r\ntrain <- training(splits)\r\ntest <- testing(splits)\r\n\r\n\r\n\r\nLa distribuci贸n de la variable target (Class) en el dataset de train\r\nes 0.17 y en el dataset de test es 0.18\r\n锔 Instanciamos los modelos\r\na entrenar\r\nPrimeramente creamos la receta y luego instanciamos los modelos.\r\n\r\n\r\n#Armamos la receta \r\n\r\nfraud_rcp <- recipe(Class ~ ., data)\r\n\r\n\r\n#Instanciamos los modelos\r\n\r\nlogistic <-\r\n  logistic_reg() %>%\r\n  set_engine('glm')\r\n\r\n\r\ndecision_tree <-\r\n  decision_tree() %>%\r\n  set_engine('rpart') %>%\r\n  set_mode('classification')\r\n\r\nrand_forest <-\r\n  rand_forest() %>%\r\n  set_engine('ranger') %>%\r\n  set_mode('classification')\r\n\r\nxgboost <-\r\n  boost_tree() %>%\r\n  set_engine('xgboost') %>%\r\n  set_mode('classification')\r\n\r\n\r\n\r\nCreamos una funci贸n que nos permita evaluar los modelos instanciados\r\nen la etapa anterior y nos devuelva como resultado la matriz de\r\nconfusi贸n y las m茅tricas. Se seleccionaron accuracy, recall y\r\nroc_auc para evaluar los modelos. La inclusi贸n del roc_auc como\r\nm茅trica responde a que se ajusta de forma adecuada a la evaluaci贸n de\r\nmodelos entrenados con conjuntos de datos desbalanceados.\r\n\r\n\r\n#Creo una funci贸n para evaluar los modelos\r\n\r\nrun_exploration <- function(model, receta){\r\n\r\n#Entrenamos el modelo\r\nset.seed(123)\r\n\r\nmodel.fit <- workflow()%>%\r\n  add_recipe(receta) %>%\r\n  add_model(model) %>%\r\n  fit(train)\r\n\r\n\r\n#Obtenemos las m茅tricas de error\r\nset.seed(123)\r\n\r\n#Predecimos los valores\r\ny_predicha <- model.fit %>%\r\n  predict(test)\r\n\r\n#Uno los valores predichos al test\r\n\r\nresult <- test %>%\r\n  select(Class)%>% \r\n  bind_cols(y_predicha) %>%\r\n  as.data.frame()\r\n\r\n#Seteo las m茅tricas\r\neval_metrics <- metric_set(recall, accuracy)\r\n\r\n#Genero las m茅tricas\r\nmetricas = eval_metrics(data = result, truth = Class, estimate = .pred_class)\r\n\r\nmetricas <- as.data.frame(metricas)\r\n\r\nmodelo = deparse(substitute(model))\r\n\r\nmetricas$model <- rep(modelo,nrow(metricas))\r\n\r\n#Creo la matriz de confusi贸n\r\n\r\ncm <- conf_mat(data = result, truth = Class, estimate = .pred_class)\r\n\r\n#Grafico la matriz\r\ncm_graf <- autoplot(cm, type = \"heatmap\") +\r\n  scale_fill_gradient(low = \"white\", high = \"#badb33\")\r\n\r\n#Obtenemos las probabilidades\r\n\r\ny_predicha_prob <- model.fit %>%\r\n  predict(test, type= \"prob\")\r\n\r\n# Unimos las probabilidades al test\r\n\r\nresult_prob <- test %>%\r\n  select(Class)%>% \r\n  bind_cols(y_predicha_prob) %>%\r\n  as.data.frame()\r\n\r\n#Calculamos el ROC_AUC y se suma a metricas\r\n\r\nroc_auc <- result_prob %>%\r\n  roc_auc(Class, .pred_1)\r\n\r\nroc_auc$model <- rep(modelo,nrow(roc_auc))\r\n\r\n#Uno el ROC_AUC al resto de las metricas\r\n\r\nmetricas <- rbind(metricas,roc_auc)\r\n\r\n\r\nreturn(list(\"result\" = result,\"metricas\"= metricas, \"cm\"= cm_graf))\r\n\r\n}\r\n\r\n\r\n\r\n Evaluamos los modelos\r\n\r\n\r\ntest_log = run_exploration(logistic,fraud_rcp)\r\ntest_dt = run_exploration(decision_tree,fraud_rcp)\r\ntest_rf = run_exploration(rand_forest,fraud_rcp)\r\ntest_xgboost = run_exploration(xgboost,fraud_rcp)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndecision_tree\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\nmodel\r\n\r\n\r\nrecall\r\n\r\n\r\nbinary\r\n\r\n\r\n0.728\r\n\r\n\r\ndecision_tree\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.999\r\n\r\n\r\ndecision_tree\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.901\r\n\r\n\r\ndecision_tree\r\n\r\n\r\nlogistic\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\nmodel\r\n\r\n\r\nrecall\r\n\r\n\r\nbinary\r\n\r\n\r\n0.570\r\n\r\n\r\nlogistic\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.999\r\n\r\n\r\nlogistic\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.973\r\n\r\n\r\nlogistic\r\n\r\n\r\nrand_forest\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\nmodel\r\n\r\n\r\nrecall\r\n\r\n\r\nbinary\r\n\r\n\r\n0.755\r\n\r\n\r\nrand_forest\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n1.000\r\n\r\n\r\nrand_forest\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.954\r\n\r\n\r\nrand_forest\r\n\r\n\r\nxgboost\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\nmodel\r\n\r\n\r\nrecall\r\n\r\n\r\nbinary\r\n\r\n\r\n0.755\r\n\r\n\r\nxgboost\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n1.000\r\n\r\n\r\nxgboost\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.947\r\n\r\n\r\nxgboost\r\n\r\n\r\n\r\n\r\nObservaciones\r\nEn los 4 modelos entrenados puede observarse el escenario de un\r\nvalor alto de accuracy (precisi贸n) pero bajo de recall (sensibilidad).\r\nEsto quiere decir que nuestro modelo es eficiente en clasificar\r\ncorrectamente los datos pero es deficiente para clasificar correctamente\r\nlos casos (en nuestro conjunto la clase minoritaria).\r\nEl modelo logistico fu茅 el que peor recall mostr贸, en tanto que el\r\nmodelo random forest com xgboost mostraron los mejores valores de\r\nrecall.\r\nTodos los modelos presentaron valores alto de roc_auc.\r\n\r\n锔 Estrategias para tratar con datos desbalanceados\r\nExisten diversas estrategias para tratar con datos desbalanceados.\r\nEntre ellas, el sobremuestreo (Over-sampling) de la clase\r\nminoritariao el submuestreo (Under-sampling) de la\r\nclase mayoritaria.\r\nEn el ecosistema de tidymoldels contamos con el paquete\r\nthemis que permite agregar pasos a la receta.\r\nVeamos como funcionan algunas de estas estrategias.\r\nSobremuestreamos\r\nla clase minoritaria hasta igualar el 100% de la mayoritaria\r\n\r\n\r\nrecipe(Class ~ ., data) %>%\r\n  step_mutate_at(Class,fn = factor) %>%\r\n  step_relevel(Class,ref_level = \"1\") %>%\r\n  step_upsample(Class,over_ratio = 0.5) %>%\r\n  prep() %>%\r\n  bake(new_data = NULL) %>%\r\n  group_by(Class)%>%\r\n  summarise(n= n()) %>%\r\n  kableExtra::kbl() %>%\r\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\r\n\r\n\r\n\r\nClass\r\n\r\n\r\nn\r\n\r\n\r\n1\r\n\r\n\r\n142157\r\n\r\n\r\n0\r\n\r\n\r\n284315\r\n\r\n\r\nSubmuestreamos\r\nla clase mayoritaria hasta igual el 100% de la minoritaria\r\n\r\n\r\nrecipe(Class ~ ., data) %>%\r\n  step_mutate_at(Class,fn = factor) %>%\r\n  step_relevel(Class,ref_level = \"1\") %>%\r\n  step_downsample(Class,under_ratio = 1) %>%\r\n  prep() %>%\r\n  bake(new_data = NULL) %>%\r\n  group_by(Class)%>%\r\n  summarise(n= n()) %>%\r\n  kableExtra::kbl() %>%\r\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\r\n\r\n\r\n\r\nClass\r\n\r\n\r\nn\r\n\r\n\r\n1\r\n\r\n\r\n492\r\n\r\n\r\n0\r\n\r\n\r\n492\r\n\r\n\r\nSobremuestro\r\nutilizando el algoritmo SMOTE (Synthetic Minority Oversampling\r\nTechnique)\r\nEste algoritmo crea nuevos ejemplos de la clase minoritaria\r\nutilizando los k vecinos m谩s cercanos.\r\n\r\n\r\nrecipe(Class ~ ., data) %>%\r\n  step_mutate_at(Class,fn = factor) %>%\r\n  step_relevel(Class,ref_level = \"1\") %>%\r\n  step_smote(Class,over_ratio = 1) %>%\r\n  prep() %>%\r\n  bake(new_data = NULL) %>%\r\n  group_by(Class)%>%\r\n  summarise(n= n()) %>%\r\n  kableExtra::kbl() %>%\r\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\r\n\r\n\r\n\r\nClass\r\n\r\n\r\nn\r\n\r\n\r\n1\r\n\r\n\r\n284315\r\n\r\n\r\n0\r\n\r\n\r\n284315\r\n\r\n\r\n锔Re-entrenamos los modelos\r\nAhora suraremos a la receta un paso adicional, que es el\r\nsobremuestreo mediante el algrotimo SMOTE.\r\n\r\n\r\nfraud_resample_rec <- recipe(Class ~ ., data) %>%\r\n  step_mutate_at(Class,fn = factor) %>%\r\n  step_relevel(Class,ref_level = \"1\") %>%\r\n  step_smote(Class,over_ratio = 1)\r\n\r\n\r\n\r\nObtenemos las m茅tricas\r\n\r\n\r\ntest_log_resample = run_exploration(logistic,fraud_resample_rec)\r\ntest_dt_resample = run_exploration(decision_tree,fraud_resample_rec)\r\ntest_xgboost_resample = run_exploration(xgboost,fraud_resample_rec)\r\n\r\n\r\n\r\n\r\n\r\ndecision_tree\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\nmodel\r\n\r\n\r\nrecall\r\n\r\n\r\nbinary\r\n\r\n\r\n0.861\r\n\r\n\r\ndecision_tree\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.965\r\n\r\n\r\ndecision_tree\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.914\r\n\r\n\r\ndecision_tree\r\n\r\n\r\nlogistic\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\nmodel\r\n\r\n\r\nrecall\r\n\r\n\r\nbinary\r\n\r\n\r\n0.854\r\n\r\n\r\nlogistic\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.993\r\n\r\n\r\nlogistic\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.976\r\n\r\n\r\nlogistic\r\n\r\n\r\nxgboost\r\n\r\n.metric\r\n\r\n\r\n.estimator\r\n\r\n\r\n.estimate\r\n\r\n\r\nmodel\r\n\r\n\r\nrecall\r\n\r\n\r\nbinary\r\n\r\n\r\n0.841\r\n\r\n\r\nxgboost\r\n\r\n\r\naccuracy\r\n\r\n\r\nbinary\r\n\r\n\r\n0.997\r\n\r\n\r\nxgboost\r\n\r\n\r\nroc_auc\r\n\r\n\r\nbinary\r\n\r\n\r\n0.979\r\n\r\n\r\nxgboost\r\n\r\n\r\n\r\n\r\nObservaciones\r\nLos modelos reentrenados mostraron una mejor铆a sustancial en el\r\nrecall,destac谩ndose el modelo log铆stico que paso de un recall de 57% a\r\n85%, pasando a ser el modelo con mayor recall de los entrenados.\r\nLo que s铆 debemos considerar que esta mejor铆a en el recall trae\r\naparejado un incremento en los Falsos positivos. Por ejemplo en\r\nel caso del modelo log铆stico el n煤mero de Falsos positivos pas贸 de 11 a\r\n616.\r\nEl modelo xgboost si bien tiene un menor recall que el anterior,\r\ngenera menor cantidad de Falsos positivos (266), pero vale la pena\r\nplantear en escenarios el impacto en el negocio.\r\n\r\nAnalizamos el impacto del reentrenamiento en el negocio\r\nPara esto plantearemos 3 escenarios:\r\nEscenario 1: Analizar el costo (Amount) de los\r\nVerdaderos positivos (Fraudes) en el modelo XGBoost sin la estrategia de\r\nsobremuestreo.\r\nEscenario 2: Analizar el costo (Amount) de los\r\nVerdaderos positivos (Fraudes) en el modelo XGBoost con sobremuestreo.\r\nDe esta forma sabremos si logramos evitar mayores p茅rdidas monetarias\r\nversus el escenario 1.\r\nEscenario 3: Analizar el costo (Amount) de los\r\nFalsos positivos (Transacciones clasificadas como fraude sin serlo) en\r\nel modelo XGBoost con sobremuestreo.\r\nEscenario 1\r\n\r\n\r\ntest %>%\r\n  select(Amount) %>%\r\n  bind_cols(test_xgboost$result) %>%\r\n  summarise(Amount = sum(Amount[.pred_class== 1 & Class == 1])) %>%\r\n  kableExtra::kbl(format.args = list(big.mark= \".\", decimal.mark=\",\"),\r\n                  col.names = \"Escenario 1\")%>%\r\n  kableExtra::kable_classic_2(full_width= F)\r\n\r\n\r\n\r\nEscenario 1\r\n\r\n\r\n11.767,1\r\n\r\n\r\nEscenario 2\r\n\r\n\r\ntest %>%\r\n  select(Amount) %>%\r\n  bind_cols(test_xgboost_resample$result) %>%\r\n  summarise(Amount = sum(Amount[.pred_class== 1 & Class == 1])) %>%\r\n  kableExtra::kbl(format.args = list(big.mark= \".\", decimal.mark=\",\"),\r\n                  col.names = \"Escenario 2\",)%>%\r\n  kableExtra::kable_classic_2(full_width= F)\r\n\r\n\r\n\r\nEscenario 2\r\n\r\n\r\n13.424,04\r\n\r\n\r\nComo se puede observar, la mejor铆a en el rendimiento del modelo nos\r\npermite evitar fraudes por 13.424,04, es decir 1.656,94 m谩s que con el\r\nprimer modelo. Pero cabe preguntarse 驴Que monto representan las\r\ntransacciones clasificadas como fraude sin serlo?\r\nEscenario 3\r\n\r\n\r\ntest %>%\r\n  select(Amount) %>%\r\n  bind_cols(test_xgboost_resample[1]) %>%\r\n  summarise(Amount = sum(Amount[.pred_class== 1 & Class == 0])) %>%\r\n  kableExtra::kbl(format.args = list(big.mark= \".\", decimal.mark=\",\"),\r\n                  col.names = \"Escenario 3\")%>%\r\n  kableExtra::kable_classic_2(full_width= F)\r\n\r\n\r\n\r\nEscenario 3\r\n\r\n\r\n56.508,75\r\n\r\n\r\n En el escenario 3 puede verse el impacto que tienen los falsos\r\npositivos clasificados por nuestro modelo. Recordemos que en nuestro\r\nconjunto de datos la mediana del monto de las transacciones fraudulentas\r\nes menor que el de las no fraudulentas. Es muy importante considerar\r\ntodos los escenarios posibles y el impacto que el o los modelos pueden\r\ntener en el negocio.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-04-fraud-classification/imagenes/credit_card_fraud.jpg",
    "last_modified": "2023-05-04T16:00:40-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-06-compartimos-nuestro-reporte-en-un-email/",
    "title": "Compartimos nuestro reporte en un email",
    "description": "En un post anterior mostramos como segmentar nuestros clientes mediante un modelo RFM. En este post voy a mostrar como enviarlo en un correo electr贸nico para compartir nuestro reporte con los stakeholder y apoya la toma de decisiones.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-10-02",
    "categories": [],
    "contents": "\r\nIntroducci贸n \r\nSin dudas uno de los puntos m谩s importantes en el mundo del an谩lisis\r\nde datos es disponibilizar de forma oportuna y 谩gil nuestros reportes a\r\nlos diversos stakeholder de la organizaci贸n.\r\nExisten diversas estrategias, como por ejemplo elaborar un\r\ndashboards. En esta ocasi贸n voy a mostrar un ejemplo mediante el cu谩l\r\npodemos crear un correo electr贸nico 锔 con nuestro reporte y enviarlo a\r\nlos remitentes que nosotros deseemos.\r\nPara ello utilizaremos la libreria  blastula junto\r\ncon Rmarkdown.\r\nEstaremos compartiendo el reporte RFM que elaboramos\r\nen este post.\r\nCreamos el reporte \r\nDebemos crear un archivo Rmarkdown con formato de salida output:\r\nblastula::blastula_email\r\n\r\n\r\ntitle: \"RFM_report\"\r\nauthor: \"Hernan Hernandez\"\r\ndate: '2022-10-02'\r\noutput: blastula::blastula_email\r\n\r\n\r\n\r\n Creamos\r\nel script para enviar el correo electr贸nico\r\nPara este paso usaremos la funci贸n render_email de\r\nblastula indicandole la ubicaci贸n de nuestro archivo .rmd\r\n\r\n\r\nemail <- render_email(\"C:/Users/usuario/Desktop/Reporte_RFM.Rmd\")\r\n\r\n\r\n\r\n Configuramos el correo\r\nelectr贸nico\r\n\r\n\r\nemail %>%\r\n  smtp_send(to = \"****\",\r\n            from = \"****\",\r\n            subject = paste0(\"Reporte RFM\",format(lubridate::ymd(Sys.Date()),\"%B %Y\")),\r\n            credentials = creds_file(file = \"gmail_creds\"))\r\n\r\n\r\n\r\n锔 Para poder usar blastula debemos tener contrase帽a de aplicaci贸n de\r\ngmail.\r\n Veamos el reporte\r\n\r\n\r\n\r\nFigure 1: Vista de la bandeja de entradas\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 2: Vista del cuerpo del email\r\n\r\n\r\n\r\nComentarios finales \r\n锔 Resulta muy interesante la funcionalidad que nos ofrece\r\nblastula permitiendo crear un correo electr贸nico con\r\nnuestro reporte directamente desde un archivo\r\nRmarkdown.\r\n锔Adem谩s, aunque no lo abordamos en este ejemplo, admite la\r\nutilizaci贸n de par谩metros los que nos permitir铆a crear\r\nreportes que segmentan los datos y se env铆an a diferentes destinatarios.\r\nEn este caso podr铆amos crear un reporte RFM por pa铆s y enviarlos a\r\ndistintos destinatarios.\r\n锔 Sin dudas poder compartir un correo electr贸nico con nuestro\r\nreporte es una gran estrategia de comunicaci贸n de datos\r\na los stakeholder.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-06-compartimos-nuestro-reporte-en-un-email/imagenes/email_preview.jpg",
    "last_modified": "2023-05-04T14:35:57-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-05-retencion-de-clientes-analisis-por-cohortes/",
    "title": "Retencion de clientes: Analisis por cohortes",
    "description": "En la estrategia comercial de cualquier emprendimiento uno de los KPI麓s que se vuelve imprescindible monitorear es cu谩ntos clientes nos siguen comprando en un per铆odo de tiempo dado o cuantos clientes nos han abandonado. En este post voy a mostrar como calcular este indicador mediante el an谩lisis de cohortes.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-09-05",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroducci贸n\r\n\r\nComenzamos el procesamiento \r\nAlgunas observaciones \r\nPodemos explorar a nivel de\r\npa铆ses 猴\r\n\r\nComentarios finales \r\n\r\nIntroducci贸n \r\nBienvenidos y bienvenidas a un nuevo post sobre un\r\nKPI (Key Performance Indicator) muy importante al\r\nmomento de analizar la estrategia comercial de\r\ncualquier empresa o negocio y que responde a una cuesti贸n central: 驴que\r\nproporci贸n de los clientes que nos compraron en alg煤n momento lo siguen\r\nhaciendo en determinado per铆odo? y en este misma l铆nea: 驴que proporci贸n\r\ndej贸 de hacerlo?.\r\nDe lo anterior se desprenden tres conceptos importantes:\r\nTasa de retenci贸n: representa la proporci贸n de\r\nclientes que nos siguen comprando en determinado per铆odo de\r\ntiempo.\r\nChurn de clientes o tasa de abandono: es la\r\nproporci贸n de clientes que han abandonado nuestro negocio.\r\nAn谩lisis de cohortes: representa un m茅todo de\r\nan谩lisis de la retenci贸n o abandono en el cu谩l hacemos un seguimiento de\r\nlos clientes a lo largo de un tiempo (seguimiento longitudinal). A su\r\nvez, una cohorte est谩 caracterizada por un criterio\r\ncom煤n que los agrupa, como puede ser por ejemplo la fecha de su primer\r\ncompra en nuestro negocio.\r\nSi nos resultan confusos estos conceptos , veamos un ejemplo y\r\nrevisemos la construcci贸n de c贸digo.\r\nCargamos el\r\ndataset y activamos los paquetes \r\nPara este ejemplo utilizaremos el mismo dataset que trabajamos en el\r\nart铆culo de An谩lisis RFM\r\n, el cual lo hab铆amos obtenido de Kaggle. Recordemos que se trataba de las ventas\r\ne-commerce de una tienda minorista con sede en el Reino Unido y\r\noperaciones en varios pa铆ses.\r\n\r\n\r\nShow code\r\n\r\n##Cargo el dataset y las funciones\r\n\r\ndf <- read.csv(\"data/data.csv\")\r\n\r\n#Cargo las librerias\r\n\r\nlibrary(dplyr)\r\nlibrary(lubridate)\r\nlibrary(tidyr)\r\nlibrary(ggplot2)\r\nlibrary(plotly)\r\n\r\n\r\n\r\nVeamos  las variables del dataset 1:\r\n\r\n\r\nShow code\r\n\r\nglimpse(df)\r\n\r\n\r\n## Rows: 541,909\r\n## Columns: 8\r\n## $ InvoiceNo   <chr> \"536365\", \"536365\", \"536365\", \"536365\"~\r\n## $ StockCode   <chr> \"85123A\", \"71053\", \"84406B\", \"84029G\",~\r\n## $ Description <chr> \"WHITE HANGING HEART T-LIGHT HOLDER\", ~\r\n## $ Quantity    <int> 6, 6, 8, 6, 6, 2, 6, 6, 6, 32, 6, 6, 8~\r\n## $ InvoiceDate <chr> \"12/1/2010 8:26\", \"12/1/2010 8:26\", \"1~\r\n## $ UnitPrice   <dbl> 2.55, 3.39, 2.75, 3.39, 3.39, 7.65, 4.~\r\n## $ CustomerID  <int> 17850, 17850, 17850, 17850, 17850, 178~\r\n## $ Country     <chr> \"United Kingdom\", \"United Kingdom\", \"U~\r\n\r\nDe todas las variables de dataset, utilizaremos para nuestro an谩lisis\r\nla fecha de factura (InvoiceData), la\r\nidentificaci贸n del usuario (CustomerID) y el\r\npa铆s (Country).\r\nComenzamos el procesamiento \r\nEn esta etapa realizaremos dos transformaciones centrales en el\r\nan谩lisis de la retenci贸n por cohortes:\r\nCreamos las cohortes: el criterio para agrupar\r\nlos usuarios y crear los cohortes es el mes en la cual realizaron la\r\nprimer compra a nuestro negocio.\r\nCreamos los meses de permanencia: son los meses\r\nen los cuales los usuarios registran compras a nuestro negocio.\r\nAdem谩s vamos a excluir los usuarios sin identificaci贸n y adem谩s vamos\r\na convertir la variable de fecha de facturaci贸n que es de cadena en una\r\nvariable de fecha.\r\n\r\n\r\nShow code\r\n\r\ndf <- df %>%\r\n  mutate(Date= as_date(lubridate::mdy_hm(InvoiceDate))) %>%\r\n  filter(!is.na(CustomerID))%>% #Excluimos usuarios sin identificaci贸n\r\n  group_by(CustomerID)%>%\r\n  mutate(min_fecha= min(Date), \r\n         cohorte = paste0(month(min_fecha),\"-\",year(min_fecha)), #Creamos la cohorte seg煤n la fecha de la m铆nima compra\r\n         mes= interval(min(Date),Date) %/% months(1)) #Representa los meses en que el usuario ha realizado compras\r\n\r\n\r\n\r\nObtenemos la tasa de retenci贸n\r\nР\r\nEn este punto del an谩lisis vamos a obtener la tasa de\r\nretenci贸n. Para ello, contaremos los usuarios (distintos) por\r\ncohorte y por mes de compra. Luego,\r\ncalculamos la tasa de retenci贸n como el cociente entre la cantidad de\r\nusuarios en determinado mes por la cantidad de usuarios en el mes\r\ninicial de seguimiento de la cohorte. Por 煤ltimo, mostramos la evoluci贸n\r\nde la tasa en un heatmap.\r\n\r\n\r\nShow code\r\n\r\nrate.retention <- df %>%\r\n  group_by(cohorte,mes)%>%\r\n  summarise(n = n_distinct(CustomerID))%>%\r\n  mutate(rate= round(n*100/n[mes==0],1),\r\n         cohorte= format(lubridate::my(cohorte),\"%b-%Y\")) %>%\r\n  arrange(cohorte) %>%\r\n  as.data.frame() %>%\r\n  ggplot(aes(x= mes, y= reorder(cohorte,mes), fill= rate))+\r\n  geom_tile()+\r\n  geom_text(aes(label = rate), color = \"white\", size = 3) +\r\n  scale_fill_gradient2(low = \"#E0F3DB\",mid=\"#A8DDB5\",high = \"#43A2CA\") +\r\n  scale_x_continuous(breaks = seq(0,12),expand = c(0,0))+\r\n  labs(y= \"Cohorte\", x= \"Mes\", fill= \"Tasa\")+\r\n  coord_fixed()+\r\n  theme_minimal()\r\n\r\nrate.retention\r\n\r\n\r\n\r\n\r\nFigure 1: Tasa de retenci贸n 12 meses\r\n\r\n\r\n\r\nAlgunas observaciones \r\nEl an谩lisis por cohortes nos permite hacer seguimiento de los\r\nusuarios pudiendo encontrar patrones de comportamiento distintos seg煤n\r\nel mes en el cu谩l realizaron la primer compra a nuestro negocio. De esta\r\nforma podemos evaluar como impacta una campa帽a o una promoci贸n que\r\nlanzamos en determinado momento sobre la fidelizaci贸n de nuestros\r\nclientes.\r\nAsimismo, podr铆amos monetizar el comportamiento de las cohorte\r\nobteniendo el promedio gastado por determinada cohorte a lo largo del\r\ntiempo de seguimiento (Ver Figura 2).\r\nEn el an谩lisis vertical (por columnas) podemos evaluar el\r\ncomportamiento de las cohortes por mes. Por ejemplo: podemos decir que\r\nexcluyendo noviembre y diciembre de 2011, entre el 17 y el 38% de los\r\nclientes que realizan una compra en determinado mes, vuelven a tener una\r\nal mes siguiente.\r\nEn el an谩lisis horizontal (por filas), excluyendo diciembre de\r\n2011 que no est谩 completo (la fecha m谩xima es 9 de diciembre), casi un\r\n50% de los usuarios a煤n siguen realizando compras al a帽o. De hecho, la\r\ncohorte de dic-2010 muestra una mejora en la retenci贸n al mes 10 y 12,\r\nlo que ameritar铆a investigar si se implement贸 alguna campa帽a o\r\npromoci贸n.\r\n\r\n\r\nShow code\r\n\r\nmonetize <- df %>%\r\n  group_by(cohorte,mes)%>%\r\n  summarise(prom= round(mean(UnitPrice),1))%>%\r\n  mutate(cohorte= format(lubridate::my(cohorte),\"%b-%Y\")) %>%\r\n  arrange(cohorte) %>%\r\n  as.data.frame() %>%\r\nggplot(aes(x= mes, y= reorder(cohorte,mes), fill= prom))+\r\n  geom_tile()+\r\n  geom_text(aes(label = prom), color = \"white\", size = 3) +\r\n  scale_fill_gradient2(low = \"#E0F3DB\",mid=\"#A8DDB5\",high = \"#43A2CA\") +\r\n  scale_x_continuous(breaks = seq(0,12),expand = c(0,0))+\r\n  labs(y= \"Cohorte\", x= \"Mes\", fill= \"Tasa\")+\r\n  coord_fixed()+\r\n  theme_minimal()\r\n\r\nmonetize\r\n\r\n\r\n\r\n\r\nFigure 2: Promedio gastado por las cohorte\r\n\r\n\r\n\r\nPodemos explorar a nivel de\r\npa铆ses 猴\r\nComo hab铆amos dicho el dataset tiene los datos de ventas en UK y en\r\nvarios pa铆ses asi que podemos explorar este aspecto.\r\nPara eso seleccionaremos los principales 3 pa铆ses seg煤n la cantidad\r\nde clientes, que son: Reino Unido, Francia y Alemania.\r\nManos a la obra \r\n\r\n\r\nShow code\r\n\r\npaises <- c(\"United Kingdom\",\"France\",\"Germany\")\r\n\r\nrate.retention.country <- list()\r\n\r\nfor(pais in paises){\r\n \r\nrate.retention.country[[pais]]<-\r\n  df %>%\r\n  filter(Country== pais)%>%\r\n  group_by(cohorte,mes)%>%\r\n  summarise(n = n_distinct(CustomerID))%>%\r\n  mutate(rate= round(n*100/n[mes==0],1),\r\n         cohorte= format(lubridate::my(cohorte),\"%b-%Y\")) %>%\r\n  arrange(cohorte) %>%\r\n  as.data.frame() %>%\r\n  ggplot(aes(x= mes, y= reorder(cohorte,mes), fill= rate))+\r\n  geom_tile()+\r\n  geom_text(aes(label = rate), color = \"white\", size = 3) +\r\n  scale_fill_gradient2(low = \"#E0F3DB\",mid=\"#A8DDB5\",high = \"#43A2CA\") +\r\n  scale_x_continuous(breaks = seq(0,12))+\r\n  labs(y= \"Cohorte\", x= \"Mes\", fill= \"Tasa\")+\r\n  coord_fixed()\r\n  \r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nxaringanExtra::use_panelset()\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfor(i in 1:length(rate.retention.country)){\r\n  cat(\"::: {.panel}\\n\")             \r\n  cat(\"##\",paises[i], \"{.panel-name}\\n\")\r\n  print(rate.retention.country[[i]])\r\n  cat(\"\\n\") \r\n  cat(\":::\\n\")\r\n}\r\n\r\n\r\nUnited Kingdom\r\n\r\nFrance\r\n\r\nGermany\r\n\r\n\r\n\r\nC贸mo puede verse en los gr谩ficos, a nivel pa铆s se obtienen tasas de\r\nretenci贸n que resultan inestables por lo que los datos deben ser tomados\r\ncon cautela. Lo que si resulta claro es que el comportamiento de la tasa\r\nde retenci贸n en general responde casi exclusivamente al comportamiento\r\nde los clientes de UK.\r\nComentarios finales \r\n El an谩lisis de la tasa de retenci贸n mediante cohortes representan\r\nun aliado a la hora de evaluar la estrategia comercial a la luz del\r\ncomportamiento de nuestros clientes y c贸mo estos responden a las\r\ndiferentes acciones como las campa帽as, las promociones, el lanzamiento\r\nde nuevos productos, etc.\r\nTambi茅n este tipo de an谩lisis podemos profundizarlos con otros para\r\nestimar el Customer Lifetime Value (CLV) y conocer cuanto puede ganar la\r\nempresa del cliente promedio en el transcurso de la relaci贸n, pero eso\r\nser谩 parte de otro post .\r\n\r\nPuede encontrar una exploraci贸n\r\ncompleta del dataset en el An谩lisis RFM en el siguiente\r\nlink.╋\r\n",
    "preview": "posts/2022-09-05-retencion-de-clientes-analisis-por-cohortes/imagenes/retencion.jpg",
    "last_modified": "2023-05-04T14:35:57-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-27-analisis-rfm-para-la-segmentacion-de-clientes/",
    "title": "Analisis RFM para la segmentacion de clientes",
    "description": "Los modelos RFM (recency, frequency, monetary) son ampliamente utilizados en las areas de marketing para la segmentaci贸n de sus clientes, pudiendo identificar los clientes m谩s leales, o aqu茅llos que no deber铆a perder. En este post voy a mostrarles como implementar un modelo RFM, asi que 隆隆all谩 vamos!!.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-08-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroducci贸n\r\n\r\nExploraci贸n de los datos \r\nImplementamos el modelo \r\nNormalizaci贸n de la tabla 锔\r\n\r\nCreamos los puntajes RFM 锔\r\n驴C贸mo\r\nse asignan los puntajes? 驴Que representan esos valores? \r\n\r\nCreamos los segmentos 锔\r\nComenzamos\r\npor definir los segmentos y los umbrales \r\n驴Que podemos decir de los\r\nsegmentos? \r\n\r\n\r\nComentarios finales \r\n\r\nIntroducci贸n \r\nComo dec铆amos en la preview de este post, los modelos RFM son\r\nampliamente utilizados en las 谩reas de marketing para segmentar sus\r\ncarteras de clientes. Esta segmentaci贸n se realiza a partir de un\r\nscoring (que veremos m谩s adelante), sobre tres par谩metros:\r\nrecency: identifica el tiempo transcurrido\r\n(generalmente en d铆as), desde la 煤ltima actividad del usuario y hasta la\r\nfecha de establecida para el modelo.\r\nfrequency: refiere a con qu茅 frecuencia realiza\r\nmovimientos (de compra o venta) el usuario..\r\nmonetary: refiere al valor o monto de los\r\nmovimientos del usuario.\r\nEl modelo RFM descansa sobre el siguiente principio\r\n:\r\n\r\nEl 80% de tu negocio proviene del\r\n20% de tus clientes .\r\n\r\nEn este post, iremos mostrando una serie de funciones que he creado\r\npara procesar los datos y obtener los segmentos:\r\nnormalize_table: con esta funci贸n normalizamos\r\nlos nombres del dataset para que puedan ser procesados por la funci贸n\r\nque genera el scoring.\r\nrfm_category: con esta funci贸n obtenemos la\r\ntabla RFM, la cu谩l contiene los puntajes para los par谩metros (recency,\r\nfrequency, monetary). Adem谩s, nos devuelve una serie de m茅tricas que\r\nutilizaremos para explorar nuestros datos.\r\nsegment_rfm: esta funci贸n nos permite crear los\r\nsegmentos a partir del scoring de la tabla anterior (tabla RFM), m谩s la\r\ndefinici贸n de los threshold. Tambi茅n nos devuelve algunas\r\nm茅tricas que nos permiten explorar nuestros resultados.\r\nExploraci贸n de los datos \r\nUtilizaremos un dataset tomado de Kaggle\r\nque cuenta con informaci贸n de ventas de una tienda minorista en l铆nea\r\nregistrada y con sede en el Reino Unido. El dataset muestra las\r\noperaciones en distintos pa铆ses lo cu谩l resulta muy atractivo a la hora\r\nde implementar un modelo RFM que contemple los datos a este nivel de\r\nagregaci贸n.\r\nVamos la usar el paquete summarytools para obtener\r\nuna tabla resumen con los valores para todas las variables.\r\n\r\n\r\nShow code\r\n\r\nsummarytools::st_options(lang = 'es')\r\nsummarytools::dfSummary(df, plain.ascii  = FALSE, \r\n                                           style        = \"grid\", \r\n                                           graph.magnif = 0.75, \r\n                                           valid.col    = FALSE,\r\n                                           tmp.img.dir  = \"/tmp\")\r\n\r\n\r\nTabla resumen\r\ndf\r\nDimensiones: 541909 x 8Duplicados: 5268\r\nNo\r\nVariable\r\nEstad铆sticas / Valores\r\nFrec. (% sobre v谩lidos)\r\nGr谩fico\r\nPerdidos\r\n1\r\nInvoiceNo\r\n[character]\r\n1. 573585\r\n2. 581219\r\n3. 581492\r\n4. 580729\r\n5. 558475\r\n6. 579777\r\n7. 581217\r\n8. 537434\r\n9. 580730\r\n10. 538071\r\n[ 25890 otros ]\r\n1114 ( 0.2%)\r\n749 ( 0.1%)\r\n731 ( 0.1%)\r\n721 ( 0.1%)\r\n705 ( 0.1%)\r\n687 ( 0.1%)\r\n676 ( 0.1%)\r\n675 ( 0.1%)\r\n662 ( 0.1%)\r\n652 ( 0.1%)\r\n534537 (98.6%)\r\n\r\n0\r\n(0.0%)\r\n2\r\nStockCode\r\n[character]\r\n1. 85123A\r\n2. 22423\r\n3. 85099B\r\n4. 47566\r\n5. 20725\r\n6. 84879\r\n7. 22720\r\n8. 22197\r\n9. 21212\r\n10. 20727\r\n[ 4060 otros ]\r\n2313 ( 0.4%)\r\n2203 ( 0.4%)\r\n2159 ( 0.4%)\r\n1727 ( 0.3%)\r\n1639 ( 0.3%)\r\n1502 ( 0.3%)\r\n1477 ( 0.3%)\r\n1476 ( 0.3%)\r\n1385 ( 0.3%)\r\n1350 ( 0.2%)\r\n524678 (96.8%)\r\n\r\n0\r\n(0.0%)\r\n3\r\nDescription\r\n[character]\r\n1. WHITE HANGING HEART T-LIG\r\n2. REGENCY CAKESTAND 3 TIER\r\n3. JUMBO BAG RED RETROSPOT\r\n4. PARTY BUNTING\r\n5. LUNCH BAG RED RETROSPOT\r\n6. ASSORTED COLOUR BIRD ORNA\r\n7. SET OF 3 CAKE TINS PANTRY\r\n8. (Cadena vac铆a)\r\n9. PACK OF 72 RETROSPOT CAKE\r\n10. LUNCH BAG BLACK SKULL.\r\n[ 4214 otros ]\r\n2369 ( 0.4%)\r\n2200 ( 0.4%)\r\n2159 ( 0.4%)\r\n1727 ( 0.3%)\r\n1638 ( 0.3%)\r\n1501 ( 0.3%)\r\n1473 ( 0.3%)\r\n1454 ( 0.3%)\r\n1385 ( 0.3%)\r\n1350 ( 0.2%)\r\n524653 (96.8%)\r\n\r\n0\r\n(0.0%)\r\n4\r\nQuantity\r\n[integer]\r\nMedia (d-s) : 9.6 (218.1)\r\nmin < mediana < max:\r\n-80995 < 3 < 80995\r\nRI (CV) : 9 (22.8)\r\n722 valores distintos\r\n\r\n0\r\n(0.0%)\r\n5\r\nInvoiceDate\r\n[character]\r\n1. 10/31/2011 14:41\r\n2. 12/8/2011 9:28\r\n3. 12/9/2011 10:03\r\n4. 12/5/2011 17:24\r\n5. 6/29/2011 15:58\r\n6. 11/30/2011 15:13\r\n7. 12/8/2011 9:20\r\n8. 12/6/2010 16:57\r\n9. 12/5/2011 17:28\r\n10. 12/9/2010 14:09\r\n[ 23250 otros ]\r\n1114 ( 0.2%)\r\n749 ( 0.1%)\r\n731 ( 0.1%)\r\n721 ( 0.1%)\r\n705 ( 0.1%)\r\n687 ( 0.1%)\r\n676 ( 0.1%)\r\n675 ( 0.1%)\r\n662 ( 0.1%)\r\n652 ( 0.1%)\r\n534537 (98.6%)\r\n\r\n0\r\n(0.0%)\r\n6\r\nUnitPrice\r\n[numeric]\r\nMedia (d-s) : 4.6 (96.8)\r\nmin < mediana < max:\r\n-11062.1 < 2.1 < 38970\r\nRI (CV) : 2.9 (21)\r\n1630 valores distintos\r\n\r\n0\r\n(0.0%)\r\n7\r\nCustomerID\r\n[integer]\r\nMedia (d-s) : 15287.7 (1713.6)\r\nmin < mediana < max:\r\n12346 < 15152 < 18287\r\nRI (CV) : 2838 (0.1)\r\n4372 valores distintos\r\n\r\n135080\r\n(24.9%)\r\n8\r\nCountry\r\n[character]\r\n1. United Kingdom\r\n2. Germany\r\n3. France\r\n4. EIRE\r\n5. Spain\r\n6. Netherlands\r\n7. Belgium\r\n8. Switzerland\r\n9. Portugal\r\n10. Australia\r\n[ 28 otros ]\r\n495478 (91.4%)\r\n9495 ( 1.8%)\r\n8557 ( 1.6%)\r\n8196 ( 1.5%)\r\n2533 ( 0.5%)\r\n2371 ( 0.4%)\r\n2069 ( 0.4%)\r\n2002 ( 0.4%)\r\n1519 ( 0.3%)\r\n1259 ( 0.2%)\r\n8430 ( 1.6%)\r\n\r\n0\r\n(0.0%)\r\n\r\nEn la tabla resumen puede verse que la mayor铆a (91,4%) de las\r\noperaciones se concentran en UK,y en menos medida en Alemania y Francia.\r\nAdem谩s, podemos ver la que la fecha de la factura (InvoiceDate) es una\r\nvariable texto con formato m/dd/yyyy hh:mm. Tambi茅n debe destacarse el\r\nhecho de que existen cantidades vendidas con valores menores a 0.\r\nConsiderando lo anterior transformamos la variable InvoiceDate a tipo\r\nDate (yyyy-mm-dd), excluimos cualquier valor n煤merico menor a 0 y\r\nseleccionamos los datos para los primeros 3 pa铆ses, es decir UK,\r\nAlemania y Francia.\r\nLuego, agrupamos por pais, Date y CustomerID las cantidades vendidas\r\ny el monto de las mismas.\r\n\r\n\r\nShow code\r\n\r\ndf <- df %>%\r\n  mutate(Date= as_date(lubridate::mdy_hm(InvoiceDate)))%>%\r\n  group_by(Country,Date,CustomerID)%>%\r\n  summarise(Quantity= sum(Quantity),\r\n            Monetary= sum(UnitPrice)) %>%\r\n  filter(Country %in% c(\"United Kingdom\",\"Germany\",\"France\")) %>%\r\n  filter_if(is.numeric, ~ .x > 0)\r\n\r\nDT::datatable(df,filter= 'top',options = list(pageLength = 5, dom = 'tip'))\r\n\r\n\r\n\r\n\r\nEn la tabla podemos visualizar como nos qued贸 conformado el\r\ndataset.\r\nImplementamos el modelo \r\nEsta etapa est谩 compuesta por tres subetapas: a) normalizaci贸n de la\r\ntabla, 2) creaci贸n de los puntajes RFM, 3) creaci贸n de los\r\nsegmentos.\r\nNormalizaci贸n de la tabla 锔\r\nVamos a normalizar los nombres de nuestro dataset para hacerlos\r\ncoincidir con los par谩metros con los que la funci贸n\r\nrfm_category asigna los puntajes. Para ello usaremos la\r\nnormalize_table que recibe el dataset con los actuales nombres\r\nde los par谩metros.\r\n\r\n\r\nShow code\r\n\r\ndf <- normalize_table(df = df,\r\n                date = \"Date\",\r\n                id_costumer = \"CustomerID\",\r\n                cantidad = \"Quantity\",\r\n                monto = \"Monetary\")\r\n\r\nDT::datatable(df,filter= 'top',options = list(pageLength = 5, dom = 'tip'))\r\n\r\n\r\n\r\n\r\n\r\nCreamos los puntajes RFM 锔\r\nLa funci贸n rfm_category, admite 5 par谩metros:\r\ndf: el dataset a procesar\r\nfecha_analisis: representa la fecha de\r\nimplementaci贸n del modelo y es 煤til para determinar los d铆as desde la\r\n煤ltima venta (recency).\r\nbins: nos permite indicar el n煤mero de segmentos\r\nen los cu谩les queremos cortar nuestra poblaci贸n.\r\ngroup_by: nos permite utilizar distintos niveles\r\nde agregaci贸n. Como el CostumerID puede repetirse entre los pa铆ses, con\r\neste par谩metro nos aseguramos que ese valor sea 煤nico.\r\ninherits.threshold: por defecto es NULL. Este\r\npar谩metro nos permite customizar los umbrales para la asignaci贸n de\r\npuntajes. Por ejemplo podr铆amos utilizar para los comercios los umbrales\r\ndel pa铆s y de esa forma estandarizarlos para su comparaci贸n.\r\nA su vez la funci贸n nos devuelve:\r\nresultado_rfm: una tabla que contiene el\r\nscoring\r\nheatmap: un gr谩fico que muestra el promedio del\r\nmonto para las combinaciones de recency y de frequency.\r\nthreshold: una tabla con los umbrales para\r\nrecency, frequency y monetary.\r\nSin m谩s pre谩mbulos, miremos el resultado de la funci贸n.\r\n\r\n\r\nShow code\r\n\r\ntabla_rfm <- df %>%\r\n  mutate(email= \"cliente@rfm.com\")%>%\r\n  split(.$Country) %>%\r\n  map(~ rfm_category(df = .,\r\n                     fecha_analisis = '2011-12-09',\r\n                     bins = 4,\r\n                     group_by = \"Country\",\r\n                     inherits.threshold = NULL))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nxaringanExtra::use_panelset()\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfor(i in 1:length(tabla_rfm)){\r\n  cat(\"::: {.panel}\\n\")             \r\n  cat(\"##\", unique(tabla_rfm[[i]]$resultado_rfm$Country) , \"{.panel-name}\\n\") \r\n  print(tabla_rfm[[i]]$heatmap)\r\n  cat(\"\\n\")\r\n  print(kableExtra::kbl(tabla_rfm[[i]]$threshold,\r\n                      format.args = list(decimal.mark = ',', big.mark = \".\"), col.names = rep(c(\"lower\",\"upper\"),3),\r\n                      caption = paste0(\"Puntos de corte scoring. \",unique(tabla_rfm[[i]]$resultado_rfm$Country),\".\"))%>%\r\n        kableExtra::add_header_above(c(\"Recency\" = 2, \"Frequency\" = 2, \"Monetary\"= 2),color = \"#191C3C\", bold = T,align = \"center\") %>%   \r\nkableExtra::kable_paper())\r\n  cat(\"\\n\") \r\n  cat(\":::\\n\")\r\n}\r\n\r\n\r\nFrance\r\n\r\nTable 1: Puntos de corte scoring. France.\r\n\r\n\r\n\r\nRecency\r\n\r\n\r\n\r\n\r\nFrequency\r\n\r\n\r\n\r\n\r\nMonetary\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\n0,0\r\n\r\n\r\n11,5\r\n\r\n\r\n1,0\r\n\r\n\r\n211,0\r\n\r\n\r\n0,010\r\n\r\n\r\n79,125\r\n\r\n\r\n11,5\r\n\r\n\r\n32,0\r\n\r\n\r\n211,0\r\n\r\n\r\n446,0\r\n\r\n\r\n79,125\r\n\r\n\r\n186,770\r\n\r\n\r\n32,0\r\n\r\n\r\n111,0\r\n\r\n\r\n446,0\r\n\r\n\r\n1.674,5\r\n\r\n\r\n186,770\r\n\r\n\r\n350,815\r\n\r\n\r\n111,0\r\n\r\n\r\n372,0\r\n\r\n\r\n1.674,5\r\n\r\n\r\n10.924,0\r\n\r\n\r\n350,815\r\n\r\n\r\n2.030,560\r\n\r\nGermany\r\n\r\nTable 1: Puntos de corte scoring. Germany.\r\n\r\n\r\n\r\nRecency\r\n\r\n\r\n\r\n\r\nFrequency\r\n\r\n\r\n\r\n\r\nMonetary\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\n0,00\r\n\r\n\r\n17,25\r\n\r\n\r\n1,0\r\n\r\n\r\n260,0\r\n\r\n\r\n0,0100\r\n\r\n\r\n84,7925\r\n\r\n\r\n17,25\r\n\r\n\r\n32,00\r\n\r\n\r\n260,0\r\n\r\n\r\n610,0\r\n\r\n\r\n84,7925\r\n\r\n\r\n185,5250\r\n\r\n\r\n32,00\r\n\r\n\r\n93,00\r\n\r\n\r\n610,0\r\n\r\n\r\n1.629,5\r\n\r\n\r\n185,5250\r\n\r\n\r\n496,5550\r\n\r\n\r\n93,00\r\n\r\n\r\n373,00\r\n\r\n\r\n1.629,5\r\n\r\n\r\n8.213,0\r\n\r\n\r\n496,5550\r\n\r\n\r\n2.431,2800\r\n\r\nUnited Kingdom\r\n\r\nTable 1: Puntos de corte scoring. United Kingdom.\r\n\r\n\r\n\r\nRecency\r\n\r\n\r\n\r\n\r\nFrequency\r\n\r\n\r\n\r\n\r\nMonetary\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\n0\r\n\r\n\r\n18\r\n\r\n\r\n1\r\n\r\n\r\n155\r\n\r\n\r\n0,0100\r\n\r\n\r\n49,8050\r\n\r\n\r\n18\r\n\r\n\r\n51\r\n\r\n\r\n155\r\n\r\n\r\n366\r\n\r\n\r\n49,8050\r\n\r\n\r\n123,4400\r\n\r\n\r\n51\r\n\r\n\r\n144\r\n\r\n\r\n366\r\n\r\n\r\n946\r\n\r\n\r\n123,4400\r\n\r\n\r\n285,5625\r\n\r\n\r\n144\r\n\r\n\r\n374\r\n\r\n\r\n946\r\n\r\n\r\n69.982\r\n\r\n\r\n285,5625\r\n\r\n\r\n41.377,3300\r\n\r\n\r\n\r\nAqu铆 podemos ver los primeros 10 registros de la tabla RFM para\r\nUK.\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(tabla_rfm[[3]]$resultado_rfm %>% head(10),filter= 'top',options = list(pageLength = 5, dom = 'tip'))\r\n\r\n\r\n\r\n\r\n驴C贸mo\r\nse asignan los puntajes? 驴Que representan esos valores? \r\nC贸mo se puede observar en la tabla anterior, al dataset se le han\r\na帽adido 3 columnas (recency_cut, frequency_cut y monetary_cut), con\r\nvalores de 1 a 4 1(seg煤n la cantidad de cortes o bins\r\nelegidos podr铆a ser 5), que deben interpretarse del siguiente modo:\r\nrecency: la puntuaci贸n se genera asignando a los\r\nclientes con las fechas de ventas m谩s recientes la m谩xima puntuaci贸n (4\r\nen este caso), y aquellos con fechas de ventas m谩s distante reciben una\r\nclasificaci贸n de actualidad de 1.\r\nfrequency: a los usuarios con mayor cantidad de\r\nunidades vendidas se les asigna una puntuaci贸n m谩s alta (4) y a los de\r\nmenor cantidad una puntuaci贸n de 1.\r\nmonetary: se asigna sobre la base del monto total de\r\nlas ventas al usuario en el per铆odo considerado para el an谩lisis. A los\r\nclientes con mayores montos de venta se les asigna una puntuaci贸n m谩s\r\nalta, mientras que a los que tienen montos de venta m谩s bajos se les\r\nasigna una puntuaci贸n de 1.\r\nLos heatmap, confirman -aunque en menor medida para Francia-\r\nque los montos promedios m谩s altos povienen que los usuarios con mayor\r\ncantidad de bienes vendidos y con recientes operaciones.\r\nCuando se implement贸 el modelo con bins= 5, para Francia y Alemania\r\nse obtuvo un gr谩fico inconsistente (no completo todos sus casilleros),\r\npor lo que se decidi贸 utilizar 4 cortes.\r\nCreamos los segmentos 锔\r\nPara la creaci贸n de los segmentos utilizaremos la funci贸n\r\nsegment_rfm, a la cu谩l debemos pasarle 3 grupos de\r\npar谩metros:\r\ntabla_rfm: es la tabla resultante de la funci贸n\r\nanterior y contiene cada uno de nuestros costumer con un puntaje de\r\nrecency, frequency y monetary asignado.\r\nnombres_segmentos: es un vector de datos con la\r\ncantidad de nombres comos segmentos queramos utilizar. En nuestro caso\r\nutilizaremos 7 segmentos (incluyendo los sin clasificar) para hacerlo\r\nm谩s comprensible.\r\numbrales de los segmentos: llevan los nombres\r\nrecency_lower, recency_upper, frequency_lower, frequency_upper,\r\nmonetary_lower, monetary_upper y son vectores que contienen los umbrales\r\ncon los cu谩les agrupamos en segmentos los scoring rfm.\r\nEsta funci贸n nos retorna:\r\ntabla_rfm: que contiene cada uno de nuestros\r\ncustomer segmentados\r\nbar_chart: gr谩fico de barras que contabiliza los\r\ncustomer seg煤n el puntaje de recency, frequency y monetary. Representa\r\nuna estrategia visual para observar la composici贸n de nuestros\r\nclientes.\r\nComposicion_segmento: muestra la frecuencia absoluta\r\ny relativa de los customer seg煤n los segmentos.\r\ntreemap_segmentos : gr谩fico treemap con la\r\ncomposici贸n por segmentos.\r\nimpact_segment:contabiliza en t茅rminos absolutos y\r\nrelativos el impacto que tienen los segmentos sobre las ventas y el\r\nmonto en el pa铆s.\r\nComenzamos\r\npor definir los segmentos y los umbrales \r\nComo se mencion贸 anteriormente trabajaremos con los siguientes\r\nsegmentos:\r\nChampions: es el segmento que re煤ne el m谩ximo\r\npuntaje en los tres par谩metros. Para las ventas en UK este segmento\r\nconcentra el 43% de las mismas y el 40% del monto facturado.\r\nLoyalist: son los clientes que tienen el m谩ximo\r\npuntaje en la cantidad de productos comprados. Representan el segundo\r\nsegmento en t茅rminos de monto facturado.\r\nBig Spenders: este segmento tiene el m谩ximo\r\npuntaje en los montos de compras. Si bien en t茅rmino de cantidad\r\nadquirida no son influyente, respresentan el tercer segmento en monto\r\nfacturado.\r\nPromising: este segmento muestra puntajes de RFM\r\npor encima del resto, pero por debajo de los champions.\r\nEst谩n t铆midos pero son sensibles de estimular.\r\nNew costumer: tienen alto puntaje en recency, es\r\ndecir han realizado compras recientes aunque no por mucha cantidad ni de\r\nmontos altos. Debemos estimularlos.\r\nHibernating: muestran los puntajes m谩s bajos en\r\nlas tres categor铆as.\r\nAsignamos los umbrales\r\nrecency_lower : 4,1,1,2,4,1\r\nrecency_upper: 4,4,4,4,4,1\r\nfrequency_lower: 4,4,1,2,1,1\r\nfrequency_upper: 4,4,4,4,4,1\r\nmonetary_lower: 4,1,4,2,1,1\r\nmonetary_upper: 4,4,4,4,4,1\r\n\r\n\r\nShow code\r\n\r\nnombres_segmentos <- c(\"Champions\",\"Loyalist\",\"Big Spenders\",\r\n                       \"Promising\",\"New Customers\",\"Hibernating\")\r\nrecency_lower <-   c(4,1,1,2,4,1)\r\nrecency_upper <-   c(4,4,4,4,4,1)\r\nfrequency_lower <- c(4,4,1,2,1,1)\r\nfrequency_upper <- c(4,4,4,4,4,1)\r\nmonetary_lower <-  c(4,1,4,2,1,1)\r\nmonetary_upper <-  c(4,4,4,4,4,1)\r\n\r\nrfm <- list()\r\n\r\nfor(i in 1:length(tabla_rfm)){\r\n  rfm[[i]] <- segment_rfm(tabla_rfm = tabla_rfm[[i]],\r\n                          nombres_segmentos = nombres_segmentos,\r\n                          recency_lower,\r\n                          recency_upper,\r\n                          frequency_lower,\r\n                          frequency_upper,\r\n                          monetary_lower,\r\n                          monetary_upper\r\n  )\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfor(i in 1:length(rfm)){\r\n\r\n  cat(\"::: {.panel}\\n\")             \r\n\r\n  cat(\"###\", unique(rfm[[i]]$tabla_rfm$Country), \"{.panel-name}\\n\")\r\ncat(\"\\n\")\r\ncat(\"#### Distribuci贸n de los clientes seg煤n el scoring (RFM\")\r\nprint(rfm[[i]]$bar_chart)\r\ncat(\"\\n\")\r\ncat(\"#### Composici贸n de los segmentos\")\r\ncat(\"\\n\")\r\n\r\nprint(kableExtra::kbl(rfm[[i]]$Composicion_segmento,\r\n\r\n        format.args = list(decimal.mark = ',', big.mark = \".\")) %>%\r\n\r\n     kableExtra::kable_paper())\r\n\r\n cat(\"\\n\")\r\n cat(\"#### Gr谩fico composici贸n de los segmentos\")\r\n print(rfm[[i]]$treemap_segmentos)\r\n cat(\"\\n\")\r\n\r\n cat(\"#### Impacto de los segmentos en las Ventas y el Monto\")\r\n\r\n  cat(\"\\n\")\r\n\r\n  print(kableExtra::kbl(rfm[[i]]$impact_segment,\r\n        col.names = c(\"Segmentos\",\"Ventas\",\"%\",\"Monto\",\"%\"),\r\n                      \r\n        format.args = list(decimal.mark = ',', big.mark = \".\")) %>%\r\n        \r\n        kableExtra::kable_paper())\r\n\r\n  cat(\"\\n\")\r\n\r\n  cat(\":::\\n\")\r\n\r\n}\r\n\r\n\r\nFrance\r\nDistribuci贸n\r\nde los clientes seg煤n el scoring\r\n(RFM\r\nComposici贸n de los segmentos\r\n\r\nsegmento\r\n\r\n\r\ncantidad\r\n\r\n\r\n%\r\n\r\n\r\nPromising\r\n\r\n\r\n31\r\n\r\n\r\n35,6\r\n\r\n\r\nUsuals\r\n\r\n\r\n18\r\n\r\n\r\n20,7\r\n\r\n\r\nChampions\r\n\r\n\r\n11\r\n\r\n\r\n12,6\r\n\r\n\r\nHibernating\r\n\r\n\r\n11\r\n\r\n\r\n12,6\r\n\r\n\r\nLoyalist\r\n\r\n\r\n11\r\n\r\n\r\n12,6\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n3\r\n\r\n\r\n3,4\r\n\r\n\r\nNew Customers\r\n\r\n\r\n2\r\n\r\n\r\n2,3\r\n\r\nGr谩fico composici贸n de los\r\nsegmentos\r\nImpacto de\r\nlos segmentos en las Ventas y el Monto\r\n\r\nSegmentos\r\n\r\n\r\nVentas\r\n\r\n\r\n%\r\n\r\n\r\nMonto\r\n\r\n\r\n%\r\n\r\n\r\nLoyalist\r\n\r\n\r\n47.400\r\n\r\n\r\n42,9\r\n\r\n\r\n9.095,71\r\n\r\n\r\n33,0\r\n\r\n\r\nChampions\r\n\r\n\r\n35.434\r\n\r\n\r\n32,1\r\n\r\n\r\n8.474,69\r\n\r\n\r\n30,7\r\n\r\n\r\nPromising\r\n\r\n\r\n16.877\r\n\r\n\r\n15,3\r\n\r\n\r\n5.988,87\r\n\r\n\r\n21,7\r\n\r\n\r\nUsuals\r\n\r\n\r\n5.384\r\n\r\n\r\n4,9\r\n\r\n\r\n1.651,01\r\n\r\n\r\n6,0\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n4.012\r\n\r\n\r\n3,6\r\n\r\n\r\n1.877,45\r\n\r\n\r\n6,8\r\n\r\n\r\nHibernating\r\n\r\n\r\n979\r\n\r\n\r\n0,9\r\n\r\n\r\n419,30\r\n\r\n\r\n1,5\r\n\r\n\r\nNew Customers\r\n\r\n\r\n381\r\n\r\n\r\n0,3\r\n\r\n\r\n88,94\r\n\r\n\r\n0,3\r\n\r\nGermany\r\nDistribuci贸n de\r\nlos clientes seg煤n el scoring\r\n(RFM\r\nComposici贸n de los\r\nsegmentos\r\n\r\nsegmento\r\n\r\n\r\ncantidad\r\n\r\n\r\n%\r\n\r\n\r\nUsuals\r\n\r\n\r\n30\r\n\r\n\r\n31,9\r\n\r\n\r\nPromising\r\n\r\n\r\n24\r\n\r\n\r\n25,5\r\n\r\n\r\nLoyalist\r\n\r\n\r\n13\r\n\r\n\r\n13,8\r\n\r\n\r\nChampions\r\n\r\n\r\n11\r\n\r\n\r\n11,7\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n7\r\n\r\n\r\n7,4\r\n\r\n\r\nHibernating\r\n\r\n\r\n7\r\n\r\n\r\n7,4\r\n\r\n\r\nNew Customers\r\n\r\n\r\n2\r\n\r\n\r\n2,1\r\n\r\nGr谩fico composici贸n de\r\nlos\r\nsegmentos\r\nImpacto de\r\nlos segmentos en las Ventas y el Monto\r\n\r\nSegmentos\r\n\r\n\r\nVentas\r\n\r\n\r\n%\r\n\r\n\r\nMonto\r\n\r\n\r\n%\r\n\r\n\r\nChampions\r\n\r\n\r\n43.710\r\n\r\n\r\n36,7\r\n\r\n\r\n11.334,46\r\n\r\n\r\n32,7\r\n\r\n\r\nLoyalist\r\n\r\n\r\n38.720\r\n\r\n\r\n32,5\r\n\r\n\r\n9.612,45\r\n\r\n\r\n27,8\r\n\r\n\r\nPromising\r\n\r\n\r\n15.851\r\n\r\n\r\n13,3\r\n\r\n\r\n4.867,00\r\n\r\n\r\n14,1\r\n\r\n\r\nUsuals\r\n\r\n\r\n11.791\r\n\r\n\r\n9,9\r\n\r\n\r\n3.103,82\r\n\r\n\r\n9,0\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n7.655\r\n\r\n\r\n6,4\r\n\r\n\r\n5.212,01\r\n\r\n\r\n15,1\r\n\r\n\r\nHibernating\r\n\r\n\r\n1.020\r\n\r\n\r\n0,9\r\n\r\n\r\n351,03\r\n\r\n\r\n1,0\r\n\r\n\r\nNew Customers\r\n\r\n\r\n430\r\n\r\n\r\n0,4\r\n\r\n\r\n147,49\r\n\r\n\r\n0,4\r\n\r\nUnited Kingdom\r\nDistribuci贸n de\r\nlos clientes seg煤n el scoring\r\n(RFM\r\nComposici贸n de los\r\nsegmentos\r\n\r\nsegmento\r\n\r\n\r\ncantidad\r\n\r\n\r\n%\r\n\r\n\r\nUsuals\r\n\r\n\r\n1.205\r\n\r\n\r\n30,8\r\n\r\n\r\nPromising\r\n\r\n\r\n963\r\n\r\n\r\n24,6\r\n\r\n\r\nLoyalist\r\n\r\n\r\n603\r\n\r\n\r\n15,4\r\n\r\n\r\nChampions\r\n\r\n\r\n375\r\n\r\n\r\n9,6\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n345\r\n\r\n\r\n8,8\r\n\r\n\r\nHibernating\r\n\r\n\r\n289\r\n\r\n\r\n7,4\r\n\r\n\r\nNew Customers\r\n\r\n\r\n136\r\n\r\n\r\n3,5\r\n\r\nGr谩fico composici贸n de\r\nlos\r\nsegmentos\r\nImpacto de\r\nlos segmentos en las Ventas y el Monto\r\n\r\nSegmentos\r\n\r\n\r\nVentas\r\n\r\n\r\n%\r\n\r\n\r\nMonto\r\n\r\n\r\n%\r\n\r\n\r\nChampions\r\n\r\n\r\n1.768.772\r\n\r\n\r\n43,6\r\n\r\n\r\n439.585,40\r\n\r\n\r\n39,9\r\n\r\n\r\nLoyalist\r\n\r\n\r\n1.372.598\r\n\r\n\r\n33,8\r\n\r\n\r\n217.268,88\r\n\r\n\r\n19,7\r\n\r\n\r\nPromising\r\n\r\n\r\n422.793\r\n\r\n\r\n10,4\r\n\r\n\r\n136.777,67\r\n\r\n\r\n12,4\r\n\r\n\r\nUsuals\r\n\r\n\r\n259.241\r\n\r\n\r\n6,4\r\n\r\n\r\n87.422,87\r\n\r\n\r\n7,9\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n189.988\r\n\r\n\r\n4,7\r\n\r\n\r\n205.524,46\r\n\r\n\r\n18,7\r\n\r\n\r\nNew Customers\r\n\r\n\r\n23.664\r\n\r\n\r\n0,6\r\n\r\n\r\n8.008,58\r\n\r\n\r\n0,7\r\n\r\n\r\nHibernating\r\n\r\n\r\n21.020\r\n\r\n\r\n0,5\r\n\r\n\r\n6.672,41\r\n\r\n\r\n0,6\r\n\r\n\r\n\r\n驴Que podemos decir de los\r\nsegmentos? \r\n Los patrones y tendencias son m谩s f谩cil de obervar en UK en raz贸n\r\nde una mayor cantidad de observaciones. El bar_chart que\r\ncombina los scoring de los tres par谩metros muestra que los clientes que\r\nrealizan compras de mayor valor adem谩s tienen altos puntajes de recency\r\ny de frequency (esquina superior derecha). En el opuesto, los que tienen\r\nmenores puntajes de monetary coincide con los menores tambi茅n de recency\r\ny frequency. Esto se da tambi茅n en los clientes de Alemania y Francia\r\npero con menos nitidez.\r\nUn punto interesante es el segmento Usuals, que representa\r\na los clientes no clasificados en los umbrales y que es el principal\r\nsegmento de UK y Alemania. El porcentaje de este segmento est谩\r\ndirectamente vinculado a los objetivos de la compa帽铆a en relaci贸n a las\r\npol铆ticas de fidelizaci贸n y en t茅rminos estad铆sticos a la\r\nsensibilidad y especificidad con la que creamos los\r\numbrales. En nuestro caso. si tom谩ramos umbrales m谩s amplios lograr铆amos\r\ndiminuir ese conjunto de cliente (usuals) pero corremos el riesgo de por\r\nejemplo clasificar como Champions clientes que no lo son o viceversa,\r\ncomo Hibernating a clientes que est谩n activos. Tambi茅n podr铆amos generar\r\nnuevos segmentos para incluir este grupo de clientes. Esta es una\r\ndiscusi贸n por dem谩s interesante que requiere de un dialogo fluido entre\r\nla estad铆stica y el marketing.\r\nSalvo para Francia, los segmentos Champions y Loyalist concentran\r\nla mayor parte de las ventas y la facturaci贸n (Monto). Los Big Spenders\r\nen estos pa铆ses representan el tercer segmento en cuanto a facturaci贸n.\r\nLos Promising son una oportunidad para el negocio ya que son sensibles a\r\npromociones y estrategias de marketing.\r\nComentarios finales \r\nSon varias cosas las que me surgen para este apartado final pero\r\nvoy a abocarme s贸lo a algunas de ellas. Por un lado, la oportunidad que\r\nrepresentan los an谩lisis RFM para el di谩logo entre la estad铆stica, los\r\ndatos y el apoyo a los decision makers.\r\nPor otro lado,en este post he mostrado 3 funciones desarrolladas\r\n铆ntegramente en R que pueden adaptarse para su uso en distintos\r\nescenarios y que cuentan con la posibilidad de ajustar los par谩metros\r\ncentrales para lograr robustez en los resultados.\r\nSin dudas hay mucho camino por recorrer a煤n en el mundo de los\r\ndatos., as铆 que sigamos viajando锔.\r\n\r\nla cantidad de cortes seleccionados\r\ndivide a la poblaci贸n en partes iguales. Si seleccionamos 4 cortes,\r\ndividimos a la poblaci贸n en cuartiles cada uno de los cu谩les representa\r\nel 25%. Si decidi茅ramos utilizar 5 cortes, dividir铆amos la poblaci贸n en\r\nquintiles, representando cada uno un 20%.╋\r\n",
    "preview": "posts/2022-08-27-analisis-rfm-para-la-segmentacion-de-clientes/imagenes/cluster.jpg",
    "last_modified": "2023-05-04T14:35:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-26-consumo-de-geoservicios-con-r-y-su-uso-para-la-gestin-local/",
    "title": "Consumo de geoservicios con R y su uso para la gesti贸n local",
    "description": "En este post voy a mostrar el uso de algunas librerias de R para el consumo de geoservicios. Adem谩s, veremos como visualizar las capas de geoservicios con leaflet y haremos algunas operaciones para apoyar la gesti贸n local. Espero que este mundo los atrape tanto como a m铆.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-08-26",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nContexto\r\n驴Que es un IDE?\r\n\r\n\r\nA los datos\r\nMapeamos\r\nlas capas\r\nTiempo\r\nde ejercicio 锔\r\nMetodolgia Ь\r\nExtraemos la informacion\r\nde las capas\r\nHomogeneizamos las\r\nproyecciones\r\nComentarios finales \r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(sf)\r\nlibrary(leaflet)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(purrr)\r\nlibrary(ggplot2)\r\nlibrary(leaflet.extras2)\r\nlibrary(osrm)\r\n\r\n\r\n\r\nContexto\r\nEn muchas 谩reas de la vida social, econ贸mica, cultural, etc, la\r\ncomprensi贸n del espacio o lo espacial se vuelve fundamental para la\r\ncomprensi贸n de ciertas din谩micas o tendencias. En el 谩rea de salud esto\r\nse hace m谩s evidente ya que en gran parte la salud de un individuo o de\r\nuna comunidad est谩 determinada por como esta configurado este espacio\r\nque lo rodea.\r\nPor eso aqu铆 vamos a trabajar un ejemplo del uso de los\r\ngeoservicios para la gesti贸n local en salud, pero 驴que\r\nson los geoservicios?.\r\nVamos a decir que son un Servicio Web espec铆fico que permite\r\nintercambiar informaci贸n 煤nicamente de componente geogr谩fica. Para la\r\ngeneraci贸n y utilizaci贸n de los estos se utilizan lenguajes espec铆ficos\r\ny protocolos est谩ndares definidos.\r\n驴Que es un IDE? \r\nEs la sigla para representar el t茅rmino Infraestructura de Datos\r\nEspaciales, que cumple la funci贸n de permitir:\r\n\r\nacceder a datos, productos y servicios geoespaciales, publicados en\r\ninternet bajo est谩ndares y normas definidos, asegurando su\r\ninteroperabilidad y uso, como as铆 tambi茅n la propiedad sobre la\r\ninformaci贸n por parte de los organismos que la publican y su\r\nresponsabilidad en la actualizaci贸n (IDERA)\r\n\r\nEn Argentina, este rol lo cumple IDERA, una comunidad geoespacial que\r\ninvolucra actores estatales, de la sociedad civil y privados.\r\nA los datos\r\nLuego de la peque帽a intro vamos a los datos. El ejemplo que\r\nutilizaremos para revisar el uso de las librerias corresponde a los\r\ngeoservicios de la ciudad de Gualeguaych煤, que adem谩s de ser mi ciudad\r\nde nacimiento, recientemente ha disponibilizado esta informaci贸n para la\r\ncomunidad en el siguiente link.\r\nRealizaremos la lectura del servicio con la libreria sf, la\r\nla usaremos la lo largo de este post ya que nos permite una amplia gama\r\nde operaciones. Es importante saber que existen dos tipos de servicios\r\nWMS y WFS 1.\r\n\r\n\r\nShow code\r\n\r\n#Guardamos la URL en un objeto\r\n\r\ngeo_gchu <- \"https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\"\r\n\r\n#Exploramos las capas\r\n\r\nlayers <- st_layers(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\")\r\nhead(layers$name,10)\r\n\r\n\r\n##  [1] \"gis:AG_Control\"                         \r\n##  [2] \"gis:AG_Nodos\"                           \r\n##  [3] \"gis:AG_Symbols\"                         \r\n##  [4] \"gis:AG_Tramos\"                          \r\n##  [5] \"gis:aerodromo\"                          \r\n##  [6] \"gis:aerodromo_poligono\"                 \r\n##  [7] \"gis:turismo_alojamientos\"               \r\n##  [8] \"gis:antenas\"                            \r\n##  [9] \"gis:antiguos_nombres_de_calles\"         \r\n## [10] \"gis:area_de_fabricacion_y_procesamiento\"\r\n\r\nLa funci贸n st_layers de sf nos\r\npermite explorar todas las capas disponibles en el geoservicio. En\r\nnuestro caso tomaremos 3:\r\nEstablecimientos de Salud (gis:est_salud)\r\nAreas program谩ticas (gis:areas_programaticas)\r\nRadios Censales (gis:radios_censales)\r\nMapeamos las capas\r\nMiremos ahora  las capas en un mapa, y usaremos en esta oportunidad\r\nla librer铆a leaflet que nos permite acceder a mapas\r\ninteractivos muy poderosos. Adem谩s, utilizaremos la libreria\r\nleaflet.extras2 que nos da la posibilidad de ver\r\ncaracter铆sticas extras a nuestras capas.\r\nUsaremos Argenmapcomo\r\ntesela 2 base y luego iremos agregando las\r\nrespectivas capas. Adem谩s, generaremos un control de capas que nos va a\r\npermitir seleccionar cu谩l o cu谩les queremos ver.\r\n\r\n\r\nShow code\r\n\r\nleaflet() %>%\r\n  addTiles(urlTemplate = \"https://wms.ign.gob.ar/geoserver/gwc/service/tms/1.0.0/mapabase_gris@EPSG%3A3857@png/{z}/{x}/{-y}.png\",\r\n           tileOptions(tms = TRUE,maxZoom = 14), attribution = '<a target=\"_blank\" href=\"https://www.ign.gob.ar/argenmap/argenmap.jquery/docs/#datosvectoriales\" style=\"color: black; text-decoration: underline; font-weight: normal;\">Datos IGN Argentina // OpenStreetMap<\/a>',\r\n           group = \"Argenmap\") %>% #Aqu铆 agregamos Argenmap\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"radios_censales\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Radios Censales\"\r\n  ) %>% #Agregamos layer de Radios Censales\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"areas_programaticas\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Areas Program谩ticas\"\r\n  )  %>% #Agregamos layer de Areas Program谩ticas\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"est_salud\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Est de salud\"\r\n  ) %>% #Agregamos layer de Establecimientos de Salud\r\n   addLayersControl(\r\n    baseGroups = \"Argenmap\",\r\n    overlayGroups = c(\"Areas Program谩ticas\",\"Est de salud\",\"Radios Censales\"),\r\n    options = layersControlOptions(collapsed = FALSE)) %>%\r\n  setView(lng = -58.52597, lat = -33.00606,zoom = 11)\r\n\r\n\r\n\r\n\r\nSi cliquean sobre el 谩rea program谩tica pueden obtener informaci贸n del\r\nresponsable del 谩rea, ubicaci贸n, tel茅fono. Si en cambio cliquean el\r\nradio censal les devuelve informaci贸n de la cantidad de hogares,\r\npersonas e incluso el 谩rea .\r\nTiempo de ejercicio 锔\r\nEl acceso al sistema de salud puede ser abordado desde distintas\r\nperspectivas, barreras culturales, geogr谩ficas, presencia de medios de\r\ntransporte, etc.\r\nEn este ejercicio intentar茅 abordar la accesibilidad que tiene cada\r\n谩rea program谩tica a un centro de salud, para poder determinar en\r\nt茅rminos de tiempo y distancia cu谩l tiene una mayor accesibilidad y cu谩l\r\nmenor.\r\nAntes de avanzar, cu谩ndo hablamos de 谩rea\r\nprogram谩tica hacemos referencia al 谩rea de influencia de un\r\ndeterminado centro asistencial que se expresa, aunque no 煤nicamente, en\r\nun pol铆gono geogr谩fico. La idea central es que las\r\npersonas que viven en esa 谩rea se referencian con el centro de salud o\r\nlos centros de salud contenidos en 茅sta 3.\r\nMetodolgia Ь\r\nLa metodolog铆a que voy a usar para determinar la distancia por 谩rea\r\nprogram谩tica al centro de salud, es asignar los radios censales a cada\r\nuno de los poligonos que representan el 谩rea program谩tica.\r\nLuego, voy a calcular los centroides de cada radio censal y voy a\r\nmedir la distancia en tiempo (a pie) y en kil贸metros hasta el centro o\r\nlos centros de salud que contiene el area.\r\nResulta un poco confuso , veamos el c贸digo.\r\nExtraemos la informacion\r\nde las capas\r\n\r\n\r\nShow code\r\n\r\n#Extraemos las areas programaticas\r\nareas_programaticas <- st_read(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\",\"gis:areas_programaticas\")\r\n\r\n\r\n## Reading layer `gis:areas_programaticas' from data source \r\n##   `WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities' \r\n##   using driver `WFS'\r\n## Simple feature collection with 9 features and 5 fields\r\n## Geometry type: MULTISURFACE\r\n## Dimension:     XY\r\n## Bounding box:  xmin: 5628553 ymin: 6342554 xmax: 5641830 ymax: 6350683\r\n## Projected CRS: POSGAR 98 / Argentina 5\r\n\r\nShow code\r\n\r\n#Extraemos los establecimientos de salud\r\n\r\nest_salud <- st_read(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\", \"gis:est_salud\")\r\n\r\n\r\n## Reading layer `gis:est_salud' from data source \r\n##   `WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities' \r\n##   using driver `WFS'\r\n## Simple feature collection with 11 features and 17 fields\r\n## Geometry type: MULTIPOINT\r\n## Dimension:     XY\r\n## Bounding box:  xmin: -58.54233 ymin: -33.03131 xmax: -58.50246 ymax: -32.99495\r\n## Geodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\n#Extraemos los radios censales\r\n\r\nradios_censales <- st_read(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\",\"gis:radios_censales\")\r\n\r\n\r\n## Reading layer `gis:radios_censales' from data source \r\n##   `WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities' \r\n##   using driver `WFS'\r\n## Simple feature collection with 95 features and 10 fields\r\n## Geometry type: MULTISURFACE\r\n## Dimension:     XY\r\n## Bounding box:  xmin: 5625130 ymin: 6334522 xmax: 5675083 ymax: 6353581\r\n## Projected CRS: POSGAR 98 / Argentina 5\r\n\r\nShow code\r\n\r\nradios_censales <- radios_censales %>% \r\n  filter(!gml_id %in% c(\"radios_censales.2\",\"radios_censales.11\"))\r\n\r\n\r\n\r\nHomogeneizamos las\r\nproyecciones\r\n\r\n\r\nShow code\r\n\r\n#homogeneizamos las proyecciones\r\n\r\n# Centros de salud\r\ncsalud <- est_salud %>% st_drop_geometry()\r\n\r\ncsalud <- st_as_sf(csalud[c(\"fid\",\"nombre\",\"longitud\",\"latitud\")], coords = c(\"longitud\", \"latitud\"), \r\n         crs = 4326, remove= FALSE) %>% st_transform(crs = 4326)\r\n\r\n\r\nest_salud <- st_transform(est_salud, 4326)\r\n\r\n# Areas programaticas\r\n\r\nareas_programaticas <- st_transform(areas_programaticas, 4326)\r\n\r\nareas_programaticas <- st_cast(areas_programaticas, \"GEOMETRYCOLLECTION\") %>% st_collection_extract(\"POLYGON\")\r\n\r\n# Radios censales\r\nradios_censales <- st_transform(radios_censales, 4326)\r\n\r\nradios_censales <- st_cast(radios_censales, \"GEOMETRYCOLLECTION\") %>% st_collection_extract(\"POLYGON\")\r\n\r\n\r\n\r\nCalculamos los\r\ncentroides y los visualizamos\r\nPrimero utilizamos la funci贸n st_centroid de\r\nsf y lo agregamos como layers a nuestro mapa.\r\n\r\n\r\nShow code\r\n\r\nradios_censales$centroides <- radios_censales %>%  st_centroid() %>%\r\n  st_geometry() \r\n\r\ncentroides <- radios_censales %>%\r\n  select(gml_id)%>%\r\n  st_centroid() %>%\r\n  st_as_sf()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nleaflet() %>%\r\n  addTiles(urlTemplate = \"https://wms.ign.gob.ar/geoserver/gwc/service/tms/1.0.0/mapabase_gris@EPSG%3A3857@png/{z}/{x}/{-y}.png\",\r\n           tileOptions(tms = TRUE,maxZoom = 14), attribution = '<a target=\"_blank\" href=\"https://www.ign.gob.ar/argenmap/argenmap.jquery/docs/#datosvectoriales\" style=\"color: black; text-decoration: underline; font-weight: normal;\">Datos IGN Argentina // OpenStreetMap<\/a>',\r\n           group = \"Argenmap\") %>% #Aqu铆 agregamos Argenmap\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"radios_censales\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Radios Censales\"\r\n  ) %>% #Agregamos layer de Radios Censales\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"areas_programaticas\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Areas Program谩ticas\"\r\n  )  %>% #Agregamos layer de Areas Program谩ticas\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"est_salud\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Est de salud\"\r\n  ) %>% #Agregamos layer de Establecimientos de Salud\r\n  addCircles(data= centroides,group = \"centroides\",color = \"black\")%>% #Agregamos el circulo con los centroides\r\n   addLayersControl(\r\n    baseGroups = \"Argenmap\",\r\n    overlayGroups = c(\"Areas Program谩ticas\",\"Est de salud\",\"Radios Censales\",\"centroides\"),\r\n    options = layersControlOptions(collapsed = FALSE)) %>%\r\n  setView(lng = -58.52597, lat = -33.00606,zoom = 11)\r\n\r\n\r\n\r\n\r\nHacemos los joins \r\nEn esta etapa vamos a asignar los centros de salud y\r\nlos radios censales a cada 谩rea program谩tica, para\r\nluego poder hacer los c谩lculos. Utilizamos la funci贸n\r\nst_join de sf.\r\n\r\n\r\nShow code\r\n\r\njoin <- st_join(areas_programaticas,csalud,join= st_intersects)\r\n\r\n\r\n\r\nVeamos que los centros de salud han sido asignado a areas\r\nprogramatica\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(join %>% st_drop_geometry() %>% head(5),\r\n              options = list(scrollX=TRUE))\r\n\r\n\r\n\r\n\r\nHacemos la asignaci贸n de los radios censales a las 谩reas\r\nprogram谩ticas.\r\n\r\n\r\nShow code\r\n\r\njoin2 <- st_join(join,radios_censales,join= st_intersects)\r\n\r\n\r\n\r\nRevisamos el resultado.\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(join2 %>% st_drop_geometry() %>% head(5),\r\n               options = list(scrollX=TRUE))\r\n\r\n\r\n\r\n\r\nMedimos las distancias \r\nLlegamos a una etapa por dem谩s interesante, en la que obtendremos las\r\ndistancias entre los centroides y los centros de salud. Para ello,\r\nutilizo el paquete OSRM que se basa en el proyecto que lleva el mismo\r\nnombre y que es un Servicio de enrutamiento basado en datos de\r\nOpenStreetMap. Vale decir, que el resultado de las\r\nmediciones es la distancia en tiempo a pie y en kil贸metros.\r\nVale aclarar que en el c贸digo de lectura de los radios censales se\r\nexcluyen los radios 11 (Pueblo Belgrano) y 2, ya que abarcan territorio\r\nmuy por fuera del ejido.\r\nPara tener las distancias, voy a:\r\ncrear un data frame con los puntos de los establecimientos de\r\nsalud y los centroides por area programatica.\r\n\r\n\r\nShow code\r\n\r\n#Armo un dataframe de distancias\r\n\r\ndistancias <- st_join(est_salud %>%\r\n       select(\"fid\",\"nombre\")%>%\r\n       rename(\"geomcs\"=\"geom\"),join2 %>%\r\n         select(\"gml_id.y\",\"centroides\"), join= st_intersects) %>%\r\n  distinct()\r\n\r\n\r\n\r\nseparar el data frame anterior en uno de origen (from) y uno de\r\ndestino (to).\r\n\r\n\r\nShow code\r\n\r\nfrom <- distancias %>%\r\n  select(gml_id.y,centroides)%>%\r\n  st_drop_geometry(geomcs) %>%\r\n  st_as_sf()\r\n\r\nto <- distancias %>%\r\n  select(fid,nombre,geomcs)\r\n\r\n\r\n\r\nobtener las distancias\r\n\r\n\r\nShow code\r\n\r\nrutas <- list()\r\n\r\nfor(i in 1:nrow(from)){\r\n\r\nrutas[[i]] <- osrmRoute(src = from[i,],\r\n          dst = to[i,],\r\n          overview = FALSE)\r\n}\r\n\r\ndf <- do.call(rbind,rutas)\r\n\r\n\r\n\r\nunificar los dataset.\r\n\r\n\r\nShow code\r\n\r\ndistancias_x_cs <- cbind(distancias[,c(1,2)] %>% st_drop_geometry(),df)\r\n\r\n\r\n\r\nMostramos las\r\ndistancias por 谩rea program谩tica\r\n\r\n\r\nShow code\r\n\r\njoin %>%\r\n  st_drop_geometry()%>%\r\n  select(gml_id,fid)%>%\r\n  left_join(distancias_x_cs, by= \"fid\") %>%\r\n  DT::datatable(options = list(pageLength = 25, dom = 'tip'))\r\n\r\n\r\n\r\n\r\nResumen distancias tiempo a\r\npie\r\n\r\n\r\nShow code\r\n\r\njoin %>%\r\n  st_drop_geometry()%>%\r\n  select(gml_id,fid)%>%\r\n  left_join(distancias_x_cs, by= \"fid\") %>%\r\n  group_by(gml_id)%>%\r\n  filter(!is.na(duration)) %>%\r\n  summarise(min= round(min(duration),1),\r\n            prom= round(mean(duration),1),\r\n            mediana= round(median(duration),1),\r\n            max= round(max(duration),1)) %>%\r\n  arrange(prom)%>%\r\n  kableExtra::kbl(format.args = list(big.mark = \".\",\r\n                                     decimal.mark= \",\")) %>%\r\n  kableExtra::kable_classic_2()\r\n\r\n\r\n\r\ngml_id\r\n\r\n\r\nmin\r\n\r\n\r\nprom\r\n\r\n\r\nmediana\r\n\r\n\r\nmax\r\n\r\n\r\nareas_programaticas.4\r\n\r\n\r\n1,0\r\n\r\n\r\n2,5\r\n\r\n\r\n2,1\r\n\r\n\r\n7,2\r\n\r\n\r\nareas_programaticas.12\r\n\r\n\r\n0,8\r\n\r\n\r\n2,6\r\n\r\n\r\n2,5\r\n\r\n\r\n5,4\r\n\r\n\r\nareas_programaticas.11\r\n\r\n\r\n0,6\r\n\r\n\r\n2,8\r\n\r\n\r\n3,0\r\n\r\n\r\n4,3\r\n\r\n\r\nareas_programaticas.3\r\n\r\n\r\n0,1\r\n\r\n\r\n3,0\r\n\r\n\r\n3,0\r\n\r\n\r\n5,2\r\n\r\n\r\nareas_programaticas.9\r\n\r\n\r\n0,4\r\n\r\n\r\n3,7\r\n\r\n\r\n3,8\r\n\r\n\r\n10,1\r\n\r\n\r\nareas_programaticas.8\r\n\r\n\r\n1,1\r\n\r\n\r\n3,9\r\n\r\n\r\n3,9\r\n\r\n\r\n6,5\r\n\r\n\r\nareas_programaticas.2\r\n\r\n\r\n2,1\r\n\r\n\r\n4,6\r\n\r\n\r\n3,6\r\n\r\n\r\n11,9\r\n\r\n\r\nareas_programaticas.5\r\n\r\n\r\n1,0\r\n\r\n\r\n5,1\r\n\r\n\r\n2,8\r\n\r\n\r\n11,9\r\n\r\n\r\nResumen distancias en\r\nkil贸metros\r\n\r\n\r\nShow code\r\n\r\njoin %>%\r\n  st_drop_geometry()%>%\r\n  select(gml_id,fid)%>%\r\n  left_join(distancias_x_cs, by= \"fid\") %>%\r\n  group_by(gml_id)%>%\r\n  filter(!is.na(duration)) %>%\r\n  summarise(min= round(min(distance),1),\r\n            prom= round(mean(distance),1),\r\n            mediana= round(median(distance),1),\r\n            max= round(max(distance),1)) %>%\r\n  arrange(prom)%>%\r\n  kableExtra::kbl(format.args = list(big.mark = \".\",\r\n                                     decimal.mark= \",\")) %>%\r\n  kableExtra::kable_classic_2()\r\n\r\n\r\n\r\ngml_id\r\n\r\n\r\nmin\r\n\r\n\r\nprom\r\n\r\n\r\nmediana\r\n\r\n\r\nmax\r\n\r\n\r\nareas_programaticas.4\r\n\r\n\r\n0,4\r\n\r\n\r\n1,1\r\n\r\n\r\n0,8\r\n\r\n\r\n3,1\r\n\r\n\r\nareas_programaticas.11\r\n\r\n\r\n0,1\r\n\r\n\r\n1,3\r\n\r\n\r\n1,4\r\n\r\n\r\n2,5\r\n\r\n\r\nareas_programaticas.12\r\n\r\n\r\n0,3\r\n\r\n\r\n1,3\r\n\r\n\r\n1,2\r\n\r\n\r\n3,0\r\n\r\n\r\nareas_programaticas.3\r\n\r\n\r\n0,1\r\n\r\n\r\n1,4\r\n\r\n\r\n1,4\r\n\r\n\r\n2,5\r\n\r\n\r\nareas_programaticas.8\r\n\r\n\r\n0,4\r\n\r\n\r\n1,7\r\n\r\n\r\n1,6\r\n\r\n\r\n2,6\r\n\r\n\r\nareas_programaticas.9\r\n\r\n\r\n0,2\r\n\r\n\r\n1,9\r\n\r\n\r\n1,9\r\n\r\n\r\n5,0\r\n\r\n\r\nareas_programaticas.2\r\n\r\n\r\n0,8\r\n\r\n\r\n2,0\r\n\r\n\r\n1,4\r\n\r\n\r\n6,1\r\n\r\n\r\nareas_programaticas.5\r\n\r\n\r\n0,5\r\n\r\n\r\n3,4\r\n\r\n\r\n1,2\r\n\r\n\r\n9,0\r\n\r\n\r\nComentarios finales \r\n锔 En relaci贸n al objetivo principal del ejercicio, puede observase\r\nque el 谩rea program谩tica 4, es la que tiene mayor accesibilidad con un\r\npromedio de 2,5 minutos y 1,1 Km al centro de salud San Francisco. En el\r\nopuesto el 谩rea program谩tica 5 mostr贸 la mayor distancia con un promedio\r\nde 5,1 minutos y 3,4 Km al centro de salud Suburbio Sur. Sin dudas un\r\npromedio de 5 minutos al centro de salud en el 谩rea con menor\r\naccesibilidad da cuentas de una amplia cobertura del sistema de\r\nsalud.\r\n锔En lo que respecta a la metodolog铆a sin dudas muestra limitaciones.\r\nUna de ellas es el hecho de que algunos radios censales caen en mas de\r\nun 谩rea program谩tica. Por ej: el radio censal 13 cae dentro de Villa\r\nMaria, Suburbio Sur, M茅danos. Esto implica que un habitante de esta zona\r\ncuenta con mayor oferta. Otra limitaci贸n es haber tomado los centroides,\r\nse podr铆a mejorar esto tomando puntos al azar dentro de los radios\r\ncensales.\r\n锔Por ultimo espero haber podido mostrar la potencialidad que tiene\r\nestos datos y la necesidad de bregar porque los mismos sigan siendo de\r\nacceso publico y se mantengan los est谩ndares de calidad e\r\ninteroperabilidad.\r\n\r\nWMS: permite la\r\nvisualizaci贸n de informaci贸n geogr谩fica a partir de una representaci贸n\r\nde 茅sta, de una imagen del mundo real para un 谩rea solicitada por el\r\nusuario. Puede organizarse en una o m谩s capas de datos que pueden\r\nvisualizarse u ocultarse una a una. WFS : permite el\r\nacceso y consulta de los atributos de un vector (feature) que representa\r\ninformaci贸n geogr谩fica como un r铆o, una ciudad o un lago, con una\r\ngeometr铆a descrita por un conjunto de coordenadas. El servicio WFS\r\npermite no solo visualizar la informaci贸n tal y como permite un WMS,\r\nsino tambi茅n consultarla y editarla libremente╋\r\nLas teselas vectoriales son paquetes\r\nde datos geogr谩ficos, empaquetados en 芦mosaicos禄 predefinidos de forma\r\naproximadamente cuadrada para su transferencia a trav茅s de la web.╋\r\nNo necesariamente las personas buscan\r\natenci贸n siguiendo esta racionalidad, ya que existen m煤ltiples razones\r\npara orientarse por otro establecimiento, como puede ser la complejidad\r\ndel tratamiento requerido.╋\r\n",
    "preview": "posts/2022-08-26-consumo-de-geoservicios-con-r-y-su-uso-para-la-gestin-local/imagenes/gchu_sig.PNG",
    "last_modified": "2023-05-04T14:35:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-13-ajuste-multiples-series-de-tiempo/",
    "title": "Ajuste de multiples series de tiempo y multiples modelos",
    "description": "Es muy frecuente que en el quehacer cotidiano como cient铆ficos de datos nos encontremos ante la necesidad de evaluar m煤ltiples modelos de series de tiempo para m谩s de una de ellas. \nEste post muestra como el uso de algunas paquetes como Modeltime, Timetk y sknifedatar puede ser de una gran ayuda para hacer esta tarea m谩s eficiente.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-08-13",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nActivamos las librerias \r\nIntroducci贸n\r\n\r\nWorkflow\r\nPreprocesamiento \r\nModelos 锔\r\nEntrenamiento y\r\nEvaluaci贸n de los modelos じ\r\nSelecci贸n del mejor modelo\r\n\r\nEntrenamiento del mejor modelo\r\n\r\nForecast datos\r\n\r\nAlgunos\r\ncomentarios \r\n\r\nActivamos las librerias \r\n\r\n\r\nlibrary(modeltime)\r\nlibrary(rsample)\r\nlibrary(parsnip)\r\nlibrary(recipes)\r\nlibrary(workflows)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(sknifedatar)\r\nlibrary(kableExtra)\r\nlibrary(DT)\r\nlibrary(lubridate)\r\nlibrary(httr)\r\n\r\n\r\n\r\n\r\n\r\n\r\nIntroducci贸n \r\nEl objetivo principal de este post es compartir una serie de paquetes\r\nque son realmente 煤tiles para resolver la compleja tarea de\r\najustar m煤ltiples series de tiempo y m煤ltiples\r\nmodelos. Asimismo, hay una breve explicaci贸n te贸rica -que\r\nlejos est谩 de pretender ser exhaustiva- de los aspectos m谩s\r\nsobresalientes de los modelos de series de tiempo ajustados.\r\nCargamos el conjunto de\r\ndatos\r\nPara mostrar el funcionamiento de los paquetes, vamos a utilizar el\r\nndice de Precios al Consumidor: Nivel General\r\n(mensual) por regi贸n, desde el 2017 y hasta julio de 2022. Vale\r\ndecir que el uso de estos datos es s贸lo ilustrativo ya que proyectar el\r\nIPC demandar铆a combinar marcos conceptuales econ贸micos y pol铆ticos para\r\nenriquecer los resultados. El data set se obtuvo utilizado la\r\nAPI se series de tiempo del sitio oficial datos.gob.ar\r\nA continuaci贸n se muestra como consumir los datos.\r\n\r\n\r\n# Hacemos un GET sobre el endpoint\r\nobj = GET(\"https://apis.datos.gob.ar/series/api/series?ids=145.3_INGCUYUAL_DICI_M_34,145.3_INGNEAUAL_DICI_M_33,145.3_INGNOAUAL_DICI_M_33,145.3_INGPATUAL_DICI_M_39,145.3_INGGBAUAL_DICI_M_33,145.3_INGNACUAL_DICI_M_38,145.3_INGPAMUAL_DICI_M_38&format=csv\")\r\n\r\n# Extraemos el contenido\r\ncontent <- httr::content(obj,encoding = \"UTF-8\") \r\n\r\n# Creamos un dataframe con los datos\r\ndata <- data.frame(\"Mes\"= content$indice_tiempo,\r\n      \"GBA\"= content$ipc_ng_gba_tasa_variacion_mensual,\r\n      \"Cuyo\"= content$ipc_ng_cuyo_tasa_variacion_mensual,\r\n      \"NEA\"=content$ipc_ng_nea_tasa_variacion_mensual,\r\n      \"NOA\"= content$ipc_ng_noa_tasa_variacion_mensual,\r\n      \"Pampeana\"= content$ipc_ng_pampeana_tasa_variacion_mensual,\r\n      \"Patagonia\"= content$ipc_ng_patagonia_tasa_variacion_mensual,\r\n      \"Nacional\"= content$ipc_ng_nacional_tasa_variacion_mensual) %>%\r\n  pivot_longer(!Mes,names_to = \"Region\", values_to = \"IPC\") %>%\r\n  mutate(IPC = round(IPC * 100,1)) %>%\r\n  rename(\"date\"= \"Mes\", \"value\"= \"IPC\")\r\n\r\n\r\n\r\nAhora echemos un vistazo 叼 a los datos. Para eso cree una funci贸n\r\nque recibe como par谩metros un dataframe, el valor para el eje\r\nx, eje y, el tipo de escala y un campo para\r\nfacetar el gr谩fico. Adem谩s agregamos una linea de suavizaci贸n\r\nde la tendencia para observar el comportamiento a lo largo del\r\ntiempo.\r\n\r\n\r\nview_times_series(df = data,\r\n                  x = \"date\",\r\n                  y = \"value\",\r\n                  facet = \"Region\",\r\n                  title = \"IPC Nivel General (base 2016) por regi贸n\",\r\n                  y_lab = \"IPC\",\r\n                  scales = \"free_y\"\r\n)\r\n\r\n\r\n\r\n\r\nWorkflow\r\nLa siguiente figura resume con claridad el flujo de trabajo que se\r\nimplementa, contrastando la evaluaci贸n de una 煤nica serie y varios\r\nmodelos y de varias series y varios modelos.\r\nFigura 1: Workflow evaluaci贸n de\r\nm煤ltiples modelos en m煤ltiples series de tiempo. Obtenido de https://rafael-zambrano-blog-ds.netlify.app/posts/workflowsets_timeseriesLa Figura 1 muestra la integraci贸n de dos flujos de trabajos, por un\r\nlado la de m煤ltiples series de tiempo y modelos y por otro lado el de\r\nentrenamiento de los modelos.\r\nWorkflow modelado\r\nEste workflow es implementado en el framework Tidymodels, y\r\ncomprende en general las siguientes etapas y librerias:\r\n\r\n\r\n\r\n\r\n El\r\npaquete Modeltime, representa una extensi贸n del\r\nframework Tidymodels, de ah铆 que compartan casi todo el\r\nworkflow, pero agrega al an谩lisis de series temporales una serie de\r\nfunciones que permiten la escalabilidad en la evaluaci贸n de m煤ltiples\r\nmodelos de series de tiempo. Para ello, implementan un tipo de\r\npron贸stico que definen como pron贸stico\r\nanidado y que implica convertir muchas series de tiempo en un\r\nconjunto de datos anidados y luego ajustar muchos modelos a cada uno de\r\nlos datos anidados. Haremos uso en esta ocasi贸n del paquete\r\n__sknifedatar_ que nos permite vincular modeltime con\r\nel conjunto de datos. El workflow implica entonces:\r\nPreprocesamiento de las series\r\nInstanciaci贸n de los modelos\r\nEntrenamiento de los modelos\r\nEvaluaci贸n de los modelos\r\nSelecci贸n del mejor modelo\r\nEntrenamiento del mejor modelo\r\nProyecci贸n de datos\r\nGuardamos las proyecciones y la tabla de precisi贸n del mejor\r\nmodelo\r\nPreprocesamiento \r\nEsta etapa de la evaluaci贸n de los modelos de series de tiempo abarca\r\naqu茅llas acciones de transformaci贸n de los datos para que sean adecuados\r\npara el modelado. Aqu铆 se hace uso del paquete\r\nrecipes() que nos permite organizar las\r\ntransformaciones para el preprocesamiento de la informaci贸n de forma tal\r\nque sea lo m谩s reproducible posible.\r\nAnido la serie de dato por\r\nregi贸n\r\n\r\n\r\nipc_nested = data  %>% nest(nested_column=-Region) \r\n\r\nipc_nested\r\n\r\n\r\n## # A tibble: 7 x 2\r\n##   Region    nested_column    \r\n##   <chr>     <list>           \r\n## 1 GBA       <tibble [67 x 2]>\r\n## 2 Cuyo      <tibble [67 x 2]>\r\n## 3 NEA       <tibble [67 x 2]>\r\n## 4 NOA       <tibble [67 x 2]>\r\n## 5 Pampeana  <tibble [67 x 2]>\r\n## 6 Patagonia <tibble [67 x 2]>\r\n## 7 Nacional  <tibble [67 x 2]>\r\n\r\nArmamos las recetas ィ\r\nLa receta 1, es la utilizada por los modelos de redes neuronales.\r\n\r\n\r\nreceta_IPC_1 = recipe(value ~ ., data = data %>% select(-Region)) %>%\r\n  step_date(date, features = c(\"month\", \"quarter\", \"year\"), ordinal = TRUE)\r\n\r\n\r\n\r\nEsta receta es utilizada por el resto de los modelos. Una de las\r\ntransformaciones que se hace en esta receta es la conversi贸n de la\r\nvariable de fecha a una variable de tipo num茅rica, compatible con los\r\nmodelos lineales.\r\n\r\n\r\nreceta_IPC_2 = receta_IPC_1  %>%\r\n  step_mutate(date_num = as.numeric(date)) %>% \r\n  step_normalize(date_num) %>%\r\n  step_rm(date) %>% \r\n  step_dummy(date_month)\r\n\r\n\r\n\r\nEl paquete recipes() facilita la transformaci贸n y el\r\nprocesado de los datos a trav茅s de las funciones step(), que a\r\nmodo general se utilizan del siguiente modo:\r\nstep_corr(): Elimina las variables que tienen\r\nuna correlaci贸n alta con otras variables.\r\nstep_center(): Centra los datos para que tengan\r\nmedia cero.\r\nstep_scale(): Normaliza los datos para que\r\ntengan desv铆o estandar de 1.\r\nEstos dos pasos son importantes porque cuando los predictores son\r\nnum茅ricos, la escala en la que se miden, as铆 como la magnitud de su\r\nvarianza pueden influir en gran medida en el modelo. Si no se igualan de\r\nlos predictores, aquellos que se midan en una escala mayor o que tengan\r\nm谩s varianza dominar谩n el modelo aunque no sean los que m谩s relaci贸n\r\ntienen con la variable respuesta.\r\nstep_rm(): eliminar谩 las variables en funci贸n de su\r\nnombre, tipo o funci贸n.\r\nAdem谩s, existen una diversidad de funciones asociadas a\r\nstep con diversos usos como: imputar datos a trav茅s de\r\ndiferentes estrategias (utilizando la media, un modelo lineal, etc),\r\npara transformaciones individuales (transformaci贸n logar铆tmica,\r\nexponencial, etc), ordenamiento, retrasos un muchas otras funciones.\r\nModelos 锔\r\nModelo ARIMA boosted\r\nLos modelos ARIMA fueron propuestos por Box y Jenkins en 1976 y se\r\ncaracterizan por realizar predicciones de una variable utilizando como\r\ninformaci贸n la contenida en los valores pasados de la serie temporal. La\r\nsigla ARIMA refiere a Media M贸vil Integrada\r\nAutoRegresiva. Los t茅rminos autorregresivos (AR) se refieren a\r\nlos retrasos de la serie diferenciada, los t茅rminos de promedio m贸vil\r\n(MA) se refieren a los retrasos de los errores e I es el n煤mero de\r\ndiferencia utilizado para hacer que la serie de tiempo sea\r\nestacionaria.\r\nUn punto muy importante a destacar es la implementaci贸n en este\r\nmodelo del principio de boosting para mejorar los\r\nerrores del modelado. La idea detr谩s del boosting es generar m煤ltiples\r\nmodelos de predicci贸n secuenciualmente,y que cada uno de estos tome los\r\nresultados del modelo anterior, para generar un modelo m谩s fuerte, con\r\nmejor poder predictivo y mayor estabilidad en sus resultados. Para\r\nconseguir un modelo m谩s fuerte, se emplea un algoritmo de optimizaci贸n,\r\neste caso Gradient Descent (descenso de gradiente). Durante el\r\nentrenamiento, los par谩metros de cada modelo d茅bil son ajustados\r\niterativamente tratando de encontrar el m铆nimo de una funci贸n objetivo,\r\nque puede ser la proporci贸n de error en la clasificaci贸n, el 谩rea bajo\r\nla curva (AUC), la ra铆z del error cuadr谩tico medio (RMSE) o alguna otra.\r\nCada modelo es comparado con el anterior. Si un nuevo modelo tiene\r\nmejores resultados, entonces se toma este como base para realizar nuevas\r\nmodificaciones. Si, por el contrario, tiene peores resultados, se\r\nregresa al mejor modelo anterior y se modifica ese de una manera\r\ndiferente. XGBoost o Extreme Gradient\r\nBoosting, es uno de los algoritmos de machine learning de tipo\r\nsupervisado m谩s usados en la actualidad. Este algoritmo se\r\ncaracteriza por obtener buenos resultados de predicci贸n con\r\nrelativamente poco esfuerzo, en muchos casos equiparables o mejores que\r\nlos devueltos por modelos m谩s complejos computacionalmente, en\r\nparticular para problemas con datos heterog茅neos.\r\n\r\n\r\nm_arima_boosted_ipc <- workflow() %>% \r\n  add_recipe(receta_IPC_1) %>% \r\n  add_model(arima_boost() %>% set_engine(engine = \"auto_arima_xgboost\"))\r\n\r\n\r\n\r\nModelo seasonal\r\n\r\n\r\nm_seasonal_ipc <- seasonal_reg() %>%\r\n  set_engine(\"stlm_arima\")\r\n\r\n\r\n\r\nModelo prophet boosted\r\nProphet es un procedimiento para pronosticar datos de series\r\ntemporales basado en un modelo aditivo en el que las tendencias no\r\nlineales se ajustan a la estacionalidad anual, semanal y diaria, adem谩s\r\nde los efectos de las vacaciones. Este paquete de c贸digo abierto ha sido\r\nlanzado por el equipo Core Data Science de\r\nFacebook.\r\n\r\n\r\nm_prophet_boost_ipc <- workflow() %>% \r\n  add_recipe(receta_IPC_1) %>% \r\n  add_model(prophet_boost(mode='regression') %>%set_engine(\"prophet_xgboost\"))  \r\n\r\n\r\n\r\nModelo NNetar\r\nEste modelo se basa en redes neuronales con una sola capa oculta y\r\nentradas retrasadas para realizar los pron贸sticos. Es un modelo\r\nautogresivo no lineal. Los modelos de redes neuronales o ANN (sigla del\r\ningles Artificial Neural Network), tienen la capacidad de aprender con\r\nel ejemplo y est谩n basados en los sistemas neuronales biol贸gicos. La red\r\nneuronal es un conjunto de unidades de entrada/salida conectadas en las\r\nque cada conexi贸n tiene un peso asociado. En la fase de aprendizaje, la\r\nred aprende ajustando los pesos para predecir la etiqueta de clase\r\ncorrecta de las entradas dadas. A modo general todos los modelos de ANN\r\nest谩n compuesto de: + Capa de entrada: esta\r\nrepresentada por los datos (inputs) del modelo. La cantidad de columnas\r\nque contiene nuestro archivo indicara la cantidad de neuronas o\r\nunits del modelo.\r\nCapa oculta: recibe los valores de la capa de\r\nentrada, ponderados por los pesos. El conjunto de pesos son\r\ninicializados aleatoriamente al principio y luego optimizados, mediante\r\nalgoritmos de aprendizajes.\r\nCapa de salida: indica el tipo de salida que\r\npretendemos obtener, por ejemplo si nuestro objetivo es obtener una\r\nclasificaci贸n binaria (0 y 1), entonces la capa de salida contendr谩 una\r\ns贸la neurona.\r\nFunci贸n de activaci贸n: se especifica en la capa\r\noculta y tiene el objetivo de agregar no linealidad a nuestra\r\nred y le permite aprender caracter铆sticas complejas. Algunos\r\nejemplos de estas funciones son: Sigmoid o Log铆stica,\r\nReLu (unidad lineal rectificada), tanh\r\n(tangente hiperb贸lica),etc.\r\nLa neurona es la unidad funcional de los modelos\r\nANN. Dentro de cada neurona ocurren dos operaciones: la suma ponderada\r\nde sus entradas y la aplicaci贸n de una funci贸n de activaci贸n\r\n\r\n\r\nm_nnetar_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_1) %>%\r\n  add_model(nnetar_reg() %>% set_engine(\"nnetar\"))\r\n\r\n\r\n\r\nModelo MARS\r\nMARS hace referencia a la sigla del ingl茅s Multivariate\r\nAdaptive Regression Splines, es decir es un modelo de Regresi贸n\r\nSplines Multivariante Adaptativo. De lo anterior se desprenden varios\r\nt茅rminos que es convienente abordar, como son:\r\nSplines: Spline es una funci贸n especial definida\r\npor polinomios y representa una t茅cnica de regresi贸n no\r\nparam茅trica (es decir d贸nde no debemos asumir linealidad en\r\nnuestros datos). En esta t茅cnica, el conjunto de datos se divide en\r\ncontenedores a intervalos o puntos que llamamos nudos, por lo que puede\r\ndecirse que los splines son series de segmentos polin贸micos unidos\r\nentre s铆 en nudos.\r\nLos modelos MARS superan una de las principales desventajas de la\r\nregresi贸n polinomial y que es la dependencia de indicar a priori en que\r\npuntos de la variable x deben hacerse los puntos de corte. Para\r\nello el procedimiento eval煤a cada punto de datos para cada predictor\r\ncomo un nudo y crea un modelo de regresi贸n lineal con las\r\ncaracter铆sticas candidatas. El procedimiento MARS primero buscar谩 el\r\npunto 煤nico en el rango de valores de x donde dos relaciones\r\nlineales diferentes entre x e y logran el error m谩s\r\npeque帽o\r\n\r\n\r\nm_mars_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_2) %>%\r\n  add_model(mars(mode = \"regression\") %>% set_engine(\"earth\"))\r\n\r\n\r\n\r\nModelo Elastic net\r\nEl modelo Elastic net forma parte del conjunto de\r\nmodelos de regresi贸n penalizados, que incluyen los modelos: Ridge, Lasso\r\ny Elastic net.\r\nSon particularmente 煤tiles en el contexto de tener un conjunto de\r\ndatos multivariado con una gran gran cantidad de variables, d贸nde el\r\nm茅todo de los m铆nimos cuadrados (ajuste del modelo lineal), funciona de\r\nforma incorrecta. El pincipio general implica la incorporaci贸n de un\r\nt茅rmino de penalizaci贸n que tiene como consecuencia\r\nreducir (es decir, encoger) los valores de los coeficientes hacia cero.\r\nEsto permite que las variables menos contributivas tengan un coeficiente\r\ncercano a cero o igual a cero. El modelo Elastic net\r\npenaliza con la norma L1 y la norma L2\r\n(es decir las que se utilizan en los modelos Lasso y Ridge). Esto\r\nimplica: + norma L1: es implementada en los modelos de\r\nregresi贸n Lasso. Aqu铆 la penalizaci贸n tiene el efecto de obligar a\r\nalgunas de las estimaciones de los coeficientes, con una contribuci贸n\r\nmenor al modelo, a ser exactamente iguales a cero. Es una buena\r\nalternativa para la selecci贸n de variables con el fin de reducir la\r\ncomplejidad de los modelos. + norma L2: es implementada\r\nen los modelos de regresi贸n Ridge. En este caso la penalizaci贸n tiene\r\ncomo efecto la reducci贸n de los coeficientes de regresi贸n, de modo que\r\nlas variables con una contribuci贸n menor al resultado, tengan sus\r\ncoeficientes cercanos a cero.\r\n\r\n\r\nm_glmnet_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_2) %>%\r\n  add_model(linear_reg(penalty = 0.01, mixture = 0.1) %>% set_engine(\"glmnet\"))\r\n\r\n\r\n\r\nModelo Xgboost: rbol de\r\ndecisi贸n\r\nM谩s arriba se detallo la estrategia de boosting para\r\nreducir los errores del modelado como as铆 tambi茅n las implementaciones\r\ndel paquete xgboost en R y en otros lenguajes de uso\r\ncotidiano en ciencias de datos. Pero queda abordar las caracter铆sticas\r\nm谩s destacadas de los 谩rboles de decisi贸n como modelos\r\nde machine learning. Los 谩rboles de decisi贸n\r\nson modelos predictivos formados por reglas binarias (si/no) con las que\r\nse consigue repartir las observaciones en funci贸n de sus atributos y\r\npredecir as铆 el valor de la variable respuesta. Se engloban dentro de\r\nlos modelos supervisados de machine learning es decir\r\nconocemos a priori la variable de respuesta o dependiente que utilizan\r\nt茅cnicas no param茅tricas. Adem谩s de los algoritmos\r\nGradiente Boosting, existen otros como random\r\nforest para implementar este tipo de m茅todos.\r\n\r\n\r\nm_xgboost_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_2) %>%\r\n  add_model(boost_tree() %>% set_engine(\"xgboost\"))\r\n\r\n\r\n\r\nEntrenamiento y\r\nEvaluaci贸n de los modelos じ\r\nPartici贸n\r\nComo primera medida dividimos nuestros datos en datos para\r\nentrenamiento (training) y para test (evaluaci贸n). Para este caso\r\nseparamos en 80% y 20%\r\n\r\n\r\nsplit <- 0.80\r\n\r\n\r\n\r\nTabla modelos ㄢ\r\nCreamos una tabla que contiene todos los modelos en evaluaci贸n y\r\nd贸nde se guardar谩n las m茅tricas con las que compararemos la performance\r\nde los diferentes modelos.\r\n\r\n\r\nmodel_table_ipc <- modeltime_multifit(\r\n  serie = ipc_nested,\r\n  .prop = split,\r\n  m_arima_boosted_ipc,\r\n  m_seasonal_ipc,\r\n  m_prophet_boost_ipc,\r\n  m_nnetar_ipc,\r\n  m_mars_ipc,\r\n  m_glmnet_ipc,\r\n  m_xgboost_ipc\r\n  \r\n)\r\n\r\n\r\n\r\nModelos\r\nevaluados y su correspondiente m茅trica de error\r\nEn la tabla se encuentran analizadas las siguientes m茅tricas de\r\nerrores:\r\nmae (Mean Absolute Error): es la media de los\r\nerrores en valor absoluto. Esta m茅trica favorece modelos que predicen\r\nmuy bien la gran mayor铆a de observaciones aunque en unas pocas se\r\nequivoque por mucho.\r\nmape (Mean Absolute Porcentaje Error): representa\r\nel valor absoluto expresado en porcentaje, es decir indica en t茅rmino\r\nporcentuales la diferencia promedio entre el valor predicho y el\r\nobservado.\r\nmase (Mean Absolute Scaled Error): esta metrica\r\nsurge de un algoritmo que compara los pron贸sticos con el resultado de un\r\nenfoque de pron贸stico ingenuo. El pron贸stico\r\ningenuo se genera en cualquier paso igualando el pron贸stico\r\nactual con el resultado del 煤ltimo paso de tiempo sin considerar\r\nning煤n patr贸n estacional.\r\nsmape (Symmetric Mean Absolute Percentage Error):\r\nrepresenta la diferencia absoluta entre predicho y\r\nobservado dividido por la mitad de la suma de los valores\r\nabsolutos predichos y observado. El valor de este\r\nc谩lculo se suma para cada punto ajustado t y se divide\r\nnuevamente por el n煤mero de puntos ajustados n.\r\nrmse: Es la ra铆z de la media de los errores\r\nelavados al cuadrado. Sirve para evalur el ajuste del modelo y sus\r\nvalores se expresan en la misma unidad de la variable proyectada.\r\nrsq: representa el coeficiente de determinaci贸n\r\nutilizando una correlaci贸n. Toma valores entre 0 y 1 y describe el % de\r\nvariaci贸n de la variable de respuesta (transacciones, UT, etc.)\r\nexplicado por la variable que puede explicarse por el modelo.\r\n\r\n\r\n\r\nSelecci贸n del mejor modelo \r\nLuego de la evaluaci贸n de los distintos modelos, en base a las\r\nm茅tricas antes descriptas, en esta etapa se selecciona el modelo que\r\nmejor se ajusta a los datos, es decir, aqu茅l que menos error contenga en\r\nla estimaci贸n. En este punto, se hace uso del m茅todo\r\nmodeltime_multibestmodel de la libreria\r\ntimetk y se toma el RMSE como m茅trica\r\npara seleccionar el mejor modelo.\r\n\r\n\r\nbest_model_ipc <- modeltime_multibestmodel(\r\n  .table = model_table_ipc$table_time,\r\n  .metric = \"rmse\",\r\n  .minimize = TRUE,\r\n  .forecast = FALSE\r\n  \r\n)\r\n\r\n\r\n\r\nModelo con mejor\r\nperformance por regi贸n\r\n\r\n##      Region  Modelo\r\n## 1       GBA   EARTH\r\n## 2      Cuyo XGBOOST\r\n## 3       NEA  GLMNET\r\n## 4       NOA   EARTH\r\n## 5  Pampeana   EARTH\r\n## 6 Patagonia   EARTH\r\n## 7  Nacional   EARTH\r\n\r\nEntrenamiento del mejor modelo\r\n\r\nEn esta etapa entrenamos nuestros datos con el modelo que mejor\r\nperformance mostr贸 seg煤n la m茅trica seleccionada en el paso anterior (en\r\nnuestro caso RMSE). Adem谩s se entrena el modelo con la totalidad de\r\ndatos.\r\n\r\n\r\nmodel_refit_ipc <- modeltime_multirefit(best_model_ipc)\r\n\r\n\r\n\r\n\r\n\r\n\r\nForecast datos \r\nPor 煤ltimo, con el modelo entrenado realizamos nuestras proyecciones\r\n(forecast), para un per铆odo de 12 meses\r\n\r\n\r\nforecast_ipc <- modeltime_multiforecast(models_table = model_refit_ipc,\r\n                                           .h = 12,\r\n                                           .prop = split)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ngrafico <- forecast_ipc %>%\r\n  plot_time_series(df = .,title = \"Forecast IPC\",\r\n                   y_lab = \"IPC mensual\",\r\n                   x = \"Mes\",\r\n                   y = \"IPC\",\r\n                   facet = \"Region\")\r\ngrafico\r\n\r\n\r\n\r\n\r\nAlgunos comentarios \r\n No quedan dudas de que los autores de los paquetes aqu铆 revisados\r\n(Modeltime, Timetk, Sknifedatar), han hecho un gran aporte para\r\nfacilitar el trabajo con series de tiempo y modelos en contextos\r\nhabituales d贸nde la labor se centra en ajustar m煤tiples modelos y series\r\nde tiempo.\r\nEste flujo de trabajo de ha sido muy 煤til y espero que les sea a\r\nqui茅n lea este material .\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-13-ajuste-multiples-series-de-tiempo/imagenes/preview.jpg",
    "last_modified": "2023-05-04T14:35:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-30-analisis-de-estacionalidad/",
    "title": "An谩lisis de estacionalidad",
    "description": "En cualquier 谩mbito de la vida cotidiana d贸nde pretendamos hacer proyecciones o compararnos con un momento pasado, es primordial identificar si nuestros datos tienen estacionalidad . En este post voy a mostrar algunas estrategias visuales para analizar estacionalidad en series temporales.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-07-30",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n驴Por qu茅 analizar la\r\nestacionalidad? \r\nEstacionalidad por comercio\r\n锔\r\nRanking de dias seg煤n\r\ncantidad de ventas \r\nComentarios finales \r\n\r\nCargo los paquetes\r\n\r\n\r\n\r\n驴Por qu茅 analizar la\r\nestacionalidad? \r\nCu谩ndo hablamos de estacionalidad nos referimos a un\r\ncomportamiento regular y repetitivo en nuestras\r\ntransacciones, ventas, movimientos o clientes a lo largo del tiempo.\r\nGeneralmente, la estacionalidad est谩 asociada a fen贸menos clim谩ticos o\r\nfestivos.\r\nLa estacionalidad de las series de debemos considerarlas, para:\r\nPlanificar y predecir nuestro futuro ,\r\nComprender el negocio ,\r\nImplementar campa帽as ,\r\nEstacionalidad por comercio 锔\r\nEn este post estaremos analizando el dataset Ь que contiene\r\nlas ventas por d铆a de 3 comercios ficticios, entre el\r\n01-06-2021 y el 31-12-2021  . A continuaci贸n podemos ver el resumen de\r\nlos datos del archivo.\r\n\r\n\r\n\r\n\r\nFecha\r\n\r\n\r\nMerchant\r\n\r\n\r\nCant_Ventas\r\n\r\n\r\n\r\n\r\nMin. :2021-06-01\r\n\r\n\r\nLength:642\r\n\r\n\r\nMin. : 1441\r\n\r\n\r\n\r\n\r\n1st Qu.:2021-07-24\r\n\r\n\r\nClass :character\r\n\r\n\r\n1st Qu.:10599\r\n\r\n\r\n\r\n\r\nMedian :2021-09-15\r\n\r\n\r\nMode :character\r\n\r\n\r\nMedian :18258\r\n\r\n\r\n\r\n\r\nMean :2021-09-15\r\n\r\n\r\nNA\r\n\r\n\r\nMean :17035\r\n\r\n\r\n\r\n\r\n3rd Qu.:2021-11-08\r\n\r\n\r\nNA\r\n\r\n\r\n3rd Qu.:22243\r\n\r\n\r\n\r\n\r\nMax. :2021-12-31\r\n\r\n\r\nNA\r\n\r\n\r\nMax. :58553\r\n\r\n\r\n ahora que vimos el conjunto de datos, usaremos la funci贸n\r\ncreate.seasonal. Debemos indicarle dos par谩metros, el\r\nnombre del data frame y del campo d贸nde est谩 la variable de tipo date\r\n(de fecha). El resultado es una lista que contiene un gr谩fico por cada\r\nsegmento usado (aqu铆 por cada Merchant), con la estacionalidad\r\ndiaria, semanal, mensual y trimestral .\r\n\r\n\r\nestacionalidad <- data %>%\r\n  mutate(nombre= Merchant) %>%\r\n  nest(column_nest= -c(Merchant)) %>%\r\n  mutate(seasonal_dx = map(column_nest, ~ create.seasonal(df = .,\r\n                                                          date = \"Fecha\")))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nmerchant A\r\n\r\nmerchant B\r\n\r\nmerchant C\r\n\r\n\r\n\r\nUna aclaraci贸n importante 锔, los puntos de color fucsia indican los\r\nvalores an贸malos o at铆picos los cu谩les se detectan seg煤n la siguiente\r\nf贸rmula:\r\nQ3+1.5 x IQR\r\nQ1-1.5 x IQR\r\nEs decir consideramos an贸malos todos los valores que se est谩n por lo\r\nmenos 1.5 veces el rango intercuartil por encima del cuartil 3 o por\r\ndebajo del cuartil 1 .\r\nAlgunos comentarios de los\r\ngr谩ficos \r\nEl comercio A tiene mayores ventas los viernes, s谩bados y\r\ndomingos. El comercio B y C tienen una ca铆da muy marcadas de sus ventas\r\nlos domingos.\r\nEn todos los comercios se observa que diciembre es el mejor mes en\r\ncuanto a ventas y por ello el trimestre 4 es el mejor.\r\nRanking de dias seg煤n\r\ncantidad de ventas \r\nUna estrategia visual que podemos sumar para una mejor compresi贸n del\r\ncomportamiento temporal de nuestos datos -ventas en nuestro ejemplo-, es\r\ncombinar mapas de calor (heatmap) en calendarios d贸nde\r\npodamos identificar d铆as con mayores ventas (zonas de calor) y d铆as con\r\nmenores ventas. Para ello, hacemos uso de la funci贸n\r\nheatmap.calendar que recibe 4 par谩metros: + a) el\r\nnombre del dataframe, b) fecha de inicio, c) fecha final y d) Una escala\r\nde color Brewer para identificar la intensidad de las ventas por\r\nd铆a.\r\nVeamos el resultado de la funci贸n .\r\n\r\n\r\nheatmap_calendar <- data %>%\r\n  mutate(nombre= Merchant) %>%\r\n  nest(column_nest= -c(Merchant)) %>%\r\n  mutate(heatmap_calendar = map(column_nest,~heatmap.calendar(df = .,\r\n                                                  fini = '01-06-2021',\r\n                                                  ffin = '31-12-2021',\r\n                                                  ColorBrewer = \"GnBu\")))\r\n\r\n\r\n\r\n\r\n\r\nmerchant A\r\n\r\nmerchant B\r\n\r\nmerchant C\r\n\r\n\r\n\r\nComentarios sobre el\r\nheatmap calendar サ\r\n se puede observar con claridad el patr贸n de mayor nivel de ventas\r\ndel merchant A los d铆as s谩bados y domingos, mientr谩s que para los otros\r\nmerchant caen las ventas en esos d铆as.\r\n nos permite observar el incremento en las ventas en ciertos d铆as,\r\nasociados a festividades o a acciones particulares de parte de los\r\nmerchant.\r\nざ se evidencia un claro incremento en las ventas en las d铆as\r\npr贸ximos a la nochebuena y navidad.\r\nComentarios finales \r\n En este post mostr茅 dos estrategias visuales para analizar la\r\nestacionalidad en nuestras series de datos.\r\n La aplicaci贸n de programaci贸n funcional en R hace que los scripts\r\nsean m谩s eficiente, al mismo tiempo que nos facilita la reutilizaci贸n\r\ndel c贸digo, la b煤squeda de bugs, etc.\r\n gracias por leer mi blog. Cualquier comentario u opini贸n es\r\nsiempre bienvenida .\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-30-analisis-de-estacionalidad/imagenes/estacionalidad2.jpg",
    "last_modified": "2023-05-04T16:13:50-03:00",
    "input_file": "analisis-de-estacionalidad.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Bienvenido a mi blog",
    "description": "Gracias por compartir este espacio conmigo, cuyo principal objetivo\nes crear un espacio de intercambio y crecimiento mutuo.",
    "author": [
      {
        "name": "Hern谩n Hern谩ndez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-07-18",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-05-04T15:49:59-03:00",
    "input_file": {}
  }
]
