[
  {
    "path": "posts/2022-08-27-analisis-rfm-para-la-segmentacion-de-clientes/",
    "title": "Analisis RFM para la segmentacion de clientes",
    "description": "Los modelos RFM (recency, frequency, monetary) son ampliamente utilizados en las areas de marketing para la segmentaci√≥n de sus clientes, pudiendo identificar los clientes m√°s leales, o aqu√©llos que no deber√≠a perder. En este post voy a mostrarles como implementar un modelo RFM, asi que ¬°¬°all√° vamos!!.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-08-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroducci√≥n\r\nüöÄ\r\nExploraci√≥n de los datos üßê\r\nImplementamos el modelo üí°\r\nNormalizaci√≥n de la tabla ‚úÇÔ∏è\r\n\r\nCreamos los puntajes RFM üõçÔ∏è\r\n¬øC√≥mo\r\nse asignan los puntajes? ¬øQue representan esos valores? ü§î\r\n\r\nCreamos los segmentos ‚úèÔ∏è\r\nComenzamos\r\npor definir los segmentos y los umbrales üìù\r\n¬øQue podemos decir de los\r\nsegmentos? üîà\r\n\r\n\r\nComentarios finales üòâ\r\n\r\nIntroducci√≥n üöÄ\r\nComo dec√≠amos en la preview de este post, los modelos RFM son\r\nampliamente utilizados en las √°reas de marketing para segmentar sus\r\ncarteras de clientes. Esta segmentaci√≥n se realiza a partir de un\r\nscoring (que veremos m√°s adelante), sobre tres par√°metros:\r\nrecency: identifica el tiempo transcurrido\r\n(generalmente en d√≠as), desde la √∫ltima actividad del usuario y hasta la\r\nfecha de establecida para el modelo.\r\nfrequency: refiere a con qu√© frecuencia realiza\r\nmovimientos (de compra o venta) el usuario..\r\nmonetary: refiere al valor o monto de los\r\nmovimientos del usuario.\r\nEl modelo RFM descansa sobre el siguiente principio\r\n:\r\n\r\nEl 80% de tu negocio proviene del\r\n20% de tus clientes .\r\n\r\nEn este post, iremos mostrando una serie de funciones que he creado\r\npara procesar los datos y obtener los segmentos:\r\nnormalize_table: con esta funci√≥n normalizamos\r\nlos nombres del dataset para que puedan ser procesados por la funci√≥n\r\nque genera el scoring.\r\nrfm_category: con esta funci√≥n obtenemos la\r\ntabla RFM, la cu√°l contiene los puntajes para los par√°metros (recency,\r\nfrequency, monetary). Adem√°s, nos devuelve una serie de m√©tricas que\r\nutilizaremos para explorar nuestros datos.\r\nsegment_rfm: esta funci√≥n nos permite crear los\r\nsegmentos a partir del scoring de la tabla anterior (tabla RFM), m√°s la\r\ndefinici√≥n de los threshold. Tambi√©n nos devuelve algunas\r\nm√©tricas que nos permiten explorar nuestros resultados.\r\nExploraci√≥n de los datos üßê\r\nUtilizaremos un dataset tomado de Kaggle\r\nque cuenta con informaci√≥n de ventas de una tienda minorista en l√≠nea\r\nregistrada y con sede en el Reino Unido. El dataset muestra las\r\noperaciones en distintos pa√≠ses lo cu√°l resulta muy atractivo a la hora\r\nde implementar un modelo RFM que contemple los datos a este nivel de\r\nagregaci√≥n.\r\nVamos la usar el paquete summarytools para obtener\r\nuna tabla resumen con los valores para todas las variables.\r\n\r\n\r\nShow code\r\n\r\nsummarytools::st_options(lang = 'es')\r\nsummarytools::dfSummary(df, plain.ascii  = FALSE, \r\n                                           style        = \"grid\", \r\n                                           graph.magnif = 0.75, \r\n                                           valid.col    = FALSE,\r\n                                           tmp.img.dir  = \"/tmp\")\r\n\r\n\r\nTabla resumen\r\ndf\r\nDimensiones: 541909 x 8Duplicados: 5268\r\nNo\r\nVariable\r\nEstad√≠sticas / Valores\r\nFrec. (% sobre v√°lidos)\r\nGr√°fico\r\nPerdidos\r\n1\r\nInvoiceNo\r\n[character]\r\n1. 573585\r\n2. 581219\r\n3. 581492\r\n4. 580729\r\n5. 558475\r\n6. 579777\r\n7. 581217\r\n8. 537434\r\n9. 580730\r\n10. 538071\r\n[ 25890 otros ]\r\n1114 ( 0.2%)\r\n749 ( 0.1%)\r\n731 ( 0.1%)\r\n721 ( 0.1%)\r\n705 ( 0.1%)\r\n687 ( 0.1%)\r\n676 ( 0.1%)\r\n675 ( 0.1%)\r\n662 ( 0.1%)\r\n652 ( 0.1%)\r\n534537 (98.6%)\r\n\r\n0\r\n(0.0%)\r\n2\r\nStockCode\r\n[character]\r\n1. 85123A\r\n2. 22423\r\n3. 85099B\r\n4. 47566\r\n5. 20725\r\n6. 84879\r\n7. 22720\r\n8. 22197\r\n9. 21212\r\n10. 20727\r\n[ 4060 otros ]\r\n2313 ( 0.4%)\r\n2203 ( 0.4%)\r\n2159 ( 0.4%)\r\n1727 ( 0.3%)\r\n1639 ( 0.3%)\r\n1502 ( 0.3%)\r\n1477 ( 0.3%)\r\n1476 ( 0.3%)\r\n1385 ( 0.3%)\r\n1350 ( 0.2%)\r\n524678 (96.8%)\r\n\r\n0\r\n(0.0%)\r\n3\r\nDescription\r\n[character]\r\n1. WHITE HANGING HEART T-LIG\r\n2. REGENCY CAKESTAND 3 TIER\r\n3. JUMBO BAG RED RETROSPOT\r\n4. PARTY BUNTING\r\n5. LUNCH BAG RED RETROSPOT\r\n6. ASSORTED COLOUR BIRD ORNA\r\n7. SET OF 3 CAKE TINS PANTRY\r\n8. (Cadena vac√≠a)\r\n9. PACK OF 72 RETROSPOT CAKE\r\n10. LUNCH BAG BLACK SKULL.\r\n[ 4214 otros ]\r\n2369 ( 0.4%)\r\n2200 ( 0.4%)\r\n2159 ( 0.4%)\r\n1727 ( 0.3%)\r\n1638 ( 0.3%)\r\n1501 ( 0.3%)\r\n1473 ( 0.3%)\r\n1454 ( 0.3%)\r\n1385 ( 0.3%)\r\n1350 ( 0.2%)\r\n524653 (96.8%)\r\n\r\n0\r\n(0.0%)\r\n4\r\nQuantity\r\n[integer]\r\nMedia (d-s) : 9.6 (218.1)\r\nmin < mediana < max:\r\n-80995 < 3 < 80995\r\nRI (CV) : 9 (22.8)\r\n722 valores distintos\r\n\r\n0\r\n(0.0%)\r\n5\r\nInvoiceDate\r\n[character]\r\n1. 10/31/2011 14:41\r\n2. 12/8/2011 9:28\r\n3. 12/9/2011 10:03\r\n4. 12/5/2011 17:24\r\n5. 6/29/2011 15:58\r\n6. 11/30/2011 15:13\r\n7. 12/8/2011 9:20\r\n8. 12/6/2010 16:57\r\n9. 12/5/2011 17:28\r\n10. 12/9/2010 14:09\r\n[ 23250 otros ]\r\n1114 ( 0.2%)\r\n749 ( 0.1%)\r\n731 ( 0.1%)\r\n721 ( 0.1%)\r\n705 ( 0.1%)\r\n687 ( 0.1%)\r\n676 ( 0.1%)\r\n675 ( 0.1%)\r\n662 ( 0.1%)\r\n652 ( 0.1%)\r\n534537 (98.6%)\r\n\r\n0\r\n(0.0%)\r\n6\r\nUnitPrice\r\n[numeric]\r\nMedia (d-s) : 4.6 (96.8)\r\nmin < mediana < max:\r\n-11062.1 < 2.1 < 38970\r\nRI (CV) : 2.9 (21)\r\n1630 valores distintos\r\n\r\n0\r\n(0.0%)\r\n7\r\nCustomerID\r\n[integer]\r\nMedia (d-s) : 15287.7 (1713.6)\r\nmin < mediana < max:\r\n12346 < 15152 < 18287\r\nRI (CV) : 2838 (0.1)\r\n4372 valores distintos\r\n\r\n135080\r\n(24.9%)\r\n8\r\nCountry\r\n[character]\r\n1. United Kingdom\r\n2. Germany\r\n3. France\r\n4. EIRE\r\n5. Spain\r\n6. Netherlands\r\n7. Belgium\r\n8. Switzerland\r\n9. Portugal\r\n10. Australia\r\n[ 28 otros ]\r\n495478 (91.4%)\r\n9495 ( 1.8%)\r\n8557 ( 1.6%)\r\n8196 ( 1.5%)\r\n2533 ( 0.5%)\r\n2371 ( 0.4%)\r\n2069 ( 0.4%)\r\n2002 ( 0.4%)\r\n1519 ( 0.3%)\r\n1259 ( 0.2%)\r\n8430 ( 1.6%)\r\n\r\n0\r\n(0.0%)\r\n\r\nEn la tabla resumen puede verse que la mayor√≠a (91,4%) de las\r\noperaciones se concentran en UK,y en menos medida en Alemania y Francia.\r\nAdem√°s, podemos ver la que la fecha de la factura (InvoiceDate) es una\r\nvariable texto con formato m/dd/yyyy hh:mm. Tambi√©n debe destacarse el\r\nhecho de que existen cantidades vendidas con valores menores a 0.\r\nConsiderando lo anterior transformamos la variable InvoiceDate a tipo\r\nDate (yyyy-mm-dd), excluimos cualquier valor n√∫merico menor a 0 y\r\nseleccionamos los datos para los primeros 3 pa√≠ses, es decir UK,\r\nAlemania y Francia.\r\nLuego, agrupamos por pais, Date y CustomerID las cantidades vendidas\r\ny el monto de las mismas.\r\n\r\n\r\nShow code\r\n\r\ndf <- df %>%\r\n  mutate(Date= as_date(lubridate::mdy_hm(InvoiceDate)))%>%\r\n  group_by(Country,Date,CustomerID)%>%\r\n  summarise(Quantity= sum(Quantity),\r\n            Monetary= sum(UnitPrice)) %>%\r\n  filter(Country %in% c(\"United Kingdom\",\"Germany\",\"France\")) %>%\r\n  filter_if(is.numeric, ~ .x > 0)\r\n\r\nDT::datatable(df,filter= 'top',options = list(pageLength = 5, dom = 'tip'))\r\n\r\n\r\n\r\n\r\nEn la tabla podemos visualizar como nos qued√≥ conformado el\r\ndataset.\r\nImplementamos el modelo üí°\r\nEsta etapa est√° compuesta por tres subetapas: a) normalizaci√≥n de la\r\ntabla, 2) creaci√≥n de los puntajes RFM, 3) creaci√≥n de los\r\nsegmentos.\r\nNormalizaci√≥n de la tabla ‚úÇÔ∏è\r\nVamos a normalizar los nombres de nuestro dataset para hacerlos\r\ncoincidir con los par√°metros con los que la funci√≥n\r\nrfm_category asigna los puntajes. Para ello usaremos la\r\nnormalize_table que recibe el dataset con los actuales nombres\r\nde los par√°metros.\r\n\r\n\r\nShow code\r\n\r\ndf <- normalize_table(df = df,\r\n                date = \"Date\",\r\n                id_costumer = \"CustomerID\",\r\n                cantidad = \"Quantity\",\r\n                monto = \"Monetary\")\r\n\r\nDT::datatable(df,filter= 'top',options = list(pageLength = 5, dom = 'tip'))\r\n\r\n\r\n\r\n\r\n\r\nCreamos los puntajes RFM üõçÔ∏è\r\nLa funci√≥n rfm_category, admite 4 par√°metros:\r\ndf: el dataset a procesar\r\nfecha_analisis: representa la fecha de\r\nimplementaci√≥n del modelo y es √∫til para determinar los d√≠as desde la\r\n√∫ltima venta (recency).\r\nbins: nos permite indicar el n√∫mero de segmentos\r\nen los cu√°les queremos cortar nuestra poblaci√≥n.\r\ngroup_by: nos permite utilizar distintos niveles\r\nde agregaci√≥n. Como el CostumerID puede repetirse entre los pa√≠ses, con\r\neste par√°metro nos aseguramos que ese valor sea √∫nico.\r\nA su vez la funci√≥n nos devuelve:\r\nresultado_rfm: una tabla que contiene el\r\nscoring\r\nheatmap: un gr√°fico que muestra el promedio del\r\nmonto para las combinaciones de recency y de frequency.\r\nthreshold: una tabla con los umbrales para\r\nrecency, frequency y monetary.\r\nSin m√°s pre√°mbulos, miremos el resultado de la funci√≥n.\r\n\r\n\r\nShow code\r\n\r\ntabla_rfm <- df %>%\r\n  mutate(email= \"cliente@rfm.com\")%>%\r\n  split(.$Country) %>%\r\n  map(~ rfm_category(df = .,\r\n                     fecha_analisis = '2011-12-09',\r\n                     bins = 4,\r\n                     group_by = \"Country\"))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nxaringanExtra::use_panelset()\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfor(i in 1:length(tabla_rfm)){\r\n  cat(\"::: {.panel}\\n\")             \r\n  cat(\"##\", unique(tabla_rfm[[i]]$resultado_rfm$Country) , \"{.panel-name}\\n\") \r\n  print(tabla_rfm[[i]]$heatmap)\r\n  cat(\"\\n\")\r\n  print(kableExtra::kbl(tabla_rfm[[i]]$threshold,\r\n                      format.args = list(decimal.mark = ',', big.mark = \".\"), col.names = rep(c(\"lower\",\"upper\"),3),\r\n                      caption = paste0(\"Puntos de corte scoring. \",unique(tabla_rfm[[i]]$resultado_rfm$Country),\".\"))%>%\r\n        kableExtra::add_header_above(c(\"Recency\" = 2, \"Frequency\" = 2, \"Monetary\"= 2),color = \"#191C3C\", bold = T,align = \"center\") %>%   \r\nkableExtra::kable_paper())\r\n  cat(\"\\n\") \r\n  cat(\":::\\n\")\r\n}\r\n\r\n\r\nFrance\r\n\r\nTable 1: Puntos de corte scoring. France.\r\n\r\n\r\n\r\nRecency\r\n\r\n\r\n\r\n\r\nFrequency\r\n\r\n\r\n\r\n\r\nMonetary\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\n0,0\r\n\r\n\r\n11,5\r\n\r\n\r\n1,0\r\n\r\n\r\n211,0\r\n\r\n\r\n0,010\r\n\r\n\r\n79,125\r\n\r\n\r\n11,5\r\n\r\n\r\n32,0\r\n\r\n\r\n211,0\r\n\r\n\r\n446,0\r\n\r\n\r\n79,125\r\n\r\n\r\n186,770\r\n\r\n\r\n32,0\r\n\r\n\r\n111,0\r\n\r\n\r\n446,0\r\n\r\n\r\n1.674,5\r\n\r\n\r\n186,770\r\n\r\n\r\n350,815\r\n\r\n\r\n111,0\r\n\r\n\r\n372,0\r\n\r\n\r\n1.674,5\r\n\r\n\r\n10.924,0\r\n\r\n\r\n350,815\r\n\r\n\r\n2.030,560\r\n\r\nGermany\r\n\r\nTable 1: Puntos de corte scoring. Germany.\r\n\r\n\r\n\r\nRecency\r\n\r\n\r\n\r\n\r\nFrequency\r\n\r\n\r\n\r\n\r\nMonetary\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\n0,00\r\n\r\n\r\n17,25\r\n\r\n\r\n1,0\r\n\r\n\r\n260,0\r\n\r\n\r\n0,0100\r\n\r\n\r\n84,7925\r\n\r\n\r\n17,25\r\n\r\n\r\n32,00\r\n\r\n\r\n260,0\r\n\r\n\r\n610,0\r\n\r\n\r\n84,7925\r\n\r\n\r\n185,5250\r\n\r\n\r\n32,00\r\n\r\n\r\n93,00\r\n\r\n\r\n610,0\r\n\r\n\r\n1.629,5\r\n\r\n\r\n185,5250\r\n\r\n\r\n496,5550\r\n\r\n\r\n93,00\r\n\r\n\r\n373,00\r\n\r\n\r\n1.629,5\r\n\r\n\r\n8.213,0\r\n\r\n\r\n496,5550\r\n\r\n\r\n2.431,2800\r\n\r\nUnited Kingdom\r\n\r\nTable 1: Puntos de corte scoring. United Kingdom.\r\n\r\n\r\n\r\nRecency\r\n\r\n\r\n\r\n\r\nFrequency\r\n\r\n\r\n\r\n\r\nMonetary\r\n\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\nlower\r\n\r\n\r\nupper\r\n\r\n\r\n0\r\n\r\n\r\n18\r\n\r\n\r\n1\r\n\r\n\r\n155\r\n\r\n\r\n0,0100\r\n\r\n\r\n49,8050\r\n\r\n\r\n18\r\n\r\n\r\n51\r\n\r\n\r\n155\r\n\r\n\r\n366\r\n\r\n\r\n49,8050\r\n\r\n\r\n123,4400\r\n\r\n\r\n51\r\n\r\n\r\n144\r\n\r\n\r\n366\r\n\r\n\r\n946\r\n\r\n\r\n123,4400\r\n\r\n\r\n285,5625\r\n\r\n\r\n144\r\n\r\n\r\n374\r\n\r\n\r\n946\r\n\r\n\r\n69.982\r\n\r\n\r\n285,5625\r\n\r\n\r\n41.377,3300\r\n\r\n\r\n\r\nAqu√≠ podemos ver los primeros 10 registros de la tabla RFM para\r\nUK.\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(tabla_rfm[[3]]$resultado_rfm %>% head(10),filter= 'top',options = list(pageLength = 5, dom = 'tip'))\r\n\r\n\r\n\r\n\r\n¬øC√≥mo\r\nse asignan los puntajes? ¬øQue representan esos valores? ü§î\r\nC√≥mo se puede observar en la tabla anterior, al dataset se le han\r\na√±adido 3 columnas (recency_cut, frequency_cut y monetary_cut), con\r\nvalores de 1 a 4 1(seg√∫n la cantidad de cortes o bins\r\nelegidos podr√≠a ser 5), que deben interpretarse del siguiente modo:\r\nrecency: la puntuaci√≥n se genera asignando a los\r\nclientes con las fechas de ventas m√°s recientes la m√°xima puntuaci√≥n (4\r\nen este caso), y aquellos con fechas de ventas m√°s distante reciben una\r\nclasificaci√≥n de actualidad de 1.\r\nfrequency: a los usuarios con mayor cantidad de\r\nunidades vendidas se les asigna una puntuaci√≥n m√°s alta (4) y a los de\r\nmenor cantidad una puntuaci√≥n de 1.\r\nmonetary: se asigna sobre la base del monto total de\r\nlas ventas al usuario en el per√≠odo considerado para el an√°lisis. A los\r\nclientes con mayores montos de venta se les asigna una puntuaci√≥n m√°s\r\nalta, mientras que a los que tienen montos de venta m√°s bajos se les\r\nasigna una puntuaci√≥n de 1.\r\nLos heatmap, confirman -aunque en menor medida para Francia-\r\nque los montos promedios m√°s altos povienen que los usuarios con mayor\r\ncantidad de bienes vendidos y con recientes operaciones.\r\nCuando se implement√≥ el modelo con bins= 5, para Francia y Alemania\r\nse obtuvo un gr√°fico inconsistente (no completo todos sus casilleros),\r\npor lo que se decidi√≥ utilizar 4 cortes.\r\nCreamos los segmentos ‚úèÔ∏è\r\nPara la creaci√≥n de los segmentos utilizaremos la funci√≥n\r\nsegment_rfm, a la cu√°l debemos pasarle 3 grupos de\r\npar√°metros:\r\ntabla_rfm: es la tabla resultante de la funci√≥n\r\nanterior y contiene cada uno de nuestros costumer con un puntaje de\r\nrecency, frequency y monetary asignado.\r\nnombres_segmentos: es un vector de datos con la\r\ncantidad de nombres comos segmentos queramos utilizar. En nuestro caso\r\nutilizaremos 7 segmentos (incluyendo los sin clasificar) para hacerlo\r\nm√°s comprensible.\r\numbrales de los segmentos: llevan los nombres\r\nrecency_lower, recency_upper, frequency_lower, frequency_upper,\r\nmonetary_lower, monetary_upper y son vectores que contienen los umbrales\r\ncon los cu√°les agrupamos en segmentos los scoring rfm.\r\nEsta funci√≥n nos retorna:\r\ntabla_rfm: que contiene cada uno de nuestros\r\ncostumer segmentados\r\nbar_chart: gr√°fico de barras que contabiliza los\r\ncostumer seg√∫n el puntaje de recency, frequency y monetary. Representa\r\nuna estrategia visual para observar la composici√≥n de nuestros\r\nclientes.\r\nComposicion_segmento: muestra la frecuencia absoluta\r\ny relativa de los costumer seg√∫n los segmentos.\r\nbar_chart_seg: gr√°fico de barras con la composici√≥n\r\npor segmentos.\r\nimpact_segment:contabiliza en t√©rminos absolutos y\r\nrelativos el impacto que tienen los segmentos sobre las ventas y el\r\nmonto en el pa√≠s.\r\nComenzamos\r\npor definir los segmentos y los umbrales üìù\r\nComo se mencion√≥ anteriormente trabajaremos con los siguientes\r\nsegmentos:\r\nChampions: es el segmento que re√∫ne el m√°ximo\r\npuntaje en los tres par√°metros. Para las ventas en UK este segmento\r\nconcentra el 43% de las mismas y el 40% del monto facturado.\r\nLoyalist: son los clientes que tienen el m√°ximo\r\npuntaje en la cantidad de productos comprados. Representan el segundo\r\nsegmento en t√©rminos de monto facturado.\r\nBig Spenders: este segmento tiene el m√°ximo\r\npuntaje en los montos de compras. Si bien en t√©rmino de cantidad\r\nadquirida no son influyente, respresentan el tercer segmento en monto\r\nfacturado.\r\nPromising: este segmento muestra puntajes de RFM\r\npor encima del resto, pero por debajo de los champions.\r\nEst√°n t√≠midos pero son sensibles de estimular.\r\nNew costumer: tienen alto puntaje en recency, es\r\ndecir han realizado compras recientes aunque no por mucha cantidad ni de\r\nmontos altos. Debemos estimularlos.\r\nHibernating: muestran los puntajes m√°s bajos en\r\nlas tres categor√≠as.\r\nAsignamos los umbrales\r\nrecency_lower : 4,1,1,2,4,1\r\nrecency_upper: 4,4,4,4,4,1\r\nfrequency_lower: 4,4,1,2,1,1\r\nfrequency_upper: 4,4,4,4,4,1\r\nmonetary_lower: 4,1,4,2,1,1\r\nmonetary_upper: 4,4,4,4,4,1\r\n\r\n\r\nShow code\r\n\r\nnombres_segmentos <- c(\"Champions\",\"Loyalist\",\"Big Spenders\",\r\n                       \"Promising\",\"New Customers\",\"Hibernating\")\r\nrecency_lower <-   c(4,1,1,2,4,1)\r\nrecency_upper <-   c(4,4,4,4,4,1)\r\nfrequency_lower <- c(4,4,1,2,1,1)\r\nfrequency_upper <- c(4,4,4,4,4,1)\r\nmonetary_lower <-  c(4,1,4,2,1,1)\r\nmonetary_upper <-  c(4,4,4,4,4,1)\r\n\r\nrfm <- list()\r\n\r\nfor(i in 1:length(tabla_rfm)){\r\n  rfm[[i]] <- segment_rfm(tabla_rfm = tabla_rfm[[i]],\r\n                          nombres_segmentos = nombres_segmentos,\r\n                          recency_lower,\r\n                          recency_upper,\r\n                          frequency_lower,\r\n                          frequency_upper,\r\n                          monetary_lower,\r\n                          monetary_upper\r\n  )\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nfor(i in 1:length(rfm)){\r\n\r\n  cat(\"::: {.panel}\\n\")             \r\n\r\n  cat(\"###\", unique(rfm[[i]]$tabla_rfm$Country), \"{.panel-name}\\n\")\r\ncat(\"\\n\")\r\ncat(\"#### Distribuci√≥n de los clientes seg√∫n el scoring (RFM\")\r\nprint(rfm[[i]]$bar_chart)\r\ncat(\"\\n\")\r\ncat(\"#### Composici√≥n de los segmentos\")\r\ncat(\"\\n\")\r\n\r\n  print(kableExtra::kbl(rfm[[i]]$Composicion_segmento,\r\n\r\n        format.args = list(decimal.mark = ',', big.mark = \".\")) %>%\r\n\r\n     kableExtra::kable_paper())\r\n\r\n cat(\"\\n\")\r\n cat(\"#### Gr√°fico composici√≥n de los segmentos\")\r\n print(rfm[[i]]$bar_chart_seg)\r\n cat(\"\\n\")\r\n\r\n cat(\"#### Impacto de los segmentos en las Ventas y el Monto\")\r\n\r\n  cat(\"\\n\")\r\n\r\n  print(kableExtra::kbl(rfm[[i]]$impact_segment,\r\n        col.names = c(\"Segmentos\",\"Ventas\",\"%\",\"Monto\",\"%\"),\r\n                      \r\n        format.args = list(decimal.mark = ',', big.mark = \".\")) %>%\r\n        \r\n        kableExtra::kable_paper())\r\n\r\n  cat(\"\\n\")\r\n\r\n  cat(\":::\\n\")\r\n\r\n}\r\n\r\n\r\nFrance\r\nDistribuci√≥n\r\nde los clientes seg√∫n el scoring\r\n(RFM\r\nComposici√≥n de los segmentos\r\n\r\nsegmento\r\n\r\n\r\ncantidad\r\n\r\n\r\n%\r\n\r\n\r\nPromising\r\n\r\n\r\n31\r\n\r\n\r\n35,6\r\n\r\n\r\nUsuals\r\n\r\n\r\n18\r\n\r\n\r\n20,7\r\n\r\n\r\nChampions\r\n\r\n\r\n11\r\n\r\n\r\n12,6\r\n\r\n\r\nHibernating\r\n\r\n\r\n11\r\n\r\n\r\n12,6\r\n\r\n\r\nLoyalist\r\n\r\n\r\n11\r\n\r\n\r\n12,6\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n3\r\n\r\n\r\n3,4\r\n\r\n\r\nNew Customers\r\n\r\n\r\n2\r\n\r\n\r\n2,3\r\n\r\nGr√°fico composici√≥n de los\r\nsegmentos\r\nImpacto de\r\nlos segmentos en las Ventas y el Monto\r\n\r\nSegmentos\r\n\r\n\r\nVentas\r\n\r\n\r\n%\r\n\r\n\r\nMonto\r\n\r\n\r\n%\r\n\r\n\r\nLoyalist\r\n\r\n\r\n47.400\r\n\r\n\r\n42,9\r\n\r\n\r\n9.095,71\r\n\r\n\r\n33,0\r\n\r\n\r\nChampions\r\n\r\n\r\n35.434\r\n\r\n\r\n32,1\r\n\r\n\r\n8.474,69\r\n\r\n\r\n30,7\r\n\r\n\r\nPromising\r\n\r\n\r\n16.877\r\n\r\n\r\n15,3\r\n\r\n\r\n5.988,87\r\n\r\n\r\n21,7\r\n\r\n\r\nUsuals\r\n\r\n\r\n5.384\r\n\r\n\r\n4,9\r\n\r\n\r\n1.651,01\r\n\r\n\r\n6,0\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n4.012\r\n\r\n\r\n3,6\r\n\r\n\r\n1.877,45\r\n\r\n\r\n6,8\r\n\r\n\r\nHibernating\r\n\r\n\r\n979\r\n\r\n\r\n0,9\r\n\r\n\r\n419,30\r\n\r\n\r\n1,5\r\n\r\n\r\nNew Customers\r\n\r\n\r\n381\r\n\r\n\r\n0,3\r\n\r\n\r\n88,94\r\n\r\n\r\n0,3\r\n\r\nGermany\r\nDistribuci√≥n de\r\nlos clientes seg√∫n el scoring\r\n(RFM\r\nComposici√≥n de los\r\nsegmentos\r\n\r\nsegmento\r\n\r\n\r\ncantidad\r\n\r\n\r\n%\r\n\r\n\r\nUsuals\r\n\r\n\r\n30\r\n\r\n\r\n31,9\r\n\r\n\r\nPromising\r\n\r\n\r\n24\r\n\r\n\r\n25,5\r\n\r\n\r\nLoyalist\r\n\r\n\r\n13\r\n\r\n\r\n13,8\r\n\r\n\r\nChampions\r\n\r\n\r\n11\r\n\r\n\r\n11,7\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n7\r\n\r\n\r\n7,4\r\n\r\n\r\nHibernating\r\n\r\n\r\n7\r\n\r\n\r\n7,4\r\n\r\n\r\nNew Customers\r\n\r\n\r\n2\r\n\r\n\r\n2,1\r\n\r\nGr√°fico composici√≥n de\r\nlos\r\nsegmentos\r\nImpacto de\r\nlos segmentos en las Ventas y el Monto\r\n\r\nSegmentos\r\n\r\n\r\nVentas\r\n\r\n\r\n%\r\n\r\n\r\nMonto\r\n\r\n\r\n%\r\n\r\n\r\nChampions\r\n\r\n\r\n43.710\r\n\r\n\r\n36,7\r\n\r\n\r\n11.334,46\r\n\r\n\r\n32,7\r\n\r\n\r\nLoyalist\r\n\r\n\r\n38.720\r\n\r\n\r\n32,5\r\n\r\n\r\n9.612,45\r\n\r\n\r\n27,8\r\n\r\n\r\nPromising\r\n\r\n\r\n15.851\r\n\r\n\r\n13,3\r\n\r\n\r\n4.867,00\r\n\r\n\r\n14,1\r\n\r\n\r\nUsuals\r\n\r\n\r\n11.791\r\n\r\n\r\n9,9\r\n\r\n\r\n3.103,82\r\n\r\n\r\n9,0\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n7.655\r\n\r\n\r\n6,4\r\n\r\n\r\n5.212,01\r\n\r\n\r\n15,1\r\n\r\n\r\nHibernating\r\n\r\n\r\n1.020\r\n\r\n\r\n0,9\r\n\r\n\r\n351,03\r\n\r\n\r\n1,0\r\n\r\n\r\nNew Customers\r\n\r\n\r\n430\r\n\r\n\r\n0,4\r\n\r\n\r\n147,49\r\n\r\n\r\n0,4\r\n\r\nUnited Kingdom\r\nDistribuci√≥n de\r\nlos clientes seg√∫n el scoring\r\n(RFM\r\nComposici√≥n de los\r\nsegmentos\r\n\r\nsegmento\r\n\r\n\r\ncantidad\r\n\r\n\r\n%\r\n\r\n\r\nUsuals\r\n\r\n\r\n1.205\r\n\r\n\r\n30,8\r\n\r\n\r\nPromising\r\n\r\n\r\n963\r\n\r\n\r\n24,6\r\n\r\n\r\nLoyalist\r\n\r\n\r\n603\r\n\r\n\r\n15,4\r\n\r\n\r\nChampions\r\n\r\n\r\n375\r\n\r\n\r\n9,6\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n345\r\n\r\n\r\n8,8\r\n\r\n\r\nHibernating\r\n\r\n\r\n289\r\n\r\n\r\n7,4\r\n\r\n\r\nNew Customers\r\n\r\n\r\n136\r\n\r\n\r\n3,5\r\n\r\nGr√°fico composici√≥n de\r\nlos\r\nsegmentos\r\nImpacto de\r\nlos segmentos en las Ventas y el Monto\r\n\r\nSegmentos\r\n\r\n\r\nVentas\r\n\r\n\r\n%\r\n\r\n\r\nMonto\r\n\r\n\r\n%\r\n\r\n\r\nChampions\r\n\r\n\r\n1.768.772\r\n\r\n\r\n43,6\r\n\r\n\r\n439.585,40\r\n\r\n\r\n39,9\r\n\r\n\r\nLoyalist\r\n\r\n\r\n1.372.598\r\n\r\n\r\n33,8\r\n\r\n\r\n217.268,88\r\n\r\n\r\n19,7\r\n\r\n\r\nPromising\r\n\r\n\r\n422.793\r\n\r\n\r\n10,4\r\n\r\n\r\n136.777,67\r\n\r\n\r\n12,4\r\n\r\n\r\nUsuals\r\n\r\n\r\n259.241\r\n\r\n\r\n6,4\r\n\r\n\r\n87.422,87\r\n\r\n\r\n7,9\r\n\r\n\r\nBig Spenders\r\n\r\n\r\n189.988\r\n\r\n\r\n4,7\r\n\r\n\r\n205.524,46\r\n\r\n\r\n18,7\r\n\r\n\r\nNew Customers\r\n\r\n\r\n23.664\r\n\r\n\r\n0,6\r\n\r\n\r\n8.008,58\r\n\r\n\r\n0,7\r\n\r\n\r\nHibernating\r\n\r\n\r\n21.020\r\n\r\n\r\n0,5\r\n\r\n\r\n6.672,41\r\n\r\n\r\n0,6\r\n\r\n\r\n\r\n¬øQue podemos decir de los\r\nsegmentos? üîà\r\nüëâüèº Los patrones y tendencias son m√°s f√°cil de obervar en UK en raz√≥n\r\nde una mayor cantidad de observaciones. El bar_chart que\r\ncombina los scoring de los tres par√°metros muestra que los clientes que\r\nrealizan compras de mayor valor adem√°s tienen altos puntajes de recency\r\ny de frequency (esquina superior derecha). En el opuesto, los que tienen\r\nmenores puntajes de monetary coincide con los menores tambi√©n de recency\r\ny frequency. Esto se da tambi√©n en los clientes de Alemania y Francia\r\npero con menos nitidez.\r\nüëâüèºUn punto interesante es el segmento Usuals, que representa\r\na los clientes no clasificados en los umbrales y que es el principal\r\nsegmento de UK y Alemania. El porcentaje de este segmento est√°\r\ndirectamente vinculado a los objetivos de la compa√±√≠a en relaci√≥n a las\r\npol√≠ticas de fidelizaci√≥n y en t√©rminos estad√≠sticos a la\r\nsensibilidad y especificidad con la que creamos los\r\numbrales. En nuestro caso. si tom√°ramos umbrales m√°s amplios lograr√≠amos\r\ndiminuir ese conjunto de cliente (usuals) pero corremos el riesgo de por\r\nejemplo clasificar como Champions clientes que no lo son o viceversa,\r\ncomo Hibernating a clientes que est√°n activos. Tambi√©n podr√≠amos generar\r\nnuevos segmentos para incluir este grupo de clientes. Esta es una\r\ndiscusi√≥n por dem√°s interesante que requiere de un dialogo fluido entre\r\nla estad√≠stica y el marketing.\r\nüëâüèºSalvo para Francia, los segmentos Champions y Loyalist concentran\r\nla mayor parte de las ventas y la facturaci√≥n (Monto). Los Big Spenders\r\nen estos pa√≠ses representan el tercer segmento en cuanto a facturaci√≥n.\r\nLos Promising son una oportunidad para el negocio ya que son sensibles a\r\npromociones y estrategias de marketing.\r\nComentarios finales üòâ\r\nüî¶Son varias cosas las que me surgen para este apartado final pero\r\nvoy a abocarme s√≥lo a algunas de ellas. Por un lado, la oportunidad que\r\nrepresentan los an√°lisis RFM para el di√°logo entre la estad√≠stica, los\r\ndatos y el apoyo a los decision makers.\r\nüî¶Por otro lado,en este post he mostrado 3 funciones desarrolladas\r\n√≠ntegramente en R que pueden adaptarse para su uso en distintos\r\nescenarios y que cuentan con la posibilidad de ajustar los par√°metros\r\ncentrales para lograr robustez en los resultados.\r\nSin dudas hay mucho camino por recorrer a√∫n en el mundo de los\r\ndatos., as√≠ que sigamos viajando‚úàÔ∏è.\r\n\r\nla cantidad de cortes seleccionados\r\ndivide a la poblaci√≥n en partes iguales. Si seleccionamos 4 cortes,\r\ndividimos a la poblaci√≥n en cuartiles cada uno de los cu√°les representa\r\nel 25%. Si decidi√©ramos utilizar 5 cortes, dividir√≠amos la poblaci√≥n en\r\nquintiles, representando cada uno un 20%.‚Ü©Ô∏é\r\n",
    "preview": "posts/2022-08-27-analisis-rfm-para-la-segmentacion-de-clientes/imagenes/cluster.jpg",
    "last_modified": "2022-08-28T11:12:04-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-26-consumo-de-geoservicios-con-r-y-su-uso-para-la-gestin-local/",
    "title": "Consumo de geoservicios con R y su uso para la gesti√≥n local",
    "description": "En este post voy a mostrar el uso de algunas librerias de R para el consumo de geoservicios. Adem√°s, veremos como visualizar las capas de geoservicios con leaflet y haremos algunas operaciones para apoyar la gesti√≥n local. Espero que este mundo los atrape tanto como a m√≠.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-08-26",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nContexto\r\n¬øQue es un IDE?\r\nü§î\r\n\r\nA los datos\r\nMapeamos\r\nlas capas\r\nTiempo\r\nde ejercicio üèãÔ∏è\r\nMetodolgia üßæ\r\nExtraemos la informacion\r\nde las capas\r\nHomogeneizamos las\r\nproyecciones\r\nComentarios finales üîà\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(sf)\r\nlibrary(leaflet)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(purrr)\r\nlibrary(ggplot2)\r\nlibrary(leaflet.extras2)\r\nlibrary(osrm)\r\n\r\n\r\n\r\nContexto\r\nEn muchas √°reas de la vida social, econ√≥mica, cultural, etc, la\r\ncomprensi√≥n del espacio o ‚Äúlo espacial‚Äù se vuelve fundamental para la\r\ncomprensi√≥n de ciertas din√°micas o tendencias. En el √°rea de salud esto\r\nse hace m√°s evidente ya que en gran parte la salud de un individuo o de\r\nuna comunidad est√° determinada por como esta configurado este espacio\r\nque lo rodea.\r\nPor eso aqu√≠ vamos a trabajar un ejemplo del uso de los\r\ngeoservicios para la gesti√≥n local en salud, pero ¬øque\r\nson los geoservicios?.\r\nVamos a decir que son un Servicio Web espec√≠fico que permite\r\nintercambiar informaci√≥n √∫nicamente de componente geogr√°fica. Para la\r\ngeneraci√≥n y utilizaci√≥n de los estos se utilizan lenguajes espec√≠ficos\r\ny protocolos est√°ndares definidos.\r\n¬øQue es un IDE? ü§î\r\nEs la sigla para representar el t√©rmino Infraestructura de Datos\r\nEspaciales, que cumple la funci√≥n de permitir:\r\n\r\n‚Ä¶acceder a datos, productos y servicios geoespaciales, publicados en\r\ninternet bajo est√°ndares y normas definidos, asegurando su\r\ninteroperabilidad y uso, como as√≠ tambi√©n la propiedad sobre la\r\ninformaci√≥n por parte de los organismos que la publican y su\r\nresponsabilidad en la actualizaci√≥n (IDERA)\r\n\r\nEn Argentina, este rol lo cumple IDERA, una comunidad geoespacial que\r\ninvolucra actores estatales, de la sociedad civil y privados.\r\nA los datos\r\nLuego de la peque√±a intro vamos a los datos. El ejemplo que\r\nutilizaremos para revisar el uso de las librerias corresponde a los\r\ngeoservicios de la ciudad de Gualeguaych√∫, que adem√°s de ser mi ciudad\r\nde nacimiento, recientemente ha disponibilizado esta informaci√≥n para la\r\ncomunidad en el siguiente link.\r\nRealizaremos la lectura del servicio con la libreria sf, la\r\nla usaremos la lo largo de este post ya que nos permite una amplia gama\r\nde operaciones. Es importante saber que existen dos tipos de servicios\r\nWMS y WFS 1.\r\n\r\n\r\nShow code\r\n\r\n#Guardamos la URL en un objeto\r\n\r\ngeo_gchu <- \"https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\"\r\n\r\n#Exploramos las capas\r\n\r\nlayers <- st_layers(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\")\r\nhead(layers$name,10)\r\n\r\n\r\n##  [1] \"gis:AG_Control\"                         \r\n##  [2] \"gis:AG_Nodos\"                           \r\n##  [3] \"gis:AG_Symbols\"                         \r\n##  [4] \"gis:AG_Tramos\"                          \r\n##  [5] \"gis:aerodromo\"                          \r\n##  [6] \"gis:aerodromo_poligono\"                 \r\n##  [7] \"gis:turismo_alojamientos\"               \r\n##  [8] \"gis:antenas\"                            \r\n##  [9] \"gis:antiguos_nombres_de_calles\"         \r\n## [10] \"gis:area_de_fabricacion_y_procesamiento\"\r\n\r\nLa funci√≥n st_layers de sf nos\r\npermite explorar todas las capas disponibles en el geoservicio. En\r\nnuestro caso tomaremos 3:\r\nEstablecimientos de Salud (gis:est_salud)\r\nAreas program√°ticas (gis:areas_programaticas)\r\nRadios Censales (gis:radios_censales)\r\nMapeamos las capas\r\nMiremos ahora üëÄ las capas en un mapa, y usaremos en esta oportunidad\r\nla librer√≠a leaflet que nos permite acceder a mapas\r\ninteractivos muy poderosos. Adem√°s, utilizaremos la libreria\r\nleaflet.extras2 que nos da la posibilidad de ver\r\ncaracter√≠sticas extras a nuestras capas.\r\nUsaremos Argenmapcomo\r\ntesela 2 base y luego iremos agregando las\r\nrespectivas capas. Adem√°s, generaremos un control de capas que nos va a\r\npermitir seleccionar cu√°l o cu√°les queremos ver.\r\n\r\n\r\nShow code\r\n\r\nleaflet() %>%\r\n  addTiles(urlTemplate = \"https://wms.ign.gob.ar/geoserver/gwc/service/tms/1.0.0/mapabase_gris@EPSG%3A3857@png/{z}/{x}/{-y}.png\",\r\n           tileOptions(tms = TRUE,maxZoom = 14), attribution = '<a target=\"_blank\" href=\"https://www.ign.gob.ar/argenmap/argenmap.jquery/docs/#datosvectoriales\" style=\"color: black; text-decoration: underline; font-weight: normal;\">Datos IGN Argentina // OpenStreetMap<\/a>',\r\n           group = \"Argenmap\") %>% #Aqu√≠ agregamos Argenmap\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"radios_censales\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Radios Censales\"\r\n  ) %>% #Agregamos layer de Radios Censales\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"areas_programaticas\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Areas Program√°ticas\"\r\n  )  %>% #Agregamos layer de Areas Program√°ticas\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"est_salud\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Est de salud\"\r\n  ) %>% #Agregamos layer de Establecimientos de Salud\r\n   addLayersControl(\r\n    baseGroups = \"Argenmap\",\r\n    overlayGroups = c(\"Areas Program√°ticas\",\"Est de salud\",\"Radios Censales\"),\r\n    options = layersControlOptions(collapsed = FALSE)) %>%\r\n  setView(lng = -58.52597, lat = -33.00606,zoom = 11)\r\n\r\n\r\n\r\n\r\nSi cliquean sobre el √°rea program√°tica pueden obtener informaci√≥n del\r\nresponsable del √°rea, ubicaci√≥n, tel√©fono. Si en cambio cliquean el\r\nradio censal les devuelve informaci√≥n de la cantidad de hogares,\r\npersonas e incluso el √°rea üôåüèº.\r\nTiempo de ejercicio üèãÔ∏è\r\nEl acceso al sistema de salud puede ser abordado desde distintas\r\nperspectivas, barreras culturales, geogr√°ficas, presencia de medios de\r\ntransporte, etc.\r\nEn este ejercicio intentar√© abordar la accesibilidad que tiene cada\r\n√°rea program√°tica a un centro de salud, para poder determinar en\r\nt√©rminos de tiempo y distancia cu√°l tiene una mayor accesibilidad y cu√°l\r\nmenor.\r\nAntes de avanzar, cu√°ndo hablamos de √°rea\r\nprogram√°tica hacemos referencia al √°rea de influencia de un\r\ndeterminado centro asistencial que se expresa, aunque no √∫nicamente, en\r\nun pol√≠gono geogr√°fico. La idea central es que las\r\npersonas que viven en esa √°rea se referencian con el centro de salud o\r\nlos centros de salud contenidos en √©sta 3.\r\nMetodolgia üßæ\r\nLa metodolog√≠a que voy a usar para determinar la distancia por √°rea\r\nprogram√°tica al centro de salud, es asignar los radios censales a cada\r\nuno de los poligonos que representan el √°rea program√°tica.\r\nLuego, voy a calcular los centroides de cada radio censal y voy a\r\nmedir la distancia en tiempo (a pie) y en kil√≥metros hasta el centro o\r\nlos centros de salud que contiene el area.\r\nResulta un poco confuso üôÑ, veamos el c√≥digo.\r\nExtraemos la informacion\r\nde las capas\r\n\r\n\r\nShow code\r\n\r\n#Extraemos las areas programaticas\r\nareas_programaticas <- st_read(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\",\"gis:areas_programaticas\")\r\n\r\n\r\n## Reading layer `gis:areas_programaticas' from data source \r\n##   `WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities' \r\n##   using driver `WFS'\r\n## Simple feature collection with 9 features and 5 fields\r\n## Geometry type: MULTISURFACE\r\n## Dimension:     XY\r\n## Bounding box:  xmin: 5628553 ymin: 6342554 xmax: 5641830 ymax: 6350683\r\n## Projected CRS: POSGAR 98 / Argentina 5\r\n\r\nShow code\r\n\r\n#Extraemos los establecimientos de salud\r\n\r\nest_salud <- st_read(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\", \"gis:est_salud\")\r\n\r\n\r\n## Reading layer `gis:est_salud' from data source \r\n##   `WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities' \r\n##   using driver `WFS'\r\n## Simple feature collection with 11 features and 17 fields\r\n## Geometry type: MULTIPOINT\r\n## Dimension:     XY\r\n## Bounding box:  xmin: -58.54233 ymin: -33.03131 xmax: -58.50246 ymax: -32.99495\r\n## Geodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\n#Extraemos los radios censales\r\n\r\nradios_censales <- st_read(\"WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities\",\"gis:radios_censales\")\r\n\r\n\r\n## Reading layer `gis:radios_censales' from data source \r\n##   `WFS:https://geo.gualeguaychu.gov.ar/geoserver/gis/wms?Request=GetCapabilities' \r\n##   using driver `WFS'\r\n## Simple feature collection with 95 features and 10 fields\r\n## Geometry type: MULTISURFACE\r\n## Dimension:     XY\r\n## Bounding box:  xmin: 5625130 ymin: 6334522 xmax: 5675083 ymax: 6353581\r\n## Projected CRS: POSGAR 98 / Argentina 5\r\n\r\nShow code\r\n\r\nradios_censales <- radios_censales %>% \r\n  filter(!gml_id %in% c(\"radios_censales.2\",\"radios_censales.11\"))\r\n\r\n\r\n\r\nHomogeneizamos las\r\nproyecciones\r\n\r\n\r\nShow code\r\n\r\n#homogeneizamos las proyecciones\r\n\r\n# Centros de salud\r\ncsalud <- est_salud %>% st_drop_geometry()\r\n\r\ncsalud <- st_as_sf(csalud[c(\"fid\",\"nombre\",\"longitud\",\"latitud\")], coords = c(\"longitud\", \"latitud\"), \r\n         crs = 4326, remove= FALSE) %>% st_transform(crs = 4326)\r\n\r\n\r\nest_salud <- st_transform(est_salud, 4326)\r\n\r\n# Areas programaticas\r\n\r\nareas_programaticas <- st_transform(areas_programaticas, 4326)\r\n\r\nareas_programaticas <- st_cast(areas_programaticas, \"GEOMETRYCOLLECTION\") %>% st_collection_extract(\"POLYGON\")\r\n\r\n# Radios censales\r\nradios_censales <- st_transform(radios_censales, 4326)\r\n\r\nradios_censales <- st_cast(radios_censales, \"GEOMETRYCOLLECTION\") %>% st_collection_extract(\"POLYGON\")\r\n\r\n\r\n\r\nCalculamos los\r\ncentroides y los visualizamos\r\nPrimero utilizamos la funci√≥n st_centroid de\r\nsf y lo agregamos como layers a nuestro mapa.\r\n\r\n\r\nShow code\r\n\r\nradios_censales$centroides <- radios_censales %>%  st_centroid() %>%\r\n  st_geometry() \r\n\r\ncentroides <- radios_censales %>%\r\n  select(gml_id)%>%\r\n  st_centroid() %>%\r\n  st_as_sf()\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nleaflet() %>%\r\n  addTiles(urlTemplate = \"https://wms.ign.gob.ar/geoserver/gwc/service/tms/1.0.0/mapabase_gris@EPSG%3A3857@png/{z}/{x}/{-y}.png\",\r\n           tileOptions(tms = TRUE,maxZoom = 14), attribution = '<a target=\"_blank\" href=\"https://www.ign.gob.ar/argenmap/argenmap.jquery/docs/#datosvectoriales\" style=\"color: black; text-decoration: underline; font-weight: normal;\">Datos IGN Argentina // OpenStreetMap<\/a>',\r\n           group = \"Argenmap\") %>% #Aqu√≠ agregamos Argenmap\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"radios_censales\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Radios Censales\"\r\n  ) %>% #Agregamos layer de Radios Censales\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"areas_programaticas\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Areas Program√°ticas\"\r\n  )  %>% #Agregamos layer de Areas Program√°ticas\r\n  addWMS(\r\n    geo_gchu,\r\n    layers = \"est_salud\",\r\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE,info_format = \"text/html\", tiled=FALSE),\r\n    group = \"Est de salud\"\r\n  ) %>% #Agregamos layer de Establecimientos de Salud\r\n  addCircles(data= centroides,group = \"centroides\",color = \"black\")%>% #Agregamos el circulo con los centroides\r\n   addLayersControl(\r\n    baseGroups = \"Argenmap\",\r\n    overlayGroups = c(\"Areas Program√°ticas\",\"Est de salud\",\"Radios Censales\",\"centroides\"),\r\n    options = layersControlOptions(collapsed = FALSE)) %>%\r\n  setView(lng = -58.52597, lat = -33.00606,zoom = 11)\r\n\r\n\r\n\r\n\r\nHacemos los joins ü§ù\r\nEn esta etapa vamos a asignar los centros de salud y\r\nlos radios censales a cada √°rea program√°tica, para\r\nluego poder hacer los c√°lculos. Utilizamos la funci√≥n\r\nst_join de sf.\r\n\r\n\r\nShow code\r\n\r\njoin <- st_join(areas_programaticas,csalud,join= st_intersects)\r\n\r\n\r\n\r\nVeamos que los centros de salud han sido asignado a areas\r\nprogramatica\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(join %>% st_drop_geometry() %>% head(5),\r\n              options = list(scrollX=TRUE))\r\n\r\n\r\n\r\n\r\nHacemos la asignaci√≥n de los radios censales a las √°reas\r\nprogram√°ticas.\r\n\r\n\r\nShow code\r\n\r\njoin2 <- st_join(join,radios_censales,join= st_intersects)\r\n\r\n\r\n\r\nRevisamos el resultado.\r\n\r\n\r\nShow code\r\n\r\nDT::datatable(join2 %>% st_drop_geometry() %>% head(5),\r\n               options = list(scrollX=TRUE))\r\n\r\n\r\n\r\n\r\nMedimos las distancias üìè\r\nLlegamos a una etapa por dem√°s interesante, en la que obtendremos las\r\ndistancias entre los centroides y los centros de salud. Para ello,\r\nutilizo el paquete OSRM que se basa en el proyecto que lleva el mismo\r\nnombre y que es un Servicio de enrutamiento basado en datos de\r\nOpenStreetMap. Vale decir, que el resultado de las\r\nmediciones es la distancia en tiempo a pie üö∂y en kil√≥metros.\r\nVale aclarar que en el c√≥digo de lectura de los radios censales se\r\nexcluyen los radios 11 (Pueblo Belgrano) y 2, ya que abarcan territorio\r\nmuy por fuera del ejido.\r\nPara tener las distancias, voy a:\r\ncrear un data frame con los puntos de los establecimientos de\r\nsalud y los centroides por area programatica.\r\n\r\n\r\nShow code\r\n\r\n#Armo un dataframe de distancias\r\n\r\ndistancias <- st_join(est_salud %>%\r\n       select(\"fid\",\"nombre\")%>%\r\n       rename(\"geomcs\"=\"geom\"),join2 %>%\r\n         select(\"gml_id.y\",\"centroides\"), join= st_intersects) %>%\r\n  distinct()\r\n\r\n\r\n\r\nseparar el data frame anterior en uno de origen (from) y uno de\r\ndestino (to).\r\n\r\n\r\nShow code\r\n\r\nfrom <- distancias %>%\r\n  select(gml_id.y,centroides)%>%\r\n  st_drop_geometry(geomcs) %>%\r\n  st_as_sf()\r\n\r\nto <- distancias %>%\r\n  select(fid,nombre,geomcs)\r\n\r\n\r\n\r\nobtener las distancias\r\n\r\n\r\nShow code\r\n\r\nrutas <- list()\r\n\r\nfor(i in 1:nrow(from)){\r\n\r\nrutas[[i]] <- osrmRoute(src = from[i,],\r\n          dst = to[i,],\r\n          overview = FALSE)\r\n}\r\n\r\ndf <- do.call(rbind,rutas)\r\n\r\n\r\n\r\nunificar los dataset.\r\n\r\n\r\nShow code\r\n\r\ndistancias_x_cs <- cbind(distancias[,c(1,2)] %>% st_drop_geometry(),df)\r\n\r\n\r\n\r\nMostramos las\r\ndistancias por √°rea program√°tica\r\n\r\n\r\nShow code\r\n\r\njoin %>%\r\n  st_drop_geometry()%>%\r\n  select(gml_id,fid)%>%\r\n  left_join(distancias_x_cs, by= \"fid\") %>%\r\n  DT::datatable(options = list(pageLength = 25, dom = 'tip'))\r\n\r\n\r\n\r\n\r\nResumen distancias tiempo ‚Äúa\r\npie‚Äù\r\n\r\n\r\nShow code\r\n\r\njoin %>%\r\n  st_drop_geometry()%>%\r\n  select(gml_id,fid)%>%\r\n  left_join(distancias_x_cs, by= \"fid\") %>%\r\n  group_by(gml_id)%>%\r\n  filter(!is.na(duration)) %>%\r\n  summarise(min= round(min(duration),1),\r\n            prom= round(mean(duration),1),\r\n            mediana= round(median(duration),1),\r\n            max= round(max(duration),1)) %>%\r\n  arrange(prom)%>%\r\n  kableExtra::kbl(format.args = list(big.mark = \".\",\r\n                                     decimal.mark= \",\")) %>%\r\n  kableExtra::kable_classic_2()\r\n\r\n\r\n\r\ngml_id\r\n\r\n\r\nmin\r\n\r\n\r\nprom\r\n\r\n\r\nmediana\r\n\r\n\r\nmax\r\n\r\n\r\nareas_programaticas.4\r\n\r\n\r\n1,0\r\n\r\n\r\n2,5\r\n\r\n\r\n2,1\r\n\r\n\r\n7,2\r\n\r\n\r\nareas_programaticas.12\r\n\r\n\r\n0,8\r\n\r\n\r\n2,6\r\n\r\n\r\n2,5\r\n\r\n\r\n5,4\r\n\r\n\r\nareas_programaticas.11\r\n\r\n\r\n0,6\r\n\r\n\r\n2,8\r\n\r\n\r\n3,0\r\n\r\n\r\n4,3\r\n\r\n\r\nareas_programaticas.3\r\n\r\n\r\n0,1\r\n\r\n\r\n3,0\r\n\r\n\r\n3,0\r\n\r\n\r\n5,2\r\n\r\n\r\nareas_programaticas.9\r\n\r\n\r\n0,4\r\n\r\n\r\n3,7\r\n\r\n\r\n3,8\r\n\r\n\r\n10,1\r\n\r\n\r\nareas_programaticas.8\r\n\r\n\r\n1,1\r\n\r\n\r\n3,9\r\n\r\n\r\n3,9\r\n\r\n\r\n6,5\r\n\r\n\r\nareas_programaticas.2\r\n\r\n\r\n2,1\r\n\r\n\r\n4,6\r\n\r\n\r\n3,6\r\n\r\n\r\n11,9\r\n\r\n\r\nareas_programaticas.5\r\n\r\n\r\n1,0\r\n\r\n\r\n5,1\r\n\r\n\r\n2,8\r\n\r\n\r\n11,9\r\n\r\n\r\nResumen distancias en\r\nkil√≥metros\r\n\r\n\r\nShow code\r\n\r\njoin %>%\r\n  st_drop_geometry()%>%\r\n  select(gml_id,fid)%>%\r\n  left_join(distancias_x_cs, by= \"fid\") %>%\r\n  group_by(gml_id)%>%\r\n  filter(!is.na(duration)) %>%\r\n  summarise(min= round(min(distance),1),\r\n            prom= round(mean(distance),1),\r\n            mediana= round(median(distance),1),\r\n            max= round(max(distance),1)) %>%\r\n  arrange(prom)%>%\r\n  kableExtra::kbl(format.args = list(big.mark = \".\",\r\n                                     decimal.mark= \",\")) %>%\r\n  kableExtra::kable_classic_2()\r\n\r\n\r\n\r\ngml_id\r\n\r\n\r\nmin\r\n\r\n\r\nprom\r\n\r\n\r\nmediana\r\n\r\n\r\nmax\r\n\r\n\r\nareas_programaticas.4\r\n\r\n\r\n0,4\r\n\r\n\r\n1,1\r\n\r\n\r\n0,8\r\n\r\n\r\n3,1\r\n\r\n\r\nareas_programaticas.11\r\n\r\n\r\n0,1\r\n\r\n\r\n1,3\r\n\r\n\r\n1,4\r\n\r\n\r\n2,5\r\n\r\n\r\nareas_programaticas.12\r\n\r\n\r\n0,3\r\n\r\n\r\n1,3\r\n\r\n\r\n1,2\r\n\r\n\r\n3,0\r\n\r\n\r\nareas_programaticas.3\r\n\r\n\r\n0,1\r\n\r\n\r\n1,4\r\n\r\n\r\n1,4\r\n\r\n\r\n2,5\r\n\r\n\r\nareas_programaticas.8\r\n\r\n\r\n0,4\r\n\r\n\r\n1,7\r\n\r\n\r\n1,6\r\n\r\n\r\n2,6\r\n\r\n\r\nareas_programaticas.9\r\n\r\n\r\n0,2\r\n\r\n\r\n1,9\r\n\r\n\r\n1,9\r\n\r\n\r\n5,0\r\n\r\n\r\nareas_programaticas.2\r\n\r\n\r\n0,8\r\n\r\n\r\n2,0\r\n\r\n\r\n1,4\r\n\r\n\r\n6,1\r\n\r\n\r\nareas_programaticas.5\r\n\r\n\r\n0,5\r\n\r\n\r\n3,4\r\n\r\n\r\n1,2\r\n\r\n\r\n9,0\r\n\r\n\r\nComentarios finales üîà\r\n‚úîÔ∏è En relaci√≥n al objetivo principal del ejercicio, puede observase\r\nque el √°rea program√°tica 4, es la que tiene mayor accesibilidad con un\r\npromedio de 2,5 minutos y 1,1 Km al centro de salud San Francisco. En el\r\nopuesto el √°rea program√°tica 5 mostr√≥ la mayor distancia con un promedio\r\nde 5,1 minutos y 3,4 Km al centro de salud Suburbio Sur. Sin dudas un\r\npromedio de 5 minutos al centro de salud en el √°rea con menor\r\naccesibilidad da cuentas de una amplia cobertura del sistema de\r\nsalud.\r\n‚úîÔ∏èEn lo que respecta a la metodolog√≠a sin dudas muestra limitaciones.\r\nUna de ellas es el hecho de que algunos radios censales caen en mas de\r\nun √°rea program√°tica. Por ej: el radio censal 13 cae dentro de Villa\r\nMaria, Suburbio Sur, M√©danos. Esto implica que un habitante de esta zona\r\ncuenta con mayor oferta. Otra limitaci√≥n es haber tomado los centroides,\r\nse podr√≠a mejorar esto tomando puntos al azar dentro de los radios\r\ncensales.\r\n‚úîÔ∏èPor ultimo espero haber podido mostrar la potencialidad que tiene\r\nestos datos y la necesidad de bregar porque los mismos sigan siendo de\r\nacceso publico y se mantengan los est√°ndares de calidad e\r\ninteroperabilidad.\r\n\r\nWMS: permite la\r\nvisualizaci√≥n de informaci√≥n geogr√°fica a partir de una representaci√≥n\r\nde √©sta, de una imagen del mundo real para un √°rea solicitada por el\r\nusuario. Puede organizarse en una o m√°s capas de datos que pueden\r\nvisualizarse u ocultarse una a una. WFS : permite el\r\nacceso y consulta de los atributos de un vector (feature) que representa\r\ninformaci√≥n geogr√°fica como un r√≠o, una ciudad o un lago, con una\r\ngeometr√≠a descrita por un conjunto de coordenadas. El servicio WFS\r\npermite no solo visualizar la informaci√≥n tal y como permite un WMS,\r\nsino tambi√©n consultarla y editarla libremente‚Ü©Ô∏é\r\nLas teselas vectoriales son paquetes\r\nde datos geogr√°ficos, empaquetados en ¬´mosaicos¬ª predefinidos de forma\r\naproximadamente cuadrada para su transferencia a trav√©s de la web.‚Ü©Ô∏é\r\nNo necesariamente las personas buscan\r\natenci√≥n siguiendo esta racionalidad, ya que existen m√∫ltiples razones\r\npara orientarse por otro establecimiento, como puede ser la complejidad\r\ndel tratamiento requerido.‚Ü©Ô∏é\r\n",
    "preview": "posts/2022-08-26-consumo-de-geoservicios-con-r-y-su-uso-para-la-gestin-local/imagenes/gchu_sig.PNG",
    "last_modified": "2022-08-30T10:18:57-03:00",
    "input_file": "consumo-de-geoservicios-con-r-y-su-uso-para-la-gestin-local.knit.md"
  },
  {
    "path": "posts/2022-08-13-ajuste-multiples-series-de-tiempo/",
    "title": "Ajuste de multiples series de tiempo y multiples modelos",
    "description": "Es muy frecuente que en el quehacer cotidiano como cient√≠ficos de datos nos encontremos ante la necesidad de evaluar m√∫ltiples modelos de series de tiempo para m√°s de una de ellas. \nEste post muestra como el uso de algunas paquetes como Modeltime, Timetk y sknifedatar puede ser de una gran ayuda para hacer esta tarea m√°s eficiente.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-08-13",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nActivamos las librerias üìö\r\nIntroducci√≥n\r\nü§ì\r\nWorkflow\r\nPreprocesamiento üìù\r\nModelos ‚öôÔ∏è\r\nEntrenamiento y\r\nEvaluaci√≥n de los modelos ü§∏üèΩ\r\nSelecci√≥n del mejor modelo\r\nüòé\r\nEntrenamiento del mejor modelo\r\nüèã\r\nForecast datos\r\nüîÆ\r\nAlgunos\r\ncomentarios üó£\r\n\r\nActivamos las librerias üìö\r\n\r\n\r\nlibrary(modeltime)\r\nlibrary(rsample)\r\nlibrary(parsnip)\r\nlibrary(recipes)\r\nlibrary(workflows)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(sknifedatar)\r\nlibrary(kableExtra)\r\nlibrary(DT)\r\nlibrary(lubridate)\r\nlibrary(httr)\r\n\r\n\r\n\r\n\r\n\r\n\r\nIntroducci√≥n ü§ì\r\nEl objetivo principal de este post es compartir una serie de paquetes\r\nque son realmente √∫tiles para resolver la compleja tarea de\r\najustar m√∫ltiples series de tiempo y m√∫ltiples\r\nmodelosüí™üèº. Asimismo, hay una breve explicaci√≥n te√≥rica -que\r\nlejos est√° de pretender ser exhaustiva- de los aspectos m√°s\r\nsobresalientes de los modelos de series de tiempo ajustados.\r\nCargamos el conjunto de\r\ndatos\r\nPara mostrar el funcionamiento de los paquetes, vamos a utilizar el\r\n√çndice de Precios al Consumidor: Nivel General\r\n(mensual) por regi√≥n, desde el 2017 y hasta julio de 2022. Vale\r\ndecir que el uso de estos datos es s√≥lo ilustrativo ya que proyectar el\r\nIPC demandar√≠a combinar marcos conceptuales econ√≥micos y pol√≠ticos para\r\nenriquecer los resultados. El data set se obtuvo utilizado la\r\nAPI se series de tiempo del sitio oficial datos.gob.ar\r\nA continuaci√≥n se muestra como consumir los datos.\r\n\r\n\r\n# Hacemos un GET sobre el endpoint\r\nobj = GET(\"https://apis.datos.gob.ar/series/api/series?ids=145.3_INGCUYUAL_DICI_M_34,145.3_INGNEAUAL_DICI_M_33,145.3_INGNOAUAL_DICI_M_33,145.3_INGPATUAL_DICI_M_39,145.3_INGGBAUAL_DICI_M_33,145.3_INGNACUAL_DICI_M_38,145.3_INGPAMUAL_DICI_M_38&format=csv\")\r\n\r\n# Extraemos el contenido\r\ncontent <- httr::content(obj,encoding = \"UTF-8\") \r\n\r\n# Creamos un dataframe con los datos\r\ndata <- data.frame(\"Mes\"= content$indice_tiempo,\r\n      \"GBA\"= content$ipc_ng_gba_tasa_variacion_mensual,\r\n      \"Cuyo\"= content$ipc_ng_cuyo_tasa_variacion_mensual,\r\n      \"NEA\"=content$ipc_ng_nea_tasa_variacion_mensual,\r\n      \"NOA\"= content$ipc_ng_noa_tasa_variacion_mensual,\r\n      \"Pampeana\"= content$ipc_ng_pampeana_tasa_variacion_mensual,\r\n      \"Patagonia\"= content$ipc_ng_patagonia_tasa_variacion_mensual,\r\n      \"Nacional\"= content$ipc_ng_nacional_tasa_variacion_mensual) %>%\r\n  pivot_longer(!Mes,names_to = \"Region\", values_to = \"IPC\") %>%\r\n  mutate(IPC = round(IPC * 100,1)) %>%\r\n  rename(\"date\"= \"Mes\", \"value\"= \"IPC\")\r\n\r\n\r\n\r\nAhora echemos un vistazo üïµüèΩ a los datos. Para eso cree una funci√≥n\r\nque recibe como par√°metros un dataframe, el valor para el eje\r\nx, eje y, el tipo de escala y un campo para\r\nfacetar el gr√°fico. Adem√°s agregamos una linea de suavizaci√≥n\r\nde la tendencia para observar el comportamiento a lo largo del\r\ntiempo.\r\n\r\n\r\nview_times_series(df = data,\r\n                  x = \"date\",\r\n                  y = \"value\",\r\n                  facet = \"Region\",\r\n                  title = \"IPC Nivel General (base 2016) por regi√≥n\",\r\n                  y_lab = \"IPC\",\r\n                  scales = \"free_y\"\r\n)\r\n\r\n\r\n\r\n\r\nWorkflow\r\nLa siguiente figura resume con claridad el flujo de trabajo que se\r\nimplementa, contrastando la evaluaci√≥n de una √∫nica serie y varios\r\nmodelos y de varias series y varios modelos.\r\nFigura 1: Workflow evaluaci√≥n de\r\nm√∫ltiples modelos en m√∫ltiples series de tiempo. Obtenido de https://rafael-zambrano-blog-ds.netlify.app/posts/workflowsets_timeseriesLa Figura 1 muestra la integraci√≥n de dos flujos de trabajos, por un\r\nlado la de m√∫ltiples series de tiempo y modelos y por otro lado el de\r\nentrenamiento de los modelos.\r\nWorkflow modelado\r\nEste workflow es implementado en el framework Tidymodels, y\r\ncomprende en general las siguientes etapas y librerias:\r\n\r\n\r\n\r\n\r\n El\r\npaquete Modeltime, representa una extensi√≥n del\r\nframework Tidymodels, de ah√≠ que compartan casi todo el\r\nworkflow, pero agrega al an√°lisis de series temporales una serie de\r\nfunciones que permiten la escalabilidad en la evaluaci√≥n de m√∫ltiples\r\nmodelos de series de tiempo. Para ello, implementan un tipo de\r\npron√≥stico que definen como pron√≥stico\r\nanidado y que implica ‚Äúconvertir muchas series de tiempo en un\r\nconjunto de datos anidados y luego ajustar muchos modelos a cada uno de\r\nlos datos anidados‚Äù. Haremos uso en esta ocasi√≥n del paquete\r\n__sknifedatar_ que nos permite vincular modeltime con\r\nel conjunto de datos. El workflow implica entonces:\r\nPreprocesamiento de las series\r\nInstanciaci√≥n de los modelos\r\nEntrenamiento de los modelos\r\nEvaluaci√≥n de los modelos\r\nSelecci√≥n del mejor modelo\r\nEntrenamiento del mejor modelo\r\nProyecci√≥n de datos\r\nGuardamos las proyecciones y la tabla de precisi√≥n del mejor\r\nmodelo\r\nPreprocesamiento üìù\r\nEsta etapa de la evaluaci√≥n de los modelos de series de tiempo abarca\r\naqu√©llas acciones de transformaci√≥n de los datos para que sean adecuados\r\npara el modelado. Aqu√≠ se hace uso del paquete\r\nrecipes() que nos permite organizar las\r\ntransformaciones para el preprocesamiento de la informaci√≥n de forma tal\r\nque sea lo m√°s reproducible posible.\r\nAnido la serie de dato por\r\nregi√≥n\r\n\r\n\r\nipc_nested = data  %>% nest(nested_column=-Region) \r\n\r\nipc_nested\r\n\r\n\r\n## # A tibble: 7 x 2\r\n##   Region    nested_column    \r\n##   <chr>     <list>           \r\n## 1 GBA       <tibble [67 x 2]>\r\n## 2 Cuyo      <tibble [67 x 2]>\r\n## 3 NEA       <tibble [67 x 2]>\r\n## 4 NOA       <tibble [67 x 2]>\r\n## 5 Pampeana  <tibble [67 x 2]>\r\n## 6 Patagonia <tibble [67 x 2]>\r\n## 7 Nacional  <tibble [67 x 2]>\r\n\r\nArmamos las recetas ü•£\r\nLa receta 1, es la utilizada por los modelos de redes neuronales.\r\n\r\n\r\nreceta_IPC_1 = recipe(value ~ ., data = data %>% select(-Region)) %>%\r\n  step_date(date, features = c(\"month\", \"quarter\", \"year\"), ordinal = TRUE)\r\n\r\n\r\n\r\nEsta receta es utilizada por el resto de los modelos. Una de las\r\ntransformaciones que se hace en esta receta es la conversi√≥n de la\r\nvariable de fecha a una variable de tipo num√©rica, compatible con los\r\nmodelos lineales.\r\n\r\n\r\nreceta_IPC_2 = receta_IPC_1  %>%\r\n  step_mutate(date_num = as.numeric(date)) %>% \r\n  step_normalize(date_num) %>%\r\n  step_rm(date) %>% \r\n  step_dummy(date_month)\r\n\r\n\r\n\r\nEl paquete recipes() facilita la transformaci√≥n y el\r\nprocesado de los datos a trav√©s de las funciones step(), que a\r\nmodo general se utilizan del siguiente modo:\r\nstep_corr(): Elimina las variables que tienen\r\nuna correlaci√≥n alta con otras variables.\r\nstep_center(): Centra los datos para que tengan\r\nmedia cero.\r\nstep_scale(): Normaliza los datos para que\r\ntengan desv√≠o estandar de 1.\r\nEstos dos pasos son importantes porque cuando los predictores son\r\nnum√©ricos, la escala en la que se miden, as√≠ como la magnitud de su\r\nvarianza pueden influir en gran medida en el modelo. Si no se igualan de\r\nlos predictores, aquellos que se midan en una escala mayor o que tengan\r\nm√°s varianza dominar√°n el modelo aunque no sean los que m√°s relaci√≥n\r\ntienen con la variable respuesta.\r\nstep_rm(): eliminar√° las variables en funci√≥n de su\r\nnombre, tipo o funci√≥n.\r\nAdem√°s, existen una diversidad de funciones asociadas a\r\nstep con diversos usos como: imputar datos a trav√©s de\r\ndiferentes estrategias (utilizando la media, un modelo lineal, etc),\r\npara transformaciones individuales (transformaci√≥n logar√≠tmica,\r\nexponencial, etc), ordenamiento, retrasos un muchas otras funciones.\r\nModelos ‚öôÔ∏è\r\nModelo ARIMA boosted\r\nLos modelos ARIMA fueron propuestos por Box y Jenkins en 1976 y se\r\ncaracterizan por realizar predicciones de una variable utilizando como\r\ninformaci√≥n la contenida en los valores pasados de la serie temporal. La\r\nsigla ARIMA refiere a Media M√≥vil Integrada\r\nAutoRegresiva. Los t√©rminos autorregresivos (AR) se refieren a\r\nlos retrasos de la serie diferenciada, los t√©rminos de promedio m√≥vil\r\n(MA) se refieren a los retrasos de los errores e I es el n√∫mero de\r\ndiferencia utilizado para hacer que la serie de tiempo sea\r\nestacionaria.\r\nUn punto muy importante a destacar es la implementaci√≥n en este\r\nmodelo del principio de boosting para mejorar los\r\nerrores del modelado. La idea detr√°s del boosting es generar m√∫ltiples\r\nmodelos de predicci√≥n secuenciualmente,y que cada uno de estos tome los\r\nresultados del modelo anterior, para generar un modelo m√°s ‚Äúfuerte‚Äù, con\r\nmejor poder predictivo y mayor estabilidad en sus resultados. Para\r\nconseguir un modelo m√°s fuerte, se emplea un algoritmo de optimizaci√≥n,\r\neste caso Gradient Descent (descenso de gradiente). Durante el\r\nentrenamiento, los par√°metros de cada modelo d√©bil son ajustados\r\niterativamente tratando de encontrar el m√≠nimo de una funci√≥n objetivo,\r\nque puede ser la proporci√≥n de error en la clasificaci√≥n, el √°rea bajo\r\nla curva (AUC), la ra√≠z del error cuadr√°tico medio (RMSE) o alguna otra.\r\nCada modelo es comparado con el anterior. Si un nuevo modelo tiene\r\nmejores resultados, entonces se toma este como base para realizar nuevas\r\nmodificaciones. Si, por el contrario, tiene peores resultados, se\r\nregresa al mejor modelo anterior y se modifica ese de una manera\r\ndiferente. XGBoost o Extreme Gradient\r\nBoosting, es uno de los algoritmos de machine learning de tipo\r\nsupervisado m√°s usados en la actualidad. Este algoritmo se\r\ncaracteriza por obtener buenos resultados de predicci√≥n con\r\nrelativamente poco esfuerzo, en muchos casos equiparables o mejores que\r\nlos devueltos por modelos m√°s complejos computacionalmente, en\r\nparticular para problemas con datos heterog√©neos.\r\n\r\n\r\nm_arima_boosted_ipc <- workflow() %>% \r\n  add_recipe(receta_IPC_1) %>% \r\n  add_model(arima_boost() %>% set_engine(engine = \"auto_arima_xgboost\"))\r\n\r\n\r\n\r\nModelo seasonal\r\n\r\n\r\nm_seasonal_ipc <- seasonal_reg() %>%\r\n  set_engine(\"stlm_arima\")\r\n\r\n\r\n\r\nModelo prophet boosted\r\nProphet es un procedimiento para pronosticar datos de series\r\ntemporales basado en un modelo aditivo en el que las tendencias no\r\nlineales se ajustan a la estacionalidad anual, semanal y diaria, adem√°s\r\nde los efectos de las vacaciones. Este paquete de c√≥digo abierto ha sido\r\nlanzado por el equipo Core Data Science de\r\nFacebook.\r\n\r\n\r\nm_prophet_boost_ipc <- workflow() %>% \r\n  add_recipe(receta_IPC_1) %>% \r\n  add_model(prophet_boost(mode='regression') %>%set_engine(\"prophet_xgboost\"))  \r\n\r\n\r\n\r\nModelo NNetar\r\nEste modelo se basa en redes neuronales con una sola capa oculta y\r\nentradas retrasadas para realizar los pron√≥sticos. Es un modelo\r\nautogresivo no lineal. Los modelos de redes neuronales o ANN (sigla del\r\ningles Artificial Neural Network), tienen la capacidad de aprender con\r\nel ejemplo y est√°n basados en los sistemas neuronales biol√≥gicos. La red\r\nneuronal es un conjunto de unidades de entrada/salida conectadas en las\r\nque cada conexi√≥n tiene un peso asociado. En la fase de aprendizaje, la\r\nred aprende ajustando los pesos para predecir la etiqueta de clase\r\ncorrecta de las entradas dadas. A modo general todos los modelos de ANN\r\nest√°n compuesto de: + Capa de entrada: esta\r\nrepresentada por los datos (inputs) del modelo. La cantidad de columnas\r\nque contiene nuestro archivo indicara la cantidad de neuronas o\r\nunits del modelo.\r\nCapa oculta: recibe los valores de la capa de\r\nentrada, ponderados por los pesos. El conjunto de pesos son\r\ninicializados aleatoriamente al principio y luego optimizados, mediante\r\nalgoritmos de aprendizajes.\r\nCapa de salida: indica el tipo de salida que\r\npretendemos obtener, por ejemplo si nuestro objetivo es obtener una\r\nclasificaci√≥n binaria (0 y 1), entonces la capa de salida contendr√° una\r\ns√≥la neurona.\r\nFunci√≥n de activaci√≥n: se especifica en la capa\r\noculta y tiene el objetivo de agregar no linealidad a nuestra\r\nred y le permite aprender caracter√≠sticas complejas. Algunos\r\nejemplos de estas funciones son: Sigmoid o Log√≠stica,\r\nReLu (unidad lineal rectificada), tanh\r\n(tangente hiperb√≥lica),etc.\r\nLa neurona es la unidad funcional de los modelos\r\nANN. Dentro de cada neurona ocurren dos operaciones: la suma ponderada\r\nde sus entradas y la aplicaci√≥n de una funci√≥n de activaci√≥n\r\n\r\n\r\nm_nnetar_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_1) %>%\r\n  add_model(nnetar_reg() %>% set_engine(\"nnetar\"))\r\n\r\n\r\n\r\nModelo MARS\r\nMARS hace referencia a la sigla del ingl√©s Multivariate\r\nAdaptive Regression Splines, es decir es un modelo de Regresi√≥n\r\nSplines Multivariante Adaptativo. De lo anterior se desprenden varios\r\nt√©rminos que es convienente abordar, como son:\r\nSplines: Spline es una funci√≥n especial definida\r\npor polinomios y representa una t√©cnica de regresi√≥n no\r\nparam√©trica (es decir d√≥nde no debemos asumir linealidad en\r\nnuestros datos). En esta t√©cnica, el conjunto de datos se divide en\r\ncontenedores a intervalos o puntos que llamamos nudos, por lo que puede\r\ndecirse que ‚Äúlos splines son series de segmentos polin√≥micos unidos\r\nentre s√≠ en nudos‚Äù.\r\nLos modelos MARS superan una de las principales desventajas de la\r\nregresi√≥n polinomial y que es la dependencia de indicar a priori en que\r\npuntos de la variable x deben hacerse los puntos de corte. Para\r\nello el procedimiento eval√∫a cada punto de datos para cada predictor\r\ncomo un nudo y crea un modelo de regresi√≥n lineal con las\r\ncaracter√≠sticas candidatas. El procedimiento MARS primero buscar√° el\r\npunto √∫nico en el rango de valores de x donde dos relaciones\r\nlineales diferentes entre x e y logran el error m√°s\r\npeque√±o\r\n\r\n\r\nm_mars_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_2) %>%\r\n  add_model(mars(mode = \"regression\") %>% set_engine(\"earth\"))\r\n\r\n\r\n\r\nModelo Elastic net\r\nEl modelo Elastic net forma parte del conjunto de\r\nmodelos de regresi√≥n penalizados, que incluyen los modelos: Ridge, Lasso\r\ny Elastic net.\r\nSon particularmente √∫tiles en el contexto de tener un conjunto de\r\ndatos multivariado con una gran gran cantidad de variables, d√≥nde el\r\nm√©todo de los m√≠nimos cuadrados (ajuste del modelo lineal), funciona de\r\nforma incorrecta. El pincipio general implica la incorporaci√≥n de un\r\nt√©rmino de penalizaci√≥n que tiene como consecuencia\r\nreducir (es decir, encoger) los valores de los coeficientes hacia cero.\r\nEsto permite que las variables menos contributivas tengan un coeficiente\r\ncercano a cero o igual a cero. El modelo Elastic net\r\npenaliza con la norma L1 y la norma L2\r\n(es decir las que se utilizan en los modelos Lasso y Ridge). Esto\r\nimplica: + norma L1: es implementada en los modelos de\r\nregresi√≥n Lasso. Aqu√≠ la penalizaci√≥n tiene el efecto de obligar a\r\nalgunas de las estimaciones de los coeficientes, con una contribuci√≥n\r\nmenor al modelo, a ser exactamente iguales a cero. Es una buena\r\nalternativa para la selecci√≥n de variables con el fin de reducir la\r\ncomplejidad de los modelos. + norma L2: es implementada\r\nen los modelos de regresi√≥n Ridge. En este caso la penalizaci√≥n tiene\r\ncomo efecto la reducci√≥n de los coeficientes de regresi√≥n, de modo que\r\nlas variables con una contribuci√≥n menor al resultado, tengan sus\r\ncoeficientes cercanos a cero.\r\n\r\n\r\nm_glmnet_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_2) %>%\r\n  add_model(linear_reg(penalty = 0.01, mixture = 0.1) %>% set_engine(\"glmnet\"))\r\n\r\n\r\n\r\nModelo Xgboost: √Årbol de\r\ndecisi√≥n\r\nM√°s arriba se detallo la estrategia de boosting para\r\nreducir los errores del modelado como as√≠ tambi√©n las implementaciones\r\ndel paquete xgboost en R y en otros lenguajes de uso\r\ncotidiano en ciencias de datos. Pero queda abordar las caracter√≠sticas\r\nm√°s destacadas de los √°rboles de decisi√≥n como modelos\r\nde machine learning. Los √°rboles de decisi√≥n\r\nson modelos predictivos formados por reglas binarias (si/no) con las que\r\nse consigue repartir las observaciones en funci√≥n de sus atributos y\r\npredecir as√≠ el valor de la variable respuesta. Se engloban dentro de\r\nlos modelos supervisados de machine learning es decir\r\nconocemos a priori la variable de respuesta o dependiente que utilizan\r\nt√©cnicas no param√©tricas. Adem√°s de los algoritmos\r\nGradiente Boosting, existen otros como random\r\nforest para implementar este tipo de m√©todos.\r\n\r\n\r\nm_xgboost_ipc <- workflow() %>%\r\n  add_recipe(receta_IPC_2) %>%\r\n  add_model(boost_tree() %>% set_engine(\"xgboost\"))\r\n\r\n\r\n\r\nEntrenamiento y\r\nEvaluaci√≥n de los modelos ü§∏üèΩ\r\nPartici√≥n\r\nComo primera medida dividimos nuestros datos en datos para\r\nentrenamiento (training) y para test (evaluaci√≥n). Para este caso\r\nseparamos en 80% y 20%\r\n\r\n\r\nsplit <- 0.80\r\n\r\n\r\n\r\nTabla modelos üë®‚Äçüè´\r\nCreamos una tabla que contiene todos los modelos en evaluaci√≥n y\r\nd√≥nde se guardar√°n las m√©tricas con las que compararemos la performance\r\nde los diferentes modelos.\r\n\r\n\r\nmodel_table_ipc <- modeltime_multifit(\r\n  serie = ipc_nested,\r\n  .prop = split,\r\n  m_arima_boosted_ipc,\r\n  m_seasonal_ipc,\r\n  m_prophet_boost_ipc,\r\n  m_nnetar_ipc,\r\n  m_mars_ipc,\r\n  m_glmnet_ipc,\r\n  m_xgboost_ipc\r\n  \r\n)\r\n\r\n\r\n\r\nModelos\r\nevaluados y su correspondiente m√©trica de error\r\nEn la tabla se encuentran analizadas las siguientes m√©tricas de\r\nerrores:\r\nmae (Mean Absolute Error): es la media de los\r\nerrores en valor absoluto. Esta m√©trica favorece modelos que predicen\r\nmuy bien la gran mayor√≠a de observaciones aunque en unas pocas se\r\nequivoque por mucho.\r\nmape (Mean Absolute Porcentaje Error): representa\r\nel valor absoluto expresado en porcentaje, es decir indica en t√©rmino\r\nporcentuales la diferencia promedio entre el valor predicho y el\r\nobservado.\r\nmase (Mean Absolute Scaled Error): esta metrica\r\nsurge de un algoritmo que compara los pron√≥sticos con el resultado de un\r\nenfoque de pron√≥stico ingenuo. El pron√≥stico\r\ningenuo se genera en cualquier paso igualando el pron√≥stico\r\nactual con el resultado del √∫ltimo paso de tiempo sin considerar\r\nning√∫n patr√≥n estacional.\r\nsmape (Symmetric Mean Absolute Percentage Error):\r\nrepresenta la diferencia absoluta entre predicho y\r\nobservado dividido por la mitad de la suma de los valores\r\nabsolutos predichos y observado. El valor de este\r\nc√°lculo se suma para cada punto ajustado t y se divide\r\nnuevamente por el n√∫mero de puntos ajustados n.\r\nrmse: Es la ra√≠z de la media de los errores\r\nelavados al cuadrado. Sirve para evalur el ajuste del modelo y sus\r\nvalores se expresan en la misma unidad de la variable proyectada.\r\nrsq: representa el coeficiente de determinaci√≥n\r\nutilizando una correlaci√≥n. Toma valores entre 0 y 1 y describe el % de\r\nvariaci√≥n de la variable de respuesta (transacciones, UT, etc.)\r\nexplicado por la variable que puede explicarse por el modelo.\r\n\r\n\r\n\r\nSelecci√≥n del mejor modelo üòé\r\nLuego de la evaluaci√≥n de los distintos modelos, en base a las\r\nm√©tricas antes descriptas, en esta etapa se selecciona el modelo que\r\nmejor se ajusta a los datos, es decir, aqu√©l que menos error contenga en\r\nla estimaci√≥n. En este punto, se hace uso del m√©todo\r\nmodeltime_multibestmodel de la libreria\r\ntimetk y se toma el RMSE como m√©trica\r\npara seleccionar el mejor modelo.\r\n\r\n\r\nbest_model_ipc <- modeltime_multibestmodel(\r\n  .table = model_table_ipc$table_time,\r\n  .metric = \"rmse\",\r\n  .minimize = TRUE,\r\n  .forecast = FALSE\r\n  \r\n)\r\n\r\n\r\n\r\nModelo con mejor\r\nperformance por regi√≥n\r\n\r\n##      Region  Modelo\r\n## 1       GBA   EARTH\r\n## 2      Cuyo XGBOOST\r\n## 3       NEA  GLMNET\r\n## 4       NOA   EARTH\r\n## 5  Pampeana   EARTH\r\n## 6 Patagonia   EARTH\r\n## 7  Nacional   EARTH\r\n\r\nEntrenamiento del mejor modelo\r\nüèã\r\nEn esta etapa entrenamos nuestros datos con el modelo que mejor\r\nperformance mostr√≥ seg√∫n la m√©trica seleccionada en el paso anterior (en\r\nnuestro caso RMSE). Adem√°s se entrena el modelo con la totalidad de\r\ndatos.\r\n\r\n\r\nmodel_refit_ipc <- modeltime_multirefit(best_model_ipc)\r\n\r\n\r\n\r\n\r\n\r\n\r\nForecast datos üîÆ\r\nPor √∫ltimo, con el modelo entrenado realizamos nuestras proyecciones\r\n(forecast), para un per√≠odo de 12 meses\r\n\r\n\r\nforecast_ipc <- modeltime_multiforecast(models_table = model_refit_ipc,\r\n                                           .h = 12,\r\n                                           .prop = split)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ngrafico <- forecast_ipc %>%\r\n  plot_time_series(df = .,title = \"Forecast IPC\",\r\n                   y_lab = \"IPC mensual\",\r\n                   x = \"Mes\",\r\n                   y = \"IPC\",\r\n                   facet = \"Region\")\r\ngrafico\r\n\r\n\r\n\r\n\r\nAlgunos comentarios üó£\r\nüëâüèº No quedan dudas de que los autores de los paquetes aqu√≠ revisados\r\n(Modeltime, Timetk, Sknifedatar), han hecho un gran aporte para\r\nfacilitar el trabajo con series de tiempo y modelos en contextos\r\nhabituales d√≥nde la labor se centra en ajustar m√∫tiples modelos y series\r\nde tiempo.\r\nEste flujo de trabajo de ha sido muy √∫til y espero que les sea a\r\nqui√©n lea este material üëê.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-13-ajuste-multiples-series-de-tiempo/imagenes/preview.jpg",
    "last_modified": "2022-08-23T16:38:35-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-30-analisis-de-estacionalidad/",
    "title": "An√°lisis de estacionalidad",
    "description": "En cualquier √°mbito de la vida cotidiana d√≥nde pretendamos hacer proyecciones o compararnos con un momento pasado, es primordial identificar si nuestros datos tienen estacionalidad üìÖ. En este post voy a mostrar algunas estrategias visuales para analizar estacionalidad en series temporales.",
    "author": [
      {
        "name": "Hernan Hernandez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-07-30",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n¬øPor qu√© analizar la\r\nestacionalidad? ü§î\r\nEstacionalidad por comercio\r\nüõçÔ∏è\r\nRanking de dias seg√∫n\r\ncantidad de ventas üìÖ\r\nComentarios finales üòâ\r\n\r\nCargo los paquetes\r\n\r\n\r\n\r\n¬øPor qu√© analizar la\r\nestacionalidad? ü§î\r\nCu√°ndo hablamos de estacionalidad nos referimos a un\r\ncomportamiento regular y repetitivo en nuestras\r\ntransacciones, ventas, movimientos o clientes a lo largo del tiempo.\r\nGeneralmente, la estacionalidad est√° asociada a fen√≥menos clim√°ticos o\r\nfestivos.\r\nLa estacionalidad de las series de debemos considerarlas, para:\r\nPlanificar y predecir nuestro futuro ‚úÖ,\r\nComprender el negocio ‚úÖ,\r\nImplementar campa√±as ‚úÖ,\r\nEstacionalidad por comercio üõçÔ∏è\r\nEn este post estaremos analizando el dataset üßæ que contiene\r\nlas ventas por d√≠a de 3 comercios ficticios, entre el\r\n01-06-2021 y el 31-12-2021 üìÜ . A continuaci√≥n podemos ver el resumen de\r\nlos datos del archivo.\r\n\r\n\r\n\r\n\r\nFecha\r\n\r\n\r\nMerchant\r\n\r\n\r\nCant_Ventas\r\n\r\n\r\n\r\n\r\nMin. :2021-06-01\r\n\r\n\r\nLength:642\r\n\r\n\r\nMin. : 1441\r\n\r\n\r\n\r\n\r\n1st Qu.:2021-07-24\r\n\r\n\r\nClass :character\r\n\r\n\r\n1st Qu.:10599\r\n\r\n\r\n\r\n\r\nMedian :2021-09-15\r\n\r\n\r\nMode :character\r\n\r\n\r\nMedian :18258\r\n\r\n\r\n\r\n\r\nMean :2021-09-15\r\n\r\n\r\nNA\r\n\r\n\r\nMean :17035\r\n\r\n\r\n\r\n\r\n3rd Qu.:2021-11-08\r\n\r\n\r\nNA\r\n\r\n\r\n3rd Qu.:22243\r\n\r\n\r\n\r\n\r\nMax. :2021-12-31\r\n\r\n\r\nNA\r\n\r\n\r\nMax. :58553\r\n\r\n\r\nüìù ahora que vimos el conjunto de datos, usaremos la funci√≥n\r\ncreate.seasonal. Debemos indicarle dos par√°metros, el\r\nnombre del data frame y del campo d√≥nde est√° la variable de tipo date\r\n(de fecha). El resultado es una lista que contiene un gr√°fico por cada\r\nsegmento usado (aqu√≠ por cada Merchant), con la estacionalidad\r\ndiaria, semanal, mensual y trimestral üëÄ.\r\n\r\n\r\nestacionalidad <- data %>%\r\n  mutate(nombre= Merchant) %>%\r\n  nest(column_nest= -c(Merchant)) %>%\r\n  mutate(seasonal_dx = map(column_nest, ~ create.seasonal(df = .,\r\n                                                          date = \"Fecha\")))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nmerchant A\r\n\r\nmerchant B\r\n\r\nmerchant C\r\n\r\n\r\n\r\nUna aclaraci√≥n importante ‚ö†Ô∏è, los puntos de color fucsia indican los\r\nvalores an√≥malos o at√≠picos los cu√°les se detectan seg√∫n la siguiente\r\nf√≥rmula:\r\nQ3+1.5 x IQR\r\nQ1-1.5 x IQR\r\nEs decir consideramos an√≥malos todos los valores que se est√°n por lo\r\nmenos 1.5 veces el rango intercuartil por encima del cuartil 3 o por\r\ndebajo del cuartil 1 üî•.\r\nAlgunos comentarios de los\r\ngr√°ficos üì£\r\n‚úçüèºEl comercio A tiene mayores ventas los viernes, s√°bados y\r\ndomingos. El comercio B y C tienen una ca√≠da muy marcadas de sus ventas\r\nlos domingos.\r\n‚úçüèºEn todos los comercios se observa que diciembre es el mejor mes en\r\ncuanto a ventas y por ello el trimestre 4 es el mejor.\r\nRanking de dias seg√∫n\r\ncantidad de ventas üìÖ\r\nUna estrategia visual que podemos sumar para una mejor compresi√≥n del\r\ncomportamiento temporal de nuestos datos -ventas en nuestro ejemplo-, es\r\ncombinar mapas de calor (heatmap) en calendarios d√≥nde\r\npodamos identificar d√≠as con mayores ventas (zonas de calor) y d√≠as con\r\nmenores ventas. Para ello, hacemos uso de la funci√≥n\r\nheatmap.calendar que recibe 4 par√°metros: + a) el\r\nnombre del dataframe, b) fecha de inicio, c) fecha final y d) Una escala\r\nde color Brewer para identificar la intensidad de las ventas por\r\nd√≠a.\r\nVeamos el resultado de la funci√≥n üëÄ.\r\n\r\n\r\nheatmap_calendar <- data %>%\r\n  mutate(nombre= Merchant) %>%\r\n  nest(column_nest= -c(Merchant)) %>%\r\n  mutate(heatmap_calendar = map(column_nest,~heatmap.calendar(df = .,\r\n                                                  fini = '01-06-2021',\r\n                                                  ffin = '31-12-2021',\r\n                                                  ColorBrewer = \"GnBu\")))\r\n\r\n\r\n\r\n\r\n\r\nmerchant A\r\n\r\nmerchant B\r\n\r\nmerchant C\r\n\r\n\r\n\r\nComentarios sobre el\r\nheatmap calendar ü•µ\r\nüëâ se puede observar con claridad el patr√≥n de mayor nivel de ventas\r\ndel merchant A los d√≠as s√°bados y domingos, mientr√°s que para los otros\r\nmerchant caen las ventas en esos d√≠as.\r\nüßê nos permite observar el incremento en las ventas en ciertos d√≠as,\r\nasociados a festividades o a acciones particulares de parte de los\r\nmerchant.\r\nü§∂ se evidencia un claro incremento en las ventas en las d√≠as\r\npr√≥ximos a la nochebuena y navidad.\r\nComentarios finales üòâ\r\nüî∑ En este post mostr√© dos estrategias visuales para analizar la\r\nestacionalidad en nuestras series de datos.\r\nüíé La aplicaci√≥n de programaci√≥n funcional en R hace que los scripts\r\nsean m√°s eficiente, al mismo tiempo que nos facilita la reutilizaci√≥n\r\ndel c√≥digo, la b√∫squeda de bugs, etc.\r\nüôèüèº gracias por leer mi blog. Cualquier comentario u opini√≥n es\r\nsiempre bienvenida üëêüèº.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-30-analisis-de-estacionalidad/imagenes/estacionalidad.png",
    "last_modified": "2022-08-30T10:02:41-03:00",
    "input_file": "analisis-de-estacionalidad.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Bienvenido a mi blog",
    "description": "Gracias por compartir este espacio conmigo, cuyo principal objetivo\nes crear un espacio de intercambio y crecimiento mutuo.",
    "author": [
      {
        "name": "Hern√°n Hern√°ndez",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-07-18",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-08-28T11:22:31-03:00",
    "input_file": {}
  }
]
